[0:00:01 - 0:00:27]
皆さんこんばんは、松尾研究室の川崎と申します本日は、大規模言語モデル講座Day2ってことで、ライブで皆さんご参加いただいて りがとうございます。前回1450ぐらいでした。に対して600名ぐらいですが、ともかくこの後参加していただくことを期待して講座を始めさせていただければと思います。

[0:00:27 - 0:00:58]
はい。本日はDay2の講座になります。タイトルは「PromptingとRAG」です。RAGについては皆さんかなり興味があるのではないかと思いますので、ぜひお楽しみいただければと思います。講師は松尾・岩澤研の配属学生である原田さんと、昨年LLM講座を受講され、現在内部の運営としても活躍されているベルトンさんです。BERTについても後ほど詳しく説明しますので、よろしくお願いします。

[0:00:59 - 0:02:40]
ちょっと最初に始める前にですね、エポックの方から前回もコアの数についてですけども、チャットボットの使い方を改めて簡単にお伝えできればなというふうに思います。リンクはこちらになっていて、皆さんご自身のOmnicampusのアカウントでログインしていって、このリンクを踏んでいただくと、この画面が出るのかと思います。質疑応答はですね、前回聞き漏らした方もいらっしゃるかと思うので、改めて説明しますと、この講座においての質疑応答はチャットボットをどんどん導入しているので、ここからご質問いただけるかと思います。基本的に前回開催したLM講座の情報というのをRAGで入れていて、そこで回答できないものに関しては、GPT-4oが裏側で動いて回答するような仕組みになっています。おそらくほぼほぼそちらで回答できるのかなと思っているんですが、一部、何か解決しなかったりとか、マルチなんで何回かこうやってやるうちに、なんかおかしな会話になっていったりそういうことが考えられますので、そこのちょっとご案内をさせていただければと思います。まずこの右上の「質問をする」ってボタンを押していただいて、講義海峡Day2第2回なので、このPromptingとRAGで内容に関して質問の範囲に関しては講義の範囲内の話なのか、範囲外なのかその他なのかというのを選んでいただいて、質問を打っていただく感じですね。

[0:02:40 - 0:03:47]
送信すると、返ってきますので、はい返ってきましたね、これに対してですね、必ずというか、かなりお願いしても、もし解決した場合、このGoodボタンを押してください。解決しなかったら、再度下の質問を繰り返していただくと思います。最終的に回答しなかったら、このバッドボタンを押してください。バッドボタンを押していただくと運営側に通知が行くので、ベストエフォートになりますが、講師もしくは運営の者が回答する感じになります。回答が進みましたら、皆さんのメールアドレス宛に通知が来るはずなので、そちらで最終的な回答をご覧いただけるかと思いますので、そちらをご参照いただき、GoodByeGoodBERTボタンを押していただければと思います。

[0:03:47 - 0:04:20]
ではですね、そのような性質があるため、基本的にはワンスレッド1の質問で対応できる回数をお使いいただければと思います。最初の質問に対して回答した場合は、その質問に対して解決した場合は、解決しなかった場合はいずれかのボタンを押して、セッションを終了していただくと、新しく別の質問がされた場合、されたい場合は、右下のこちらから新しいセッションを始められますので、そこでサイトで再度新しい質問を行うことができます。

[0:04:20 - 0:04:36]
打っていただければというような形でお願いできればと思いますので、よろしくお願いいたします。本日ですね、講義の中で質疑応答の時間に可能な限り取りたいと思ってますので、はい、ぜひこちらをご活用いただければというふうに思ってますので、よろしくお願いします。

[0:04:38 - 0:05:13]
はい、そうです。本日からですね。前回は座学だけでしたけれども、本日から各回ごとに演習の時間というのを設けていきたいと思います。講義時間は2時間だと思うんですけど、大体90分が最初講義で、残り30分が演習の時間というような配分で、基本的にはこのような配分で進めていければというふうに思っていますので、どうぞよろしくお願いします。練習環境に関しては、後ほど説明させていただきたいというふうに思っていますので、そちらのご案内をお待ちください。

[0:05:14 - 0:05:40]
本日はGoogle Colabを使用する予定でございます。はい、そのような形で講義を始めていければなというふうに思います。原田さん、ご準備いかがでしょうか？はい、少々お待ちください。見ていますかね。

[0:05:40 - 0:06:11]
はい、大丈夫ですよろしくお願いします。それでは本日の授業でPromptingとRAGについて話をしたいと思います。篠原だけです。軽く自己紹介ですが、松尾・岩澤研の博士課程の研究者です。去年この講座が立ち上がったときに、講座の資料作成や最終課題のコンペを作成していたりしました。

[0:06:11 - 0:06:31]
またGENIACというプロジェクトでは、皆さんから提出されたモデルの評価を担当していたりもしました。研究としては、大規模言語モデルの文脈活用能力について、教育応用場面における大規模言語モデルの活用方法などについて発表をしたりしています。

[0:06:31 - 0:06:56]
今回の講義は、本日のPromptingとRAG、来週の演習部分でPretrainingでGPT2を実装するところを担当します。よろしくお願いします。さて全体像の確認なんですが、本日はPromptingとRAGについて、学習完了後の大規模言語モデルを活用する技術についてお話します。

[0:06:57 - 0:07:35]
他の講義との関係性でいうと、今回は学習なしでどうやって性能を上げるかという話なんですが、学習によって性能を向上させるテクニックというのは、第5回第7回第11回などで扱 ます。そして今回言語言語モデル言語ドメインで言語だけでPromptingしたり言語情報だけで扱うんですが、音声や画像が与 られるようなマルチモーダル設定で ったりロボット犯したり行動選択にモデルを使うというところは第12回で扱 ます。

[0:07:37 - 0:08:03]
そして本日お話する内容がなぜ、なぜうまくいくのかその大規模言語モデルの中でどのようなことが起きていて、だから成功失敗するのかという話は第10回見て詳しくお話します。なので本日はですね、うまくその学習完了後のモデルを、こうやったらうまくいったというテクニック集を、いろんな論文からご紹介できればと思います。

[0:08:06 - 0:08:26]
本日の目的が追加学習せずに学習なしで活用する技術についてお話します。目標としてはPromptingと呼ばれる技術や文脈内学習という単語が何か説明できるようになる。そしてPromptingによって性能を改善する方法を自分自身で説明できるようになる。

[0:08:26 - 0:08:50]
そしてRAGを含む概念で る下面とLanguageモデルの必要性と原理をご自身で説明できるようになるというのを目標にしています。演習では、実際にモデルをダウンロードして、今回学んだPromptingの技法を実装してもらうのと、RAGを実際に実装してもらうのが今回の本日の演習です。

[0:08:51 - 0:09:19]
それでは始めていきます。まず前回の復習で言語モデルについて説明しました。言語モデルとは、単語の系列を文章の性質を確立するものでした。特に、文章を分解し、各単語の連鎖率で分解し、XはX次元、Xは順に繋げていって次モデル化したものは自己回帰言語モデルと呼ばれます。

[0:09:21 - 0:09:37]
このように条件つき確率がわかると生成デコーディングもできるというところで、日本の人はその次に続く最も妥当なもの、一番スコアが高いものを選ぶと、例えば「東京」というような文章が出てくるという流れになります。

[0:09:40 - 0:09:59]
このようなモデルどのように学習しますかという話なんですが、ネクストtokenフリクションといって未来の位置一つ先のtoken単語を当てる学習をすることによって、言語モデルがうまく文章をモデル化する性能を高めていきます。

[0:09:59 - 0:10:25]
予測と正解の誤差を小さくするようにニューラルネットを学習していくというのが一連の流れです。そしてそのニューラルに何を使うかというところでTrasnformerというモデルが発表されておりまして、現在の主要なLLMは、大体がこのTrasnformerを使って、学習されてます大規模に学習されてますと。

[0:10:25 - 0:11:35]
実際にどのような処理が内部でなされるかというところは来週の講義で、理論面そして実装でも理解することを目指します。このようにTransformerで言語モデルをたくさん学習させると、様々な知識を持っていて、その知識を測るベンチマークでも高い性能が報告されています。医学の知識も含まれているということが報告されています。さらに、ただその生の知識を評価するだけでなく、普段の仕事、普段のタスクで、このLLMを使うことによって、何かリスク、効率化されたりする部分があればというところも報告されていて、ビジネスコンサルタントさんがChatGPTを活用することによって、実務を模擬したタスクで質が向上していたり、またエンジニアの皆さん使ったことのあるかもしれませんが、普段そのコードを書くというのもコードを生成するLLMを使うことによってアシストすることができて、エンジニアの生産性や満足度が向上しているというような事例も報告されています。

[0:11:37 - 0:12:13]
大規模言語モデルは、大規模な学習データで大規模なモデルをたくさんの生産資源を使って学習させることで、多くの知識を持ち、高性能なモデルが作られます。非常にコストがかかるものですが、皆さんがAPIを通じて、あるいは実際に学習済みのモデルをダウンロードして簡単に利用することができます。このモデルをどのようにうまく活用できるか、その活用のための技術を学ぶことが本日の内容となります。

[0:12:14 - 0:12:33]
モデルといってもいろいろありますが、大きく分けて三つあります。一つが非公開モデルで、論文としては「こういうモデルを開発しました」というところが公開されますが、それを使用できるのは一部の研究機関や団体に限られています。

[0:12:33 - 0:13:05]
これは、有名なモデルの一つで、Gopherと呼ばれるものがあります。もう一つは、APIを通じてのみ利用できる点が特徴です。モデルのニューラルネットワークの中の重みパラメーターは公開されていませんが、そのAPIを通じてプロンプト文を投げることで、出力を得られるモデルがAPIのみで提供されるモデルです。OpenAIが開発したGPTシリーズや、GoogleのGeminiシリーズ、AnthropicのClaudeシリーズなどがこれに分類されると思います。

[0:13:06 - 0:13:43]
最後に公開モデルというところで、モデルの重みのパラメータまで公開されており、誰でもダウンロードできるようになっています。このモデルの良いところとしては、内部の処理を自分で可視化することもできるので、分析にも適しています。さらに、このモデルを使ってFine-Tuningを行うこともでき、カスタマイズ性が高いです。これらのモデルとして有名なものには、Meta社が開発するLlamaシリーズがあります。今回の演習でも触るLlamaシリーズや、Mistral、Bloom、Falconなどがあります。

[0:13:46 - 0:14:16]
これらのモデルは本当にいろいろな種類がありますが、どのモデルを使うべきか判断する際にも、軸がいろいろありますが、有名な視点で紹介しますと、まずそれぞれのモデルが解きたいタスクに近いデータで学習されているかどうか、または入力の文章の長さを受け付ける範囲が注目するポイントだと思います。

[0:14:16 - 0:14:33]
これらの点に関しては、モデルとともに公開されるテクニカルレポートというものがありますが、そちらを参照することで理解できます。それがなかったとしても、そのモデルを公開しているページがありますので、そちらに記載されているところを参照すると良いと思います。

[0:14:34 - 0:14:53]
なので僕自身見る視点としては、よく海外の有名な会社がモデル学習して公開したってなったらHuggingFaceのページを見て、Japaneseみたいな単語でとかJAとかJPとか、検索してどれぐらいのtoken数で学習してるかなみたいなところを見たりします。

[0:14:56 - 0:15:13]
次に、既存のアカデミアでよく使用されるようなベンチマークで性能がどれほど出ているかを確認します。有名なものとしては、オープンLLMリーダーボードやチャットボットアリーナなどがあります。日本では、ねずみリーダーボードも一つ有名かなと思います。

[0:15:15 - 0:15:40]
自分が解きたいタスクに近いベンチマークについて、ある程度把握しておいて、それらのタスクでどれぐらいの性能が出てくるのか確認することが重要です。そして、推論モデルが大きすぎると、コストパフォーマンスが合わないという点も考慮する必要があると思います。また、ライセンス利用規約なども、モデルを選ぶ際のポイントかなと思います。

[0:15:43 - 0:16:20]
ベンチマークの一つをちょっと紹介するんですが、地味なリーダーボードというものが日本ではよく見られると思います。このリーダーボードはaWebandbyAshes社が開発した日本の支社が開発したものなんですけど、日本のベンチマークをいろいろ集めて、それらを評価した結果という総合したスコアなどを載せています。そこを見ると、日本語で自分が解きたいタスクはどれだろうというところも、ファクタすけのスコアとかも出てるので参照することができますと思います。

[0:16:20 - 0:16:42]
そして、モデルごとの順位は総合スコアで、どのモデルが良さそうかという点も確認できます。大体は、そのAPIで提供されているモデルが適していると思われます。日本で開発されたサイバーエージェント社のモデルは、この程度のスコアが出るんだなというところがわかると思います。

[0:16:45 - 0:17:07]
他社が評価しているスコアを既存のベンチマークで評価してみるのもいいですが、やはり自分で作ったタスクで評価したいというところが重要だと思います。それらをサポートするツールもありますので、単語というか、それだけの紹介をさせていただきます。

[0:17:07 - 0:18:18]
ただ、注意してほしいことは、今回そのプロンティングについて学ぶとわかると思うんですが、細かい違いによって同じモデルでも大きく異なるスコアが出るというのが、気をつけるポイントです。さて、モデルを選びましたと、APIによって使うというのをどう使うか、そのイメージ簡単なイメージなんですが、こちらはOpenAI社が出してるモデルを使うときには、まずはAPIキーというものを取得して、OpenAIが作ってるライブラリ専用のライブラリを使って、授業垂らすで何かタスクをお願いすると返してくれるということで、簡単に使えます。各社いろいろなライブラリが出ていますが、それらの使い方を把握するのも大変かと思うので、僕がそれらをまとめたツールを使って作ってみたので、もし興味がある人が見てみてください。そして、今回の演習でも今後の演習でもそして最終課題のコンペでもずっと使うことになるTransformer図と呼ばれるライブラリが公開モデルを使うための便利なライブラリとして、デファクトスタンダードとなっています。

[0:18:18 - 0:18:47]
こちら今回のこの講義としてずっと使うライブラリになりますので、ドキュメントやチュートリアルの一読をおすすめします。細かいことが気になる方は講座も公開されているのでそちらを見てください。さて前向きが前置きが長くなってしまったんですが、これから本題学習済みモデルを活用する技術についてお話します大きく分けて二つ話します。

[0:18:47 - 0:19:17]
一つがモデル一つだけを使って工夫する技術で、Promptingと呼ばれています。そもそもモデルでタスクを解くこととはどういうことか、問い合わせと性能を引き出せるのか、そしてなぜそのPromptingと呼ばれる技術が重要なのかについて前半をお話しします。後半では、RAGを含む概念で、外部ツールや知識を利用する話についてお話します。

[0:19:20 - 0:19:47]
では前半話していきます。LLM皆さん触った事 るかと思うんですがこのように何か翻訳しお願いするだけで、英語を日本語に翻訳してくれたり、5歳児でもわかるように説明してというだけで、何かしら説明してくれたりと、指示に応じて同じモデルなんですけど指示に応じて返答が異なるという特徴が ります。

[0:19:48 - 0:20:08]
モデルとコミュニケーションを取るだけでも楽しいんですが、せっかくなので、この高性能なモデルを使って、日々の生活にどのように役立ったり、タスクを解くことができるか、言語モデルによってどのようにタスクを実行できるかという話が前半の内容になります。

[0:20:11 - 0:21:14]
例えば、QAを解かせたいとすると、日本の人はどこかというところを機構とすると、東京と答えてもらいたいといったり、感情分析、センチメント分析をさせようとして、このラーメン屋ラーメンはうまいという文章は、ポジティブかネガティブか、そういうことで企業のタスクセンチメント分析のタスクを解かせたいんですけれども、言語モデルは最もらしい文章の確立をモデル化しているわけなんですが、日本の人は、の後に必ず東京が続くとは限りません。日本の人はどこですかといったり、日本の人は京都ではないったり、平安時代の日本の人は平安京だったりといった感じで、今最もらしい文章というのはいくつか候補があります。ただ、狙いたい出力というか解かせたいタスクに応じて、より上手く入力とか条件を与ることで、そのタスクを解かせるための性能を高めることができないかというのが大きな問いです。

[0:21:16 - 0:21:37]
どのように指示出しをすると良いかというのがPromptingの技術になります。用語の確認なんですけど、Promptingとはどういうものかといいますと、特定の機能の発生を促進するものです。これがプロンプトと呼ばれるんですが、促進するような言語モデルに入力するコンテキスト部になります。

[0:21:39 - 0:21:55]
センチうんて分析ぽじ値が判定をさせたいので れば次の文章がポジティブかネガティブか分類してというように、その分類するような言語モデルが分離するような機能を発生を促進するような文章がプロンプトPromptingと呼びます。

[0:21:57 - 0:22:12]
Promptingには様々なテクニックがありますが、有名なものとしてFew-Shot Promptingと呼ばれるものがあります。このFew-Shot Promptingとは、まずこのタスクを説明するタスク指示文というものです。

[0:22:12 - 0:22:30]
【修正後】（修正されたテキストのみを出力してください）

説明というものがあります。こちらの例では、英語からフランス語に翻訳してくれというタスク指示文があります。それに追加して、実際にそれ、英語からフランス語に翻訳しているような例をデモンストレーション例として入れます。

[0:22:32 - 0:23:47]
これを英語に翻訳すると、こうなる英語矢印に、フランス語、英語矢印、フランス語となりますと、今回翻訳させたいものはチーズですと、チーズはどうなる。どうなりますかというところで、デモンストレーション例も含めてどうなりますかと聞くのがFFew-Shot Promptingです。このショットというのが例のデモンストレーションの数に対応していて、何も入れないものをZero-Shot、1個だけ入れるものをOne-Shot、1個以上のものをFew-Shotと2個以上のものをFew-Shotと呼びます。その関係性、Few-Shotの数と性能の関係性を見てみると、何も入れないZero-Shotだけだと、これは全部同じモデルを対象にしていますが性能が低いです。これが入力する数の増加に伴い良くなっていくことがわかります。さらにこのデモンストレーション例を追加することで性能が上がる幅というのが、大規模モデルが大きくなればなるほど、1.3Billion（13億パラメータ）130B（130億パラメータ）、1750B（1750億パラメータ）のモデルの違いによって、その上がり幅も全然違いました。

[0:23:47 - 0:24:07]
モデルが大規模になればなるほど、このFew-Shotの性能が効きますとこのデモンストレーション例から、何かしらこのタスクの遂行をするということを学習するため特に分野区内学習院コンテキストLearningICLと略されることが多いんですが、そのような単語で呼ばれます。

[0:24:09 - 0:24:33]
従来のFine-Tuningの重みパラメータを更新するFine-Tuningとは何が違うのかというと、従来のFine-Tuningではパラメータを更新するため、性能が出やすく、毎回事例を入れる必要がないため、その入力分が短くなり、推論コストが低くなります。しかし、タスクごとに学習させる必要があるため、学習コストがかかります。

[0:24:34 - 0:24:58]
それに対して、Few-Shot Promptingのような文面文脈内学習では、パラメータ重みを更新することなく、1回作ったモデルをどんなタスクでも使い回せます。そのため学習コストはかかりません。ただ、Fine-Tuningよりも性能が出にくいことや、入力量が増えることで、水力コストが高くなるという点も言われています。

[0:25:00 - 0:25:15]
文脈内学習、このTrasnformerの内部でどのようなことが起きてるのか、分析してみると、技術的にFine-Tuningを実パラメータを更新するようなことと同じようなことが起きているんじゃないかと指摘する研究も ります。

[0:25:16 - 0:25:56]
そういうなんで、文脈内学習ができるのかという話その他諸々のなぜという部分は、第11回の講義でお話できればと思います。Promptingその指示出しを工夫することによって、質問ですと日本の人は何ですかと選択肢を与 て答 ってところまで入れて、言語モデルに答 させる、 るいは前提条件も足して げることで、それに沿った答 を出させるというところで、プロンプトをうまく利用することによって、タスクも狙った。

[0:25:58 - 0:26:34]
振る舞いが制御できるかどうかは、プロンプトの書き方に大きく依存します。プロンプトをうまく書かないと、同じモデルで同じタスクを解かせても、全然精度が違います。例えば、ニュースのカテゴリー分類をさせるタスクの場合、そのタスクを表す文章のバリエーションをいろいろ試してみると、性能に30ポイントほどの差が生じるそうです。

[0:26:34 - 0:27:03]
同じモデル名でも違ってくるという点がありますので、プロンプトにどのようなブログを書くかが重要です。本当に細かい違いで、こちらも紹介させていただきますが、質問の間に空白が入っているかどうか、クエスチョンという文言が入っているか、タスクの説明がきちんとされているかという点で、全然性能が違ってくるという報告があります。

[0:27:03 - 0:27:58]
なので、プロンプトをしっかり設計することが重要になります。だってFew-Shot入れれば入れるほど良いのかという点も気になるところですが、最近の研究では、Few-Shotと呼ばれるのが5から10程度だったんですが、最近ROMコンテキストでいろいろな入力文が入るようになって、ひたすら入力部分に突っ込んでみたらどうなるかみたいなところが報告されています。入力文にデモンストレーションを入れて入れれば入れるほど良いというところが報告されていて、タスクによっては、その文脈内学習モデルの重みは更新していないものの、その事例をうまく入れることで、タスク特化のモデル構造で、あるいはタスクのデータで学習させたモデルと匹敵するような性能も報告しています。

[0:28:03 - 0:28:23]
さて、ここまでがPromptingの有名なテクニックであるFew-Shot Promptingについてお話しました。次に、これもまたどのモデルでもよく聞くTuning Soft Prompting、つまり連鎖的な思考を促すPromptingについてお話します。

[0:28:24 - 0:28:51]
このFew-Shotの事例のデモンストレーションのときに、なぜその答えに至るかの思考過程を得るJWebthoughtPromptingの思考過程転移を用いると、新しい質問が与えられたとしても、その答えを出すための過程を出力しながら答え、それがタスクの性能に繋がるということが報告されています。

[0:28:53 - 0:29:48]
日本語でちょっと例を示しますと、こちらが今まで説明したワンスレッドFew-ShotPromptingです。質問が与えられ、答えがこうなります。新たな質問が与えられ、どうなるかというところで、出力だけさせると違うんですが、デモンストレーションのときにただ答えだけを入れるのではなく、その途中の計算結果を入れたりして、その施工の手順を含めてPromptingすると、実際に新しいタスクが来たときも同じような手順で途中経過を踏んで答えさせます。なんでこっちはなんか暗算させてるイメージなんですけど、もちろん実際に出力しながら考慮モデルの思考能力を促進させて、そのため性能が上がるというところが報告されたPromptingになります。

[0:29:50 - 0:30:19]
こちら、様々な数学のデータセットで検証してみると、性能が大幅に上がるという報告があります。特にモデルサイズが大きいときに性能の改善が顕著です。X軸はモデルサイズ、縦軸はタスクの成功率です。一番右のところが顕著に跳ね上がっていることが確認できます。

[0:30:21 - 0:30:34]
なのでチェーンを想定しても、それがどの程度の効力があるかというと、そうでもない部分があります。皆さんが使おうとしているモデルのサイズによって、うまくいくかどうかも変わってくるので、この性質も覚えておいてください。

[0:30:37 - 0:31:24]
チーム総とPrompting思考の連鎖の派生として、Zero-Shot Chain of Thought Promptingと呼ばれるものがあります。これまでの例では、先ほど説明した通りに、その算数の数学の途中過程というのを事例に入れて答えさせるんですが、この事例なしで、ただ「レッツ Sync ステップバイステップ」という文言を入れるだけで、思考過程を踏んでくれるエプロン出力になります。それで性能が上がりますと報告されたのが、この「レッツ Sync ステップバイステップ」という文言で有名なZero-Shot CoTと呼ばれるものです。こちらが松尾・岩澤研のコジマが発表した論文になります。

[0:31:26 - 0:31:49]
これを使った場合、多段階推論を何回かそのステップを踏んで解かないといけないようなタスクにおいて、ステップバイステップと入れなかったZero-Shotと比べてZero-Shot CoTステップバイステップの方がいろいろと大幅に性能が向上します。失敗する例としては、ちょっと考 すぎて失敗する例が多いというところも報告されています。

[0:31:53 - 0:32:15]
こちらどういう文章を入れると、そういう思考の連鎖みたいなのが促進されるかというところでいろいろ調べてみましたと関係ない文章とかも入れてたりするんですが、ステップバイステップとい るのが、その小島さん人力探索で良さそうだというところが言われてました。

[0:32:16 - 0:32:45]
このプロンプトどれが効くかというのを探すのも大変なので、これを自動でプロンプト探索しましょうという提案もあり、タスクの性能が上がった、下がったそのプロンプト分、どのプロンプトを入れるべきかというところを探索して、その結果連続値6PEFTbyStepよりも、ネットワークのthisOUTinStepbyStepDay2ABCIDRAMAlignDancerみたいな感じで入れると、探索結果が良くなる。

[0:32:45 - 0:33:20]
このような報告もされています。ステップバイステップだけでなく、どのように解答すべきかという思考をどのようにプロンプトして、それに沿ってそのフレームワークが良いと性能が良くなるというところも言われています。検査計画を立ててから実行性をやったり、必要な変数を保持するようにPromptingすると、そのようなふうに振舞って性能が上がるという報告もされています。

[0:33:23 - 0:33:47]
ここまでがPrompting入力部にどのように条件づけして げることで、LLM大規模言語モデルの機能を促進させるかというところをお話しました。次にDecodingの工夫というところで、文章を工夫するんじゃなくて、モデルの推論時に工夫する手法を少しお話したいと思います。

[0:33:48 - 0:34:08]
一つがSelf-Consistencyと呼ばれるものです。これは、これまで大規模言語モデルにプロンプトを与えて1回出力させていましたが、それだけでなく、複数回の出力を行っています。

[0:34:08 - 0:34:33]
言語モデルは確率的に文章をモデル化しているため、最もらしい文章をサンプリングによって出力できます。その特性を生かして、同じ問題に対して同じモデルで複数の出力文を生成し、多数決で答えを決定させるというのがSelf-Consistencyです。

[0:34:33 - 0:34:52]
マジョリティーボートとか言われてしますが、これが示唆するところは、最も高い確率を出力すれば答えになるというものではなく、分析に最もらしいものが正しい推論とは限らないことを示唆していることでもあります。

[0:34:54 - 0:35:42]
また、似たような話として、釣り用具装置というものがあります。この装置がどのように機能するか、このグラフで説明しますが、普通のインプットを与えて出力させるというものがあります。チーム総を使えば、このアウトプットに至るまでの過程を出力させます。そして、S、こういうことになるはずだとなりますSelf-Consistencyと呼ばれるものがあります。それを複数作って、多数決でこうなるはずだというのを決めます。トリオ部ソースというのは、それぞれの思考過程というものも分岐させて、どれが正しそうかというのを分岐させて、これが答えになるはずだと選ぶのが強力です。

[0:35:45 - 0:36:25]
こちら、これまでの方法と同じモデルですが、何回も推論させることで、生の加除向上させる手法になります。そうすると、これもいいことになります。1回だけの推論だと、一番左下のスコアですが、そのモデルを推論させる回数を増やせば増やすほど、これは数学での性能というところで上がっていくというところです。つまり、モデルをうまく活用するという技術としてのDecodingの工夫をすることで、大幅に性能を向上させるということも報告されています。

[0:36:28 - 0:36:48]
はい。ここまでのまとめです。まず言語モデルの入力プロンプトを工夫することで性能が改善します。コンテキストから学習モデルのパラメータを更新していない状態で同じモデルを使用していますが、その入力文によって学習しているように見るため、文脈内学習とも呼ばれます。

[0:36:49 - 0:37:05]
そしてプロンプトがなかったり、デモンストレーション例がなかったり、その細かい文言が違うことで性能が大きく変わることもありますので注意が必要です。プロンプトテクニックとしては、Few-Shot Promptingなどが挙げられます。

[0:37:06 - 0:37:19]
そしてさらにデコーディングの工夫として、セルフコンシステنسيやツリーオブソースというものがあります。特にチェーンソーとプロンティングを使うと、推論能力が大幅に向上しているという報告があります。

[0:37:21 - 0:37:54]
これらをさまざまな方法でまとめた論文や、個別タスクをどのようにPromptingで解くかについて知りたい方は、ぜひ文献を参照してください。ここまでの話のちょっとしたおまけなんですが、これらを使いすぎるとどこまで性能が上がるかというところで、目処プロンプトと呼ばれる手法があります。Zero-Shotのときの性能は81.7点ですが、Few-Shotを導入したり、このFew-Shotにどのような例を導入すべきか、近い例を持ってくることも重要です。

[0:37:54 - 0:38:15]
そのときに何か情報をリトリーブしてくるみたいな感じなんですけど、入れてみてさらに何個か作ってみて、マジョリティになると、10ポイントぐらい性能が変わってきますよというところも報告されています。今までの話を踏まえると、この論文も読みこなせるかなと思うので、ぜひ見てみてください。

[0:38:17 - 0:38:43]
さらに、LLMを使ってPromptingを行うことでさまざまな機能を促進できるんですが、それを使って、LLMに分を評価させたり何かしら評価させるということもされています。既存のベンチマークでは評価が難しいものも、LLMを使用することで、人間の評価と一致するようなものを作成できるので、LLMに評価させるような流れもあります。

[0:38:45 - 0:39:07]
さらにさらにモデルの振る舞いをちょっとプロンティングによって攻撃してみたいところですが、攻撃する手法やその爆弾の作り方のような危ないことを言わせないようにモデルが訓練されているんですが、こういう謎の文字列を出すことによってペラペラと喋ってしまうみたいなことも言われています。

[0:39:07 - 0:39:38]
ところで、Promptingにはいろんな機能があります。すごい不思議な部分も多いんですが、それもいろいろ研究されてますというのが、おまけ程度の紹介になります。もっといろんなユースケースでどうPromptingすべきかみたいなところは、アンドリー円先生という、ディープラーニング・AIという企業での人がいろいろ講座を開いてるんですが、そちらへ松尾・岩澤研が中心となって翻訳した。

[0:39:39 - 0:39:56]
コースが るので詳しく知りたい方はぜひ動画で知りたい方はぜひ見てみてください。はい。前半部分Prompting2については、一旦終了したのでここで質問の時間をとりたいと思います。が、川崎さんお願いしたんですか。

[0:39:57 - 0:40:12]
はい、 りがとうございます。まずですねなんか若干資料が違うというような指摘が ったんですがこれって最新版になってますかね。最新版はまだアップロードしてないですちょっとそのときギリギリまでやってたのですいませんなるほど。

[0:40:12 - 0:40:25]
はいとのことなのですいません受講生の方ちょっと若干ページが合ってないと思います。が、ちょっと行ってこの講義終わってからですね、共有しますので、しばらくお待ちください。なんで講師の の画像、 の画面を見ていただければと思います。

[0:40:25 - 0:41:09]
よろしくお願いします。はい。では質問いくつかいただいててるので、回答できればと思いちょっと僕の方で画面共有していいですか。はい。はい。はい。ちょっと先ほどご案内した通りですねこちらのチャットボットで木庭と解決しなかったというフラグを立てていただいた方の中からちょっと直接講義の中で関係 りそうなやつをピックアップして、ちょっと優先度づけをさせていただいてますなので時間って何分ぐらい るんでしたっけ原さん、5分ぐらい るかなこの質問ってことですか。

[0:41:09 - 0:41:40]
そうですねはい。全然Gopher大丈夫ですはい、いただいてるやつ回答できればと思います。ちょっと下から古い順でしたから回答していいただければと思います。まずここですね連載してどんどん掛け算していくとそこはさ、小さくなりますよねという質問ですが、どうでしょうか？検査率で自己回帰玄知子回帰の話で掛け算したスコアスコアって呼んでるのが、文章の最もらしさという感じなんですか。

[0:41:40 - 0:42:02]
でも他の組み合わせと最もらしさと比べると、長さによって、その1分の生成確率みたいなのが短ければ短いほど高くもなってくるとは思うんですがそうですね掛け算をしていくので、スコアとしては小さくなるというのはその通りです。

[0:42:02 - 0:42:53]
はい。はい。 りがとうございます次です。はい。ここ半年で開発するPrompting賞について過剰箇条書きで答 たのうち各項目の概要を教 てくださいと りますこれ既に何かいくつか回答しているんですが、これだとまだないのかな何か補足等 れば、原さんよろしくお願いしますいろいろ るんですが、ジェネラルにというか当たり前にするのはFew-ShotPromptingと転移ofthoughtPromptingで他にいろいろ発生 るんですがそれはタスクごとにTuningしたPromptingかなと思います。なので、どんなタスクでもジェネラルに聞くというのはその二つでタスクごとにどういうふうに解いてで ったりとかそういう工夫がされるのかなと思うので、タスクごとによってはいろんな進展は りますという回答にします。

[0:42:53 - 0:43:29]
はい、ありがとうございます。では、次に進みましょう。ここでは、プロンプトで同じ文章を入力しても回答が異なる理由について質問があります。そうですね、これは言語モデルからどのように出力を得ているかに依存しますが、その言語モデルは確率をモデル化しています。具体的には、次の単語に続くスコア、つまりその確率を計算し、最も可能性の高いものを選んだり、それぞれの確率に基づいてサンプリングして出力しています。

[0:43:30 - 0:44:07]
もちろんです。設定によっては、毎回同じ回答にすることもできます。Greedy Decodingと呼ばれるものは、次の単語に関して最も高いスコアをひたすら取っていくというものです。ただそれだと、多様性が失われることもあります。そのため、タスクによっては向かない場合もあります。例えば、クリエイティブなタスクでは、いろいろな候補が欲しいと思います。そうなると、サンプリングによって、いろんな確率の山からサンプリングして出すというところで、ニューラルサンプリングなどを使ったりします。

[0:44:07 - 0:44:37]
なので、デコーディングの方法によって同じことにもできますし、違うことにもできます。という回答にさせていただきます。はい、ありがとうございます。言ったんは、一応確認できます。ちょっとこっから上のやつをピックアップして回答してもらいますかね。ここの範囲でちょっと見づらいので拡大できない。

[0:44:42 - 0:45:58]
どれがいいかどれも面白いんですが、プロンプトやプロンプトの話で、Promptingプロンプト自体をLM生成させるアプローチに興味があります。はい。それは難しいというわけでもなく、タスクが自分自身で準備したタスクを探索するのは難しいところもあるかもしれません。その評価というかスコアが出てこないと、そのスコアをもとに、モデルプロンプトを工夫したり、そのモデルの重みにアクセスできるか、また、自由度が違ってくるんですが、先ほどその列シンクステップバイステップじゃなくて、その探索モデル自身に探索させることによって、新しいプロンプト文を選びましたみたいな感じで、何でしょう、実現不可能な技術ではない、その、もう一番これがうまくいくみたいなものを探すとすると探索空間が広すぎるんで、これが一番うまくいくPromptingだみたいなところ厳密に言うのは難しいかもしれませんが、今、Promptingをさらに良くしていくというところは、実用化されてるかなと思います。

[0:46:01 - 0:46:41]
りがとうございます。ちょっと時間 りますかね。 と1問ぐらい行きましょうかよ。マジョリティーボーディングで最終的にそうですねLLMにというかその回答、さっきの論文中だと、その回答時なので答 は18とか答 は21みたいな感じで数値として出てくる問題だったのでその数値の一致率というか、5個中何個中8と言ったかみたいな感じで多数決になるんですけど、そういうことは難しい。

[0:46:41 - 0:47:10]
場合ですと、LLMにその回答を評価させるどれが一番いいかというのを評価させるで ったりとか別でモデルを組んで げて、その文章の良さを評価するモデルRLHF会で後半の会でリワードモデルという話が るんですけど文章に対してどれぐらい好ましいかというスコアを出すモデルを学習させることによってそのスコアをもとに選んでいくという手法が ります。

[0:47:13 - 0:47:30]
はい、 りがとうございます。一旦質疑は終了でいいですかねまたお戻しするので、引き続きお願いします関係ないすけど、1000名受講者超 ましたね1人減っちゃった。はいなんてまずはいい感じだと思います。はい。

[0:47:32 - 0:48:07]
はい。では後半の話ですね。これまでは、一つの言語モデル（LM）でLLMに指示を出したり、そのデコーディングの工夫でよくする話でしたが、性能を良くするという話でした。次は、LLMだけでなく、外部ツールや外部知識を利用することで性能を良くする手法についてお話します。ここでは、この概念というのは、RAGやツールユース、あるいはエージェントと呼ばれるものに該当すると思います。

[0:48:09 - 0:49:13]
芸名フィット言語モデルを大きく分けると、Retriever-Argument Language Modelツールやゲームといった言語モデルがあります。それぞれについてお話していきます。まずは、外部知識やツールを使いたいというモチベーションですが、大きく三つほどあります。一つ目は、タスクを解くのに必要な知識や能力が多様であるため、LLMだけでタスクを解かせようとした場合、そのLLMにその能力をすべて保持しておく必要があるという問題があります。そうすると、結構大変で、学習も大変になります。知識や外部のツールを活用することで効率よくそれを防げないか、そのツールを使うことでタスクを成功できないかというのが考えられます。例えば、小島さんがプレッシングステップバイステップで算数の問題を解いたところ、間違った問題を解いてしまいました。単純にその計算ミスで、すごく間違えていました。

[0:49:13 - 0:49:29]
計算ミスさえ合ってれば、計算さえ合ってればその過程みたいなのがあれば、成功しますよね、みたいな感じで、そのLMじゃなくても解けるような問題というか、LLMが苦手なことは、それが得意なものにやらせようというところが重要です。

[0:49:31 - 0:49:57]
もう一つは知識の更新です。知識に誤りが含まれているときに、どのように修正すればいいかということです。そもそも、LLMにどのように知識が蓄えられ、それをどのように修正するかというのは、非常に大きな研究課題であり、今も研究が進行中です。アドホックに修正するのは結構大変です。さらに新しいものを追加したいという場面もあるかと思います。

[0:49:57 - 0:50:16]
例えば、大谷翔平選手が何本ホームランを打ったかというのは、日々刻々と変わっていく事実だと思いますが、それを学習データから、その事情が変わった事実をうまく反映させるためにはどうすればいいかというのが問題です。

[0:50:18 - 0:50:45]
そして信頼性という点で、Hallucinationという大きな問題があります。言語モデルは誤った知識を持っていても、最も自然に話してしまうため、そのような問題が生じます。そのため、閉じてしまったサービスもあります。attention is all you needというのも、このような歌の歌詞のようなことを堂々と言っているようですが、そのような事実はありません。最も自然に話してしまうというのが問題なのです。

[0:50:45 - 0:50:59]
LLMにだけ生成させるんじゃなくて、何かしらこの何を元に生成させているのかというのをうまくコントロールさせて生成させたい、信頼性を向上させたいというニーズもあるため、外部知識ツールを使いたいというものがあります。

[0:51:01 - 0:51:44]
そこでまずはRAGについて説明したいと思います。RAGとは、RetrieverとGeneratorを組み合わせたモデルです。具体的には、MetaGenerationというのがRAGなんですけれども、これは検索と言語モデルを組み合わせたモデルです。例えば、このような入力文が与えられたときに、それに関連する文書、つまり知識データベース（通常はテキストデータとして保持されているもの）を検索するRetrieverという機能を持つものがあります。検索した上で、元の入力と検索された文章を組み合わせることで、LLMに出力をさせるというのが大まかな流れです。

[0:51:46 - 0:52:26]
ここで重要なRetrieverという役割が何をしているのかについてお話します。Retrieverは、必要な文書をどのように取り出すか、そのための機能を提供します。Retrieverの役割は、外部のデータベースから、与えられたQuery入力に類似した文書を見つけることが主な役割です。類似するものを見つけるか、類似度を測ることで大きく分けて二つあります。一つは、ATFIDFと呼ばれる手法に代表されるようなスパーサな表現です。

[0:52:26 - 0:52:56]
これがその単語がこの文章に含まれているかどうかを確認するところで、その文章にこの単語が含まれている場合、大きな行列を作成し、ベクトルを作成することで、その表現をRetrieverケースします。有名なTFIDFもよく使われますし、Retrieverのベースラインとしてよく使われるのが、DM25と呼ばれるものです。

[0:52:57 - 0:53:37]
もう一つは、ニューラルネットワークの表現空間で類似度を測るような、DenseRetrieverと呼ばれるものです。こちらは、その埋め込み空間で一度用いてRetrieverして、このベクトル空間に埋め込むためのモデルであれば何でも良くて、BERTやSentence-BERTと呼ばれるもの、あるいはOpenAIが提供しているエンベディングAPIと呼ばれるものも、このようにベクトル空間に押し込んでいます。密な表現のニューラルネットワークによって、セマンティックな意味的な情報もうまく埋め込むことを期待しているのです。

[0:53:38 - 0:54:18]
そのようなセマンティックな検索には、良いと言われています。Retrieverが満たすべき要件として大きく二つあります。一つは、重要なキーワードを含んでいることという点で、Queryに入力されたようなキーワードが出てくる文章を持っているかどうかというのが、まず重要です。持っていなければ答えられないもの答えられないため、重要なキーワードがQueryに含まれているかどうかが、SparseRetrieverが得意とするところです。

[0:54:18 - 0:55:38]
もう一つが文章の意味的な類似度を反映しているかという点で、同じキーワードを含んだ含めなかったとしても、似たような概念で説明しているかどうかを反映しているので、質問に答えるような文章が含まれているかどうかも重要です。そこで、DenseRetrieverが得意としていると言われるところです。ニューラルレトリーバーのDenseRetrieverは、ニューラルネットワークの表現空間で埋め込むための手法がいくつかありますが、よく使われている方法の一つは、OpenAIのエンベディングAPIでベクトル化して検索する方法です。これは、Queryとドキュメント、つまり蓄えられている知識データベースそれぞれをモデルにかけて、一つのベクトルに圧縮し、埋め込んで類似度を測定し、関連性があるかどうかを判断します。この方法では、ドキュメントを一度埋め込めば、Queryが来ればそれを埋め込んで検索するだけで済むため、計算コストが低くなります。しかし、文章のトークン列を一つのベクトルに圧し込んでしまうため、表現力が低下してしまう問題があります。

[0:55:39 - 0:56:17]
それに対して、クロスエンコーダというものがあります。Queryとドキュメントの両方を同じネットワークに入れ、入力としてTransformerで処理し、最終的にスコアを出すようなものです。こちらでは、マイドキュメントに毎期マイドキュメントのQueryが来るため、表現力は高いですが、計算コストが高いというものです。それらの良いところを取り入れたものが、コールBERTと呼ばれるような手法で、最終層の表現空間でうまく選んでいくというものです。

[0:56:18 - 0:56:59]
これらの選択肢はいろいろあるので、迷ってしまうと思うんですが、それはもう、計算コストとお金どれぐらいかけられるかのトレードオフかなと思います。DM25TASIDSのようなスパースなものだと、別にGPUがなくてもCPUだけでもすごく安く早く検索できますというのが売りなんですが、ベンチマークで検索結果のような評価すると、スコアは低くなります。DPRとかコールBERTと呼ばれるような手法は、どうしても計算時間がかかってしまうんですが、スコアというものは高くなります。

[0:57:00 - 0:57:18]
こちらCPUだけの計算結果なんですけどこれを、GPUを積んで げてCPUも積んで げると、確かに速度は速くなるんですがお金が高くなりますなのでどこまでお金をかけてどこまでパフォーマンスを上げたいかというところでどうRetrieverを使うかというのが一つ。

[0:57:19 - 0:58:04]
選択のデザインかなと思います。そして、Retrieverというものを1回だけ使うわけではなく、複数のRetrieverを組み合わせる手法もあります。それがDRAMと呼ばれるもので、大量のデータを素早く取り出します。例えば、BM25やPFIなどのSparseRetrieverで素早くトップ100を持ち込み、その上でこのトップ100に対してコールBERTやDPRエンベディングAPIなどを使って、トップ10を選びます。このような形でRetrieverを何回も様々な種類を使って、それぞれのいいところを取り入れましょうというのがDRAMと呼ばれる賞です。それでうまく検索結果を上げましょうというものがあります。

[0:58:05 - 0:58:21]
こちらもLLMにさせるという提案が近年増えており、LLMにプロンティングすることで、どの文章が最も関連しているかを順位付けして出してくれるような機能も実装されています。

[0:58:23 - 0:59:15]
ここまでがRetrieverで検索する部分です。その検索した結果をどのように活用するかというのがジェネレーションです。これもいろいろな流派がありますが、有名なものをいくつか紹介したいと思います。一つが、REPLUGと呼ばれる手法で、検索した文章を元の入力にくっつけてコンテキストとして入力する方法です。これがよくRAGと呼ばれるようなものです。RAGは入力部に加えて入力空間で条件付けを行うものです。複数の場合には複数の予測アンサンブルすることもできます。入力文に入れるんじゃなくて、その途中のネットワークの表現空間で条件づけしたりというさまざまな流派があります。

[0:59:18 - 0:59:55]
そのように条件付けして げてコンテキストで条件付けして げると、何もしないRetrieverるというのが の、ただ単にモデルにQueryを投げただけの場合と比べて検索結果を一緒に使った方が、このスコアが下がるほどいいんですけど、この赤のものを見てみると、DM25で持ってきた検索結果を一緒に整理をさせて げると、どのモデルでもスコアが下がって性能上がってますと呼ばれ、言われてるので、RAGっていいですよねというところが言われてます。

[0:59:57 - 1:00:28]
モデルサイズを大きくしても性能が改善されました。つまり、Retriever LLMを学習し、RAGの性能を上げるときに、現在のままでは使わないで、RAG RetrieverとしてGenerationを行うそのパイプラインでうまくいくことが重要です。そのため、Fine-Tuningを行う必要があります。

[1:00:28 - 1:00:56]
それでRetriever Fine-Tuningすることもできますし、LLMをFine-Tuningすることもできます。性能を上げるためには、そのような工夫も必要です。そして、ジェネレーションLanguageModelに生成させるだけでなく、RAGモデルが生成した結果を少しキャリブレーションして、修正する方法も存在します。こちらはKNプロンプトと呼ばれる手法です。

[1:00:59 - 1:01:51]
それだと、性能も上がりますよね、という点が言われています。RAGとRetriever-based Language Modelといっても様々な種類があります。どのようにテキストを着火させるのか、どのようにRetrieverをRetrieverしてきたものを使っているのかというところを、入力部に条件付けさせたり、中間層で条件付けさせたりとか、何回に1回RetrieverをRetrieverするのかみたいな感じで、本当に様々です。それぞれに良いところも悪いところもあるので、コストとパフォーマンス、どうなるかという点もあるので、そのタスクに応じて選ぶべきかなと思います。

[1:01:54 - 1:03:04]
松尾・岩澤研では、論文を解説してくれるボットが常駐しており、リンクを投げると、要約をしてくれてさらに質問論文に関する質問もSlack時計で聞けるものがデプロイされています。僕自身、質問回答にRAGを使わないかというところで発表したこともあります。詳細は割愛しますが、質問文をどのように組むかというと、質問文タイプ分けします。これが実際に去年得られたような質問を、ちょっと擬似的にいじった分文章なんですけど、こういう質問文があれば、タイプ分けすると、こういう内訳になります。これらの質問に答えるには、どういう回答が必要なのかを考えると、そのトピックが何に答えて欲しいかというトピックを特定して、それを説明し、それに関連する文章を渡すというのが重要だと考えています。

[1:03:04 - 1:03:45]
それがどのタイプでも効くかなと思ってパイプライン組みました最初が質問文が与 られるので、それをどのテーマについて答 るべきかというのをワンショットPromptingによって検索クエリ化しますとさらに大規模言語モデルの文章って結構英語のものが多いので英語で検索クエリが出るように、海外の文章にヒットするようにしますとそれをGoogleの検索APで使って、記事引っ張ってきてそこから該当する文書を引っ張ってきますとそれを含めて答 させるみたいなことを組んでみましたという形です。

[1:03:47 - 1:04:22]
そのようにRAGのさまざまな応用が可能だと、本当にデザインの余地が広がります。これだけの応用で期待されている部分もあるため、いろんな質問がくると思います。業務特化のものを作りたい場合には、ちゃんとそのデータベースをきちんと作ることが大事かなと思います。SAKeyをひたすら貯めたり、こういった質問が来そうだなという含まれるドキュメントが検索されないか、焦りやすい形にならないか、メタデータがつけられるのかなど、そういうことが大事かなと思います。

[1:04:23 - 1:04:42]
さらにベクトル検索を行うので、文書が多いと、その点がボトルネックとなり時間がかかってしまいます。そのため、ベクターDBと呼ばれるような技術でベクトル検索を高速化するための技術も参照してください。

[1:04:43 - 1:05:02]
タスクの性能を上げるためには、学習なしではなく、RetrieverやLM（Language Model）を学習することも一つの手です。最近の話題としては、Language Modelの入力にたくさんのデータを投入することが増えています。

[1:05:02 - 1:05:25]
最近のGeminiGoogleが出してるチーム内というモデルでは、100万コンテキスト入りますと一味1Milliontoken入りましたこれがどれぐらいの長さというと、70万文字文字でいうと70万文字で ったりとか、講座で った3文字とか11時間の音声データとか1時間の動画が入るようになりました。

[1:05:26 - 1:06:11]
こういったものが出てくるので、ログコンテキストそのLLMの入力部にすべて入力すればそれで終わりなのか、ロングコンテキストが出てくるような感じで、ちょっとネットで騒がれることもあるんですが、よくよくちゃんと実験してみたりすると、モデルの進化も早いのでいろいろ課題も解決されているとは思うんですが、報告されている問題としては、例えば「Lost in the Middle」と呼ばれる有名な現象があります。その文章、その正解文書がその途中で20個の関連文書を入れて、その中の一つだけが正解文書ですと、それを踏まえて答えられればいいんですけど、その文書がどこにあるのか。

[1:06:12 - 1:07:18]
真ん中に置かれると、それを参照していないその文章がなかったときの性能に比べて落ちることもありますというか、何か混乱してしまい、変なこと言っちゃうこともあるんですね。つまり、入力部に入れれば性能が上がるかというと、その入れ方も、どれぐらい入れるかというところも問題になります。さらに、踏まえるべき事実というものが、どこにあるのか、踏まえて答える必要があるのか、一文のInt形式で、この住所は何、その住所がプロンプト中にバッチと書かれているので、簡単に答えられるというのは、あるんですけど、踏まえるべき事実が複数あると、それを全部踏まえて答えるようなタスクだと難しいですよね。というのも、関係ない文章があると落ちていくというところも報告されているので、コンテキストで、とりあえずぶっ込めばいいわけではなくて、細かいデザインの必要性があるかなと思います。

[1:07:18 - 1:07:38]
ただ、Google ColabやOpenAI社などのモデル提供者側では、Fine-Tuningなどの対策を講じて問題が起きないようにしていると思われるので、モデルのバージョンが上がるごとにそのままの問題はなくなるかもしれないんですが、RAGにおいては、関連する文書をいかに取り込むかという点が大事かなと思います。

[1:07:40 - 1:08:59]
今回話しきれなかった話というのは、いろいろおすすめしてあるのでぜひ見てみてください。扇面とRAGMLの最後の部分でスツールユースとエージェントについてお話します。ツールユースのイメージなんですけれども、こちら天気のAPI、お天気の東京の天気がどうか、今日の天気はどうかというところを聞くようなツールがあれば、決まった形式でロケーション東京として挙げて、このKeyとValueで投げて、ウェザーが晴れですと返ってくるよね。LLMを活用してこのインターフェイスに合うように政府データにして、それ整形することで、ツールを使いますと、ツールを使って、それでこういうふうに返ってきます。この出力結果をうまく使って次の推論に繋ぎ合わせるというのが大まかなイメージです。そしてLLMにツールの使用までを判断させたり、またLLMに大きなタスクを割り当てられて、それをサブタスク化して、そのサブタスクがそれぞれのAPIで解けるような形でタスク化することによって、大きなタスクでもうまく解くようにするというのが例えば、エージェントのイメージです。

[1:09:02 - 1:09:32]
では、既に皆さんにも馴染みのあるものかもしれません。例えば、コードの実行環境をツールとして組み込んだ例として、Codeinterpreterというものがあります。ただ、文章だけでサインプロットを書いても、綺麗な図を作成することはできないかもしれませんが、この3波をプロットするような行動を書くことで、その行動を実行した結果をユーザーに提示して、綺麗なサインプロットが作成できるようになるかなと思います。

[1:09:34 - 1:09:57]
このようにすでに実世界でデプロイされている講義面といったLanguage Modelですが、研究分野ではいろいろあります。まず用語の整理として、知識だけでなく外部ツールで拡張されたモデルをツールアグメントインテグレーションLanguageモデルと呼びます。その使われ方としては、生成を修正したり、補強したり、全エージェントを操作したりします。

[1:09:58 - 1:10:23]
外部ツールの例としてはよく使われるのは検索、コード実行環境、特定のAPIまたは別のモデルニューラルnetだったりします。少しそれぞれの研究を簡単に説明していきますと、チェーンソーと計算結果が計算ミスで間違 ちゃうというところが りましたと。

[1:10:23 - 1:11:14]
その計算をすべてPythonでプログラミング実行環境を用意して、そのPythonプログラムに任せたら、計算させるのは実行環境でやって、その結果を戻すようにしましょうというのが、このパールプログラミングプログラムηLanguageModelと、10M思考の連鎖のときにそれぞれの計算式みたいなものを、プログラムコードで働かせるようにPromptingして推論するときに、そのコードを実行させた結果を返すようにするということで、結果を得て性能が上がりました。計算結果が間違っているために後ろの結果が間違ってしまうというのが主だった問題だったので、そこが改善されることで性能が上がりましたよね。

[1:11:17 - 1:12:05]
プログラミングを実行環境として用意して、RAG Language Modelはそのプログラムを書く能力も優れていますので、このAPIを使用するみたいな説明書があると、それに従って、このような行動をしたいときに、このAPIを使って、プログラムコードで出力させることで、何か人間がボールの中にブロックを積み上げたいような自然言語として与えられても、このAPIを組み合わせて一連のコードを書いて実行することで、さまざまなタスクを実行できるようにしましたというところが報告されています。

[1:12:08 - 1:12:38]
さらに、プログラミング実行環境だけでなく、様々なツールがあっても、特定の文字列でツールの利用方法を教えるというものがあります。例えば、検索キーワードや括弧、その他のケースを指定すると、それらを検索するようにプロンティングして、それが出てきたら、ツールを実行して返すのです。

[1:12:38 - 1:13:13]
それを組み合わせることによって、ツールを利用させるというのが可能です。このように自然言語文によってツールを使わせるというのも可能ですし、その使い方というのを、ただのプロンプティングだけじゃなくて、特定の文字列でそれが選ばれるように、そのタスクごとにファインチューニングするというのも可能ですし、文字列じゃなくて、スペシャルトークンそのトークンとして、また詳しい話は多分来週からですけど、言語モデルが扱うトークンの単位として、それを呼び出すことができます。

[1:13:14 - 1:13:38]
Fine-Tuningを行うことで、ツールの呼び出しが可能になりますという研究があります。さまざまなツールを組み合わせることで、さまざまな機能を実現できるようになります。鴨井遼という研究者が新しい研究として、これもできるんだと感じるかもしれません。

[1:13:38 - 1:14:21]
さらにさらに、LLMがツール自体を作り出すという研究も存在します。そしてまた面白いのが、LLM自体をツールとして捉えて役割分担してタスクを解くという流れです。つまり、同じLLMを使用していますが、プロンティングによって特定の役割を担うように設定することで、タスクを分担します。入力文が与えられたときに何かしら答えを生成する、タスクの基礎となるモデルがあって、それに対して出てきた結果に対して、ここを直す、ここを修正すべきだというフィードバックを行います。

[1:14:22 - 1:14:53]
Promptingによってフィードバックをするように機能させたLLMが りますと、そのフィードバックを活かして、回答を修正するLLMを作りますとこれをぐるぐる回すことで回答をよくしていきますと、同じ全部ベースのモデルは同じなんですけど、Promptingによって役割分担をして げることで、どん結構いろんなタスクでですね、この役割分担してタスクを解かせることで、出力性能が簡単に上がりますよと言われてます。

[1:14:55 - 1:15:10]
最近の研究ですと、AIサイエンティストと魚AIと日本の会社が最近話題この論文発表して話題になったんですが、この研究のワークフローというのを、LLMのエージェントによって自動化するということが ります。

[1:15:10 - 1:15:37]
研究のワークフローいろいろアイディア考 て、新規性 るか。このアイディアってどれぐらいいいかなみたいな。実験して、実験結果こうだから、もっと実験しようとか って結果を得ますと、それ得て論文化しましょうという、そういう一連のプロセスが るんですけどそれを全部LLMに、 とツールの利用もして、自動化しましたこういう論文ができましたみたいなところが報告されていたりします。

[1:15:40 - 1:16:14]
このように論文で報告されていることもありますし、手軽にツールを使ったものを自分で試してみたいという場合、LLMをAPIとして提供しているサービス会社は、自分でそのツールを使うようなエージェントを作る機能をノーコードで実現できるようにしています。そのため、そのようなサービスで提供されているもの、例えばGoogle Colabだとか、それで実際に使ってみるのもおすすめです。

[1:16:16 - 1:16:51]
Microsoft社もそのようなツールを出しているようです。もちろん、プログラミングで自分でツールを作って、別のLLMを呼び出すような複雑な処理をさせたい場合、結構コードを書かないといけないかもしれませんが、APILMをAPIとして提供しているところは、Functionコールやツールユース機能で、こういった機能を提供しているので、そちらも見てみてください。

[1:16:54 - 1:17:18]
それぞれの話題でも、広いのでまずは講義メントIntLanguageModelというところで言語モデルをどう拡張するか言語モデルが何が得意で何が苦手なのか、そういう課題を知る。その上でどう設計すべきかというところでRAGサーベイ論文が るので、LMどう拡張できる売るのかな、いろんな事例を見ながらする場合にはこちらの論文を見てください。

[1:17:18 - 1:17:57]
そして、Retrieverを使って質問回答させたいという場合であれば、有名な良いチュートリアルもありますので、そちらもどうぞ。ツールとエージェントに関しても、ぜひこれらの論文を参考にしてください。はい、画面とLanguageモデルのまとめです。ランゲー言語モデル協力なんですが、計算ミスしたり、知識が更新するのは難しかったり、Hallucinationしたりという課題があります。それらを克服するための枠組みとして、Maintenant LanguageModelというものが登場しました。

[1:17:57 - 1:18:25]
Retriever検索とRAGモデルを組み合わせたものは、どのようにゲームエントランスモデルでRetrieverを活用するか、Retrieverした情報をどのように活用するかについて、様々なデザインの余地があります。外部ツールを活用することで計算効率を高め、推論結果を改善したり、外部のモデルを活用することでタスクをより高性能に解決することができます。

[1:18:27 - 1:18:50]
いうのが後半のまとめテストで、練習エヴェルトンさんがこれからする練習は実際にPromptingKeyを学んだFew-ShotプリンティングやチェーンソーとPromptingを実際に自分で実装しています講義MetaとRAGモデルの一種で るRetrieverRAGMetaGenerationのパイプラインを実際に組んでますというのが練習になります。

[1:18:51 - 1:19:16]
講座講義の部分は以上で残りは質疑応答で、その後演習に繋げたいと思います。はい川崎さんよろしくお願いします りがとうございます。ちょっと質疑の時間にまた行きたいなと思うんですがなんか思ってたよりかなりね、大量の質問いただいてですねちょっと絞り込みできなかったんで逆に原田さんの方で今リンク貼ったんでこれ駄目かな。

[1:19:16 - 1:19:34]
ちょっと消しました。ちょっと内部で共有しますがつつそのリンク踏んでもらって、何か自分で自分でというか良さげなやつを、画面共有しつつ回答いただけると りがたい りがたいかなと思います りました。ちょっと今、はい、内部チャットの方で共有しております。

[1:19:34 - 1:20:00]
はい。どこですか。どこのLMのやつですね。Slackです。メンションしました。3見れるかな。アクセスできますか。見れそうでした。画面共有していただきます。 りがとうございます。残り10分ぐらいは質疑応答できそうで答 られるからすごいっす。

[1:20:05 - 1:20:26]
そうですね、はい。なるほど、どうしましょうか。上から見るとどんどん進んでしまいますね。Processor内は新しい順に並んでいるので、後ろから進めばいいのかもしれません。そうですね。途中まで優先度をつけてたんですが、ちょっと諦めてしまいました。

[1:20:27 - 1:21:26]
途中からですね、後半の話にちょっと文章ではなく、数値や画像のようなRAGの使い方、もちろんマルチモーダルにRAGするそのモデルもあるので、画像をもとに答えを出したり、動画の中から検索して答えを出したりと、できるようになってます。現状どれぐらいできるのかみたいなのは、GoogleのGemini 1.5のテクニカルレポートで結構面白い例が紹介されていて、例えばこの映画、映画の映画とかムービーをプロンプトに突っ込むんですけど、その映画でこういうことをしてるシーンはどこかみたいなところを答えさせたり、小説をまとめさせたりとか、音声を入力にして答えを出したりともいいですね。

[1:21:26 - 1:22:06]
マルチモーダルなRetrieverの開発も行われています。RAGの将来性について何か考えていますか。RAGを使わずに、はい。これはちょっと紹介しましたが、Lost in The Middleのようなものもありますし、来週詳しく説明する予定です。しかし、入力量が増えるほど計算コストが増えるという事情で、質の高い必要な文章を小さく絞ることでコストを効率的にするために、運用上、性能上、大事だと考えました。

[1:22:07 - 1:23:45]
ただなんですよ、既存ほど頑張らなくても、答えられるような感じにはなってくるかなと思います。が、やっぱりいかに良い文章のいいところを引っ張ってくるかってのは重要だと思います。プロンプトでRAG様に追加でインプットした情報から、外部情報だけを二値化するのは難しいですよね。今日も何かそういう論文が出てた気がするんですけど、そのモデルの内部の情報と、コンテキストに与されたRetrieverされた文章、どっちを優先して答えるかみたいなのは、結構難しいというか、そのメカニズム自体も解明するのは難しいんですが、やろうとすると、RAGも今はRetrieverして生成するだけだったんですけど、もうRAGの評価でもよく使われるんですが、出てきた文章というか、そもそも非評価の話してないんで難しいんです。ちょっとはしょっちゃったんですけど、出てきた文章や生成した文章が検索したものと一致してるか、それを含んで答えているかというのを、後段で処理するというか、それで何か変なこと言ってないかみたいなところを評価したりもするんですよね。それでそれをうまく工夫することで、なるべくその外部情報を参照して答えさせることもできるかなと思います。

[1:23:46 - 1:24:01]
さらにジェネレーション言語モデルにさせると、そのようなことも起こり得るので、3章分、その経年のようなプロンプトで3章分をうまくモデル化、3章分だけでモデル化することは可能かなと終わります。

[1:24:03 - 1:24:58]
これは、解決したのか、解決しない。一旦飛ばします。いい。どうやってデータセットを作るかですよね。よく作られるベンチマークというのは、Googleの検索窓だったり、ユーザーがタイプして持ってきたもの、それをクエスチョンとしてまず担保し、持ってきたものを保持しておきます。どのリンクが正しいか、というのも後から後付けでやって…。

[1:24:59 - 1:25:57]
というところで検索エンジンを使うことで、RAGのRetrieverの性能を評価したりとか、タスクは何でしょう。難しくなったりとか、路面がすごく固定されればされるほど、どうしても人間の手で作っちゃわないといけないので大変な部分はあると思います。これを合成データセットというか、LLMに何か関連してる関連してないとか、これで解決した解決してないみたいなシミュレーションさせて、擬似的にバーって作ってみてそれを何ですかね、いろいろなやり方がありますが、そういうLMに判断させてLMTheAzureジャッジを使って、ゴールデンデータを作る方法もありますし、元の、何でしょう、こういう文章があれば、この文章に関連する質問を作らせますと。

[1:25:57 - 1:26:14]
そうすることで、この文書を参照して答えられるような質問文をLLMに作らせてみて、そうすることで、この文章をもとに答える質問リストを作り、それをページごとに作って、かさ増しするようなこともできるのです。

[1:26:14 - 1:26:55]
なので、そうですね、そのような方法とかありますね。アーカイブIntアプリと紹介して、結構間違えたり嘘を言ったりすることもありますが、研究者が実際に論文を読むので、ざっと理解していればいいという感じで、理由は曖昧なままなので、それを完全に信じることはない気がします。しかし、問題意識としては、それを広く様々なユーザーに使ってもらおうとすると大変かもしれないですね。

[1:26:55 - 1:27:25]
なので、この回答根拠がどこから引っ張ってくるかみたいなのリンクとして、差し込んでそこをすぐ参照できるようにして、本当に合ってるかどうかを人手に委ねて げるとか、そういう工夫も ったりすると思います。なのでシステムとして完璧にHallucinationすることってのは難しいのでいかにそれを何でしょう、カバーできる、その機構をシステムとして組み込むかみたいなのが一つ重要なものかなと思います。

[1:27:25 - 1:28:24]
データの構造化は非常に重要ですね。どのような構造化を行うべきでしょうか？これは結局、RAG（Retrieverとジェネレーション）のRetriever部分とジェネレーション部分が関係してきます。検索を正しい文章を引き出すことが重要ですが、これがうまくいかないと、後続の性能も向上しません。結局、検索エンジンの高性能化という問いにぶつかることが多いと思います。皆さん、Google検索やYahoo検索などを使われているかもしれませんが、Google検索が下がれば下がるほど、RAGを自分で組んでみて、うまく文章を引き出せないようなことになるかもしれません。しかし、Google検索はやはりすごいなと思います。

[1:28:24 - 1:30:45]
それがどういう構造で、Google検索の仕組みがオープンになっていない部分があるんですが、有名なのがそのページ同士のページランクというのが、97年頃に提唱されてそれでGoogle検索エンジンを作ったみたいな感じなんですけど、そのページ、どの文章が重要かというのがページのリンク情報というか、どのページがどのページからリンクされているかが大事だというところを組み込むことで、そのページの品質をランク付けするというところで、文章だけの情報を使ってランキングするのではなく、そういうメタデータというか、文章以外に付与できる例えば、一番重要な文書群というか、このマニュアルは絶対最初に読めみたいな文章があるとしたら、そういう文書を必ず検索するようにするんだ、ですとか、そういうデータベースごとに管理を変えるとか、こういう質問はここのデータベースみたいな感じ、このデータベースにはこういう情報がありますみたいなのをデータベースごとに区切ってやるとか、メタデータとして日付が新しいものを良いとしてやるかとか、更新された日付が新しいほど情報が新しいとして、社内DBとして作るかという感じで、いかに他の情報をうまく使るかというのも大事かなと思います。SSMって何百でしょ？村口モデルじゃないかとスモールRAG社の性能との差だと思うので、どうコ・ス・パを考るかって感じなんですが、とりあえず最初RAGにしろ何にしろ何かパイプラインを組むときは一番いいモデルというか、コストは一旦考ずにどこまでいけるのかなというのを考慮する上で、一番いい性能の良いモデルでモデルとはそれを良くしていく方向というのは結構いろいろあると思うので、今どれぐらいいけるのかなというのを確かめる上でも、大きいものも出る高性能で、お金がかかるかもしれないんですけど、生のものを使うってのは大事かなと思います。というところで、時間が来たので以上にします。

[1:30:45 - 1:32:32]
それでは、ベルトンさんお願いします。はい、よろしくお願いします。ここから練習の方は宇津ベルトンさんが担当させていただきます。はい、事務局で共有しましたが、今回の演習ではGoogle Colabを使うので、まずそのGoogle Cloudという環境について軽く説明させていただきます。具体的なドキュメントの方では、結果、手引きの演習環境のGoogle Cloudの使用方法というところにあるので、詳しく見たい方はこちらを読み込んでいただいて、簡単に説明させていただきますと、Google Colabの方だとページを起動していただいて、画面の名称とか調整に関してちょっと割愛させていただくんですけど、演習した結果の保存ファイルとか保存したい場合は、ファイルメニューからドライブにコピーなどして、保存してください。ここ一番大事なところになるんですが、演習ファイルの起動の方法に関してですが、まず、皆さんドライブの方を開いていただいて、Googleドライブを開いていただいて、そこに.ipynbファイルがあるので、最初のファイルがあるので、こちらをクリックしていただければ、開ける人はそのままGoogle Colabで開いてもらって、このような画面が出る場合には、Googleからを選択することで、Colabの方で演習を実施することができます。

[1:32:33 - 1:32:53]
はい一部講義とかだとGoogleのドライブの方にせ マウントする人とかが るんですがその辺もここら辺読んでいただいていればどのような形でマウントできるかってのがわかると思います。はい。大体環境に関しては、ちょっと簡単にですが、このような形でお願いします。

[1:32:54 - 1:33:09]
ちょっと練習の方に移りたいと思います。練習の方ですねちょっと先ほど軽く本当ちょっとですけどか修正したので、事前にダウンロードしちゃってる方とかは、ちょっと再ダウンロードとかしていただけると りがたいです。

[1:33:10 - 1:33:31]
はい講義の間ではちょっと演習の方はデモンストレーションという形で講師の方が行うのでちょっと皆さん手が輪がちょっと手をと一緒に動かしていくというな形にはならないのでちょっと皆さんは、生講義終了後とかにアーカイブの動画を見ながらちょっとそれ一緒に追いながら動かしていただけると りがたいです。

[1:33:32 - 1:33:57]
はいそれでは書内容入っていきます。ですね今回の講義ではちょっとPromptingで ったり揚げメンテとLanguageモデルに関しては使ったんですが各回の演習はそれぞれ講義で扱った内容を実際に実装していくという流れになっていきます今回は特にPromptingに関してオープンに扱 るモデルというのをHuggingFaceと呼ばれるプラットフォームから読み込んでそこから実装等していきます。

[1:34:00 - 1:34:13]
具体的な内容に関しては、本講義で扱ったPromptingのいろんな手法で ったりだとか、argmentLanguageモデルの一種で るRAGRetrieverRAG面テッドGenerationに関して実装していきます。

[1:34:16 - 1:34:57]
はいですね今回ちょっとGPTの方を使うのでランタイムの方からランタイムC4のGCPで動かせるようにしているのでC4の方で動かしていただけると りがたいです。はいそれではまずはモデルをオープンに公開されてるモデルをですね実際に読み込むためにHuggingFaceの方からログインしていただき、HuggingFaceの方へログインしていただきたいんですがここを動かしていただければこのような形で出るので、そのHuggingFaceで発行されているアクセスのtoken等をここに貼り付けてもら れば、これをサクセスするってなればログインできます。

[1:34:59 - 1:35:38]
はいこれ量子化に必要なパッケージって書いて るんですけどちょっと後で話します。ですね実際にここでモデルを読み込んでいきますと僕の方、時間がかかるので事前に読み込んでいるんですがここで遠くないTheMLそのモデルをモデルが扱う値に変換するようなモデルですLLMが使 る値に変換するモデルを読み込むと、ていきます今回1000ちょっと質問をくれたんですが、Meta社が出しているLlama3の8Billonの方を用いて動かしていきたいと思い、実装していきたいと思っています。

[1:35:43 - 1:36:07]
はいこちらですね8Billonでちょっとかなり大きいモデルになるので読み込むのに時間がかかったりとかGPUの方メモリかなり使うので、メモリが足りないままや不安の場合には考慮量子化の方ここず、実行し、有効にしてここの方も有効にしていただければ、小さく載るモデル載せることができるので、よろしくお願いします。

[1:36:12 - 1:37:26]
はいそれではですね、実際にLLMの方のモデルを使っていきたいと思うんですが例 ばプロンプトですねこちらがモデルに私入力になるんですけど今回このMaxPlayerBERTOneは打ち上げ時モデルフリー負例ってことで簡潔にRAG言語モデルについて説明してというタスクを例 ば与 るとしますそのように与 られたモデルというのを遠くない座でモデルLLMが理解しやすい形に変換するといったことをして、それをですねこの実際に正例とするところの入力として与 て げますし、ここでHuggingFaceのTrasnformerの方だと基本的に同じような仕組みになるんですけど実際に出力させる性成分の長さで ったりだとか、Decodingの手法というのをここで設定することができますこのサンプルのフォースにするとですね、襟ぐり離散アグリーDecodingになるので必ず皆さん多分全く同じような結果が出てくるようになると思います とはGLUEにしてTemperatureの方とか、ここは有効にしていただくとちょっと変わった答 が出てくると思います。

[1:37:30 - 1:38:05]
HuggingFaceのTrasnformerLibraryがよく使われている理由としては、基本的にどのようなモデルに家で っても扱う方法モデルが2と同じような方法で全部動かすことができてわかりやすいからというような理由が ると生成結果とか見てみると、このXプレーンな場内ラウンジLanguageモデルフリー3という出力に対して、次に最もらしい言葉としてこのような形で出力がどんどん生成されていく様子が見 ると思います。

[1:38:06 - 1:38:40]
はい。ですねそれではここから実際にPromptingのテクニックに関して見ていきたいと思います。これ以上関係ないですまずはですねZero-Shotという形で、モデルにQAのタスクをたかせていくんですが、Zero-Shotという形でこのようなタスクとかせてみると、ちょっと時間かかりますね。

[1:38:52 - 1:39:39]
出ましたね、こんな形で本来一つの値を選んで回答させていただきたいというようなタスクにはなるんですね。洲鎌正成の結果がこのような形で、こちらユーザー側、質問者側としては全く望んでいないような巻きフォーマットで結果が出てくると、どのように解決していくかってことになりますんですけど、今ですね、出力が何回試しても変な形になるというのをわかりやすくするために、ちょっと5回ぐらい生成するようにしていますが、これ2回目の方も、剣道という形で大括弧がついちゃってる状態でした。望ましくない形で出てますね。

[1:39:40 - 1:39:57]
次に、Few-Shotと呼ばれるこのデモンストレーションをプロンプトに追加することでQAを解かせるという手法を試してみたいと思います。ただし、Queryが多いので、2回ぐらいにします。

[1:39:58 - 1:40:42]
このような形で生成させることでこちらの予想と期待としては、大学校もついてない答 だけの単語が出てくるように、このし、エーフィショットLayerなるかなというのがこちらの規定では るんですが果たして出ましたこういう形でちゃんと買い応 も ってて、ちゃんと大括弧先ほどの例だと大括弧ついちゃってたのが、つい出ないような、ちゃんとした形で出力が出ると、こういう方法でうんこういう方法で出力の性能というのを上げることができます。

[1:40:43 - 1:42:10]
はい、2回目もちゃんとこのような形だったので、たまたまってわけでもなく、configショットが、確かに3つの効果があることがわかりました。はい。それでは次にチェーン部に関して見ていきたいと思います。今回はちょっと違うソートが、Promptingが提唱された論文の例を使うんですが、まず何もない状態、何もチーム相当をしない状態で回答を生成させてみると、早いですね、20ドルとなんかよくわかんない回答が根拠もなく出てくると、それに対して、しチンarg相当と呼ばれる方法をとって、によって、こういう形で洗礼を入れて、思考の過程というのを生成させるような形で促してみると、時間がかかりますが、このような形でちゃんと施工の過程が、って且つ、回答もちゃんと正しく合っていると、こういう形で遅延を相当の方は性能の向上が見られると思います。

[1:42:11 - 1:42:44]
次に、最後にZero-Shot Charm層と呼ばれるレイヤをレッツノルマライゼーションステップというふうに追加することで性能が上がるという方法が講義中でも言及されていたと思いますが、こちらに関しても試してみると、はい、こちらも少し出力が長くなりますが、正しく思考の過程が出て、最後にちゃんと8ドルのレフトが残るという形で回答が出ています。

[1:42:45 - 1:43:23]
はい。では最後に、Prompting PLaMoとの差によって出力がどのように変わってくるかという点を見ていきたいと思います。上の文章だと改行がありません。空白がありませんが、二つ目の文章では空白を入れて、サンプルフォースという形にすることでグリーンデコーディングにしています。そのため、何かを入力しても同じような結果が皆さんの環境でも多分同じような結果が出てくるようになるので、この二つの出力の差が明確にわかると思います。

[1:43:37 - 1:44:10]
ちょっと先に下ですけど、このような形でPromptingいろんな方法が るんですが他にも、方法といろんなことが るのでここら辺の資料を見ていただければより深く知れると思います。結果出ましたが、この1個の単純な全角のスペースを入れるだけでこのような形での出力が内容はほぼ同じなんですが文章の長さって点でわかる通り出力が変わってくるってのがわかると思います。

[1:44:12 - 1:44:28]
はい。ちょっと早くなりましたがここまでがPromptingに関する内容になります。次RetrieverのargmentGenerationということで、argmentGeneration系の話に移っていきたいと思います。

[1:44:33 - 1:45:13]
ちょっとごめんなさい、ここですね、ここからはなんですけど、先ほどまで使ってたのがLlama3 8 Billionってことで、事前学習までのLlama3モデルになるんですが、ここから事後学習ではMetaのLlama3 8 Billion Instructモデルを使っていきます。理由としては、単純にその日本語において、もう割と性能がいいからですね。まずこのモデルを使って、東京大学の松尾・岩澤研が開講する大講義言語モデルではどのような内容を使いますかというような質問に関して回答させてみましょう。

[1:45:15 - 1:46:43]
ではですねつまりその言語LLMモデル側が、これに関してどのような知識を持ってるのかというのを確かめる目的でやっています。はい、出ましたこんな形で結果が出るんですが、ちょっと割と読むとですねそれっぽいことは言ってるんですが実際はそう正しくはなくてですね、このようなこと漢字で正しくない情報というのが正しい情報をどのように形で言わせるかってことで講義中でも りましたがIntelRetrieverRAGエンジン現地モデルで言われたような話ですけど、正しい情報をLMに渡して げると今回でいくと講義の内容に関して、これ講義ページに った内容ですけどこいつを入れて げますプロンプトとしてはこんな形になります先ほどの質問の上に、講義の実際の内容を入れた情報を加 て げてるという形ですねこれを訂正し、これをプロンプトにして生成させてみると、はい。

[1:46:43 - 1:47:04]
このような形で先ほどのこの与 た情報に基づいて正しいLM今回の講座に関する情報に関して説明してくれます。はいここでコンテキストRAG系の話に関してわかったところで実際のRAGの実装の方に移っていきたいと思います。

[1:47:05 - 1:47:31]
はい、まずはリトリーブのValueの実装という形で、モデルがどのように関連する情報を取得するかという実装に移っていきます。ここではエンベディングモデルを使用するので、こちらも事前にダウンロードしておきましたが、皆さんの方でダウンロードする場合、時間がかかってしまうかもしれません。

[1:47:32 - 1:47:56]
はい。では具体的な方法に関しては、質問文をベクトルに埋め込んで、そのベクトルと関連する文章のベクトルとを比較します。ここでは、コサイン類似度などで類似度を計算します。

[1:47:56 - 1:48:24]
ことになります。実際にちょっと埋め込んでみましょう。ベクトル化してみましょうそうするとこのモデルは、ではですね、1024次元に埋め埋め込んで、このような形でベクトル化されるんですが、実際正しく文書をどのように取得するかということに関して、ちょっとこちらを見ていただいて、今三つの情報を与 ています。

[1:48:24 - 1:48:47]
一つが言語モデルに関する講座の内容、もう一つは深層生成モデルに関する講座の内容で最後に強化学習に関する講座の内容になりますここから正しく文章を取ってきてほしいのでこの質問に関連する文書を取ってきてほしいのでこの一番上の本講座サマースクールの一環として、大規模言語モデルから始まる文章をうまく取ってくれたら成功になります。

[1:48:48 - 1:49:10]
ちょっと動かしたんですが、実際に埋め込んで計算させてみると、この文章のスコアは73、この文章のスコア63。この文章のスコア73.16ちょっと誤差では るんですけど最もスコアの高いインデックスがこれってことになって正しく文章が取ってくれているのがわかると思います。

[1:49:10 - 1:49:30]
このような形でRetrieverルの方ってのは、RAGの方で実装されていきます。次にですねドキュメントの用意になるんですがちょっと補足にはなるんですがRAGシステムというのは実装する上でかなり重要な工程かなと思うのが実際にドキュメントを作るってところになります。

[1:49:30 - 1:50:07]
ドキュメントは何かというと、先ほどの例でいうようなこの本講座はというのは、この一つ一つの文章の対応ドキュメントって言います。どうしてこの部分が重要かというと先ほどコンテキストレングスが伸びてたくさんの文章入れられるようになったよって話も ったんですが基本的にAPIの文字数制限等でたくさんの文章を入れられないとか、 とは実際検索の生のときに未処理の膨大な文章をそのまま使うってなると、関係ない文章とかが入ってくるので検索性能が下がったりとかしますね。

[1:50:07 - 1:50:49]
とは内容的なノイズになったりとかすることもあるので、これらWikipediaとか見て想像してもらえばわかるんですけど、長文の文章をどのように分割して一つ一つの単位としてチャンクとして見るかっていうのがRAGの課題の一つになってくるかなと思います。その方法の方法、やり方としては主によく言われてるのが三つです。例えば、文字数で1000文字単位で区切る方法や、段落や章などで区切る方法、あるいは意味的なまとまりを考慮して、例えばLLMに指で分割してみたいな感じでやらせたりすることもしますけど、そういう形でドキュメントをとってきたりします。

[1:50:50 - 1:51:12]
今回の実装では、二つ目の段落やショーなどに基づいて分割していくやり方でやっています。今回使うドキュメントとして松尾研究室がこのラボニュースというページから情報を取ってきてここに関する内容に関して、モデルでちゃんと正しく回答させることができるかってことを見ていきたいと思います。

[1:51:14 - 1:51:35]
ですね実際に文章を取ってきてみるとこのような形で、今のやり方ではURLとその公開日と内容とTaxonomyページのタイトルが、このような形で取ってこられる形になりますねこんなのでこの中から正しい文章をうまく取ってきて回答させるというのがRAG全体的なフローになります。

[1:51:36 - 1:52:38]
実際にやってみましょうということで、今回東京大学松尾・岩澤研のGENIACプロジェクトにおいて開発したモデルの何かを質問で解かせてみたいと思います。前提テキストは先ほど作ったこれらの文章になるので、このミッションの中から、例えば「タヌキ88Billon」というような言葉が入っている文章を取ってくれるかというようなことになります。実際やってみると、上位3件を取ってみると、一つ目はちょっと違うんですが、二つ目にちゃんと正しく「タヌキ8掛け8Billon」という文字が入った文章を取ってくることができています。実際のフローだと、これで取ってきた文章をそのままこのような形で前直前に挿入することで、質問に対して、「はい、こんな形でタヌキ88Billonを開発し公開しました」というような正しい最新の情報を含んだ生成をさせるという流れになります。

[1:52:42 - 1:53:29]
はいこういった補足でちょっと申し訳ないすけどこれはC4では回らないので、リソースが る方もしリソースが る方がいればLforやで回していただければいいんですが李ランクって言われる講義内で森ランクって呼ばれる方法が ったと思うんですが まりランクではですね最初にRAGにおいて使われるリランキングって方で最初に大雑把にかんコストが安いようなモデルとかでたくさん関連する文書をなるべくちょっと多くとってくると、その次に計算コスト高い色精度がηEモデル等を使って、取ってきたいくつかの文章の中から、より洗練され、洗練して文章を取ってくれってなことを行います。

[1:53:29 - 1:53:59]
ここでも実際にやってみているんですが、先ほどまで使っていた割と軽いモデルを使って、上位5件を取り出しています。これに、これから使って呼び出すちょっと重いモデル、コストが高いモデルを実際に使って、2回目の文章をやると、このような形で、より関連性の高い文書を取得するという流れが2ランクで行われています。

[1:54:02 - 1:54:21]
はいこれ補足になりますこれ動かしたい方向けに、ですね先日話も ったと思うんですがタヌキGENIACプロジェクトで開発された狸88Billonというのを実際に動かすことができるような部分を作ってみたこれはC4で動かせるので実際やってみたい方動かしてみてください。

[1:54:23 - 1:54:48]
はい演習の方流れ以上になります。 りがとうございました。川崎さんお願いします りがとうございます。ちょっと同じくですね質問いくつかいただいているので、ちょっと書いていただければなと思います。ちょっと内部のSlackで共有しますね改めてはい、わかりました。

[1:54:48 - 1:55:05]
ちょっと画面共有しつつ、上から回答しなかった演習のやつが上から三つくらいだと思います。そうですね。ただ一番上の部でプレーしないかもしれないので、飛ばしてわかりました。

[1:55:05 - 1:55:50]
はい。10サンプルフォースを使うとLlamaでQていくときに同じモデルが回答が得られるという理解で合ってますか。モデル更新とLlamaの調整等が行われて出力結果が再現されない。モデル更新が行われるかわからないですけど行われたらその読み込む時点で内部パラメータってが変わってくると思うので出力は変わって、変わっちゃうんじゃないかなと思うんですけど基本的にモデルの方で内部のパラメータとは変わらないので変わらないと思うので、まずサンプルフォース使ってればGreedyDecodingになって継続的に同じ回答を得られるかなというふうに思います。

[1:55:53 - 1:56:12]
はい。コラボの有料版を使った場合に想定されるコストですか。無料版で基本的に動かせるようにしているので、有料版を使った場合のコスト等を考慮する必要はありません。何かフィルタリングが外れちゃってますね。ごめんなさいそうですか。

[1:56:12 - 1:56:39]
リンクから踏んでもらえればと思います。ちょっと受講者の皆さんにお伝えしておくと、無償版って言ってるのがC4というインスタンスですね。基本的に演習の無償版で使っています。もう少し高ランクのところをやりたいって方は、Day4では厳しいかもしれないので、有償版のL4やそれ以上のGPTを使ったほうが良いと思います。そのような案内になるのかなと思ってます。

[1:56:39 - 1:57:23]
なんで基本的に無償版でいけるというふうに、 の後なんだ、ご理解いただければと思います。ごめんなさいこれですね演習のノートブックでログイン作成するとなりますがその次のステップでモデルのダウンロードが失敗しますってことに関してなんですけどこちらですね、何か事前に案内が った気がするんですが、今回Llama3使うということでMeta社にちょっとモデルを使いたいですというな申請の報告拡充する必要が りましてこちらの方がちょっと進んでいないとHuggingFaceの方でのログインを作成するんですがMetaの方からちょっとモデル使う権利を与 られないよって形でCANnotACCESSゲーテDay4言われるになります。

[1:57:24 - 1:57:44]
なのでですね、ここ演習の方にURLを書いてあるので、そこから申請を行っていただければ、ちゃんと動作すると思います。RAGが外れてしまっているので、別の文体で使用した方が良いかもしれません。こっちか。

[1:57:46 - 1:58:09]
かな。ちょっと俺が開きすぎてました。うん、はい、はい。グラフRAGのアプローチは、これ値演習とは違うかな。ちょっと演習に関してそしたら大丈夫ですかね、質問ははい大丈夫です回収に関しての質問は大丈夫だと思います。HBM講義はい りがとうございました。

[1:58:11 - 1:58:44]
演習は以上になりますかね。瓜田さん。はい。ちょっとください。多分大丈夫です。動いているので、ちょっと僕、ここら辺詳しくないんですが、大丈夫かなと思います。はい、はい、わかりました。ありがとうございます。また春の質問がくるかもしれないので、その定義をテキストで回答できればと思います。

[1:58:45 - 1:59:07]
はい、ベイズさん、ありがとうございます。ありがとうございます。はい、ありがとうございます。最後に、僕からちょっといくつかお話させてください。画面共有します。宿題の説明です。この手引きの方にも書いていますが、今回から宿題が開始されます。

[1:59:10 - 1:59:30]
やり方結論から言うと全部ここに書いてるんで見てもら ればということなんですが、一応簡単に進めさせていただくと、今回に関してはちょっと演習でGLUEクラブを使う、使われたと思うのでちょっとここは割愛しますねなので宿題もGoogleクラブを使います。

[1:59:30 - 2:00:14]
はい。なんで同じようにファイルを開いてくださいということになりますね。宿題のこうじゃないか。宿題の場所も同じようにリンク貼ってるので、こっからたどっていってください。Omnicampusじゃないよ、教材フォルダの中に れ れだよ、それフォルダ1はいに るので宿題からですね04番宿題というところに りますDay2の宿題AIさんのノートブックのファイルが るので、これをコラボで開いてください。

[2:00:14 - 2:00:43]
これ以上編集ファイルにも、に関しても同じですね。GoogleがLaboratoryで開くと、開いていただくとこんなような のファイルが開けると思います。ですねこれらを解いていただいて最後まで実行していただくとCSVファイルが生成されますそれをOmnicampusにて作って提出してくださいということなんですけどこれってまだ開かれてないんですかね登坂さんここの所間違 てますはい、大丈夫です。

[2:00:43 - 2:00:58]
りがとうございます。はい。なんでCSVファイルをですねOmnicampusに入って自分のコースのところで、の宿題ここに出てくるはずなんで、こっからにアップロードしていただくというようなご案内なります。

[2:00:59 - 2:01:15]
ちょっとごめんなさい、次に関数を適用すれば大丈夫だと思います。時間が過ぎてしまいましたが、もしお付き合いいただけますか？今日はこの日の分ですね。Day2が出てくるので、このような形でここをクリックしてCSVをアップロードしてください。

[2:01:15 - 2:01:35]
いうような感じでしていただくと、祭典が確か1日に1回でしたっけに走る の橋締め切り後2回か。そっか締め切りまで れですね、何国会でもファイルは更新できるか、採点は1回のみになるってことですね。なのではい。

[2:01:35 - 2:02:41]
これで提出していただければというふうに思います。詳細は手引きに書いてありますので、もし不明な点があればこちらをご覧いただければというふうに思います。はい。とですね、ちょっと一部の方からご質問をいただいたんですけども、講座で習ったことをアウトプットしたり、社内に展開したりというようなお話をいただいてまして、松尾・岩澤研のこのLMコミュニティのWikipedia的なページが欲しいというご要望がありましたので、皆さんの共有で何かナレッジを共有していければいいなというふうに思って、ちょっとスペースを作ったので、ぜひこちらの中で学びだったり気づいたところ、メモみたいなところを書いていただければというふうに思います。皆さんと共有しつつ、何か1つの大きなデータベースにできればなというふうに思ってますので、そちらも皆さんやっていきましょうというところです。詳細はSlackの方で、後ほどSlackの講座のジェネラルチャンネルで流しますので、そちらをご活用いただければというふうに思っておりますので、よろしくお願いします。

[2:02:41 - 2:02:57]
はい。ということですいませんちょっと時間過ぎちゃいまして、本日の講座、以上とさせていただきます登坂さん、最後締めをよろしくお願いします。はい、 りがとうございました。皆様ご事項お疲れ様でした。Omnicampusでログインして出欠アンケートに回答してください。

[2:02:58 - 2:03:18]
提出締切は本日から一種単語の水曜日17時です。出欠アンケートの提出をもって出席といたします。また本日の宿題を公開いたしました宿題のファイルの格納先や出席宿題の提出方法は、受講の手引きをご覧ください。受講の手引きのへのリンクは、受講のお知らせメールに記載されています。

[2:03:19 - 2:03:23]
それでは講義を終了いたします。本日はご受講いただき りがとうございました。


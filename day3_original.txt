[0:00:00 - 0:00:25]
皆さんこんばんは松尾研の川崎と申します本日はあの大規模言語モデル講座Day3Pretraining会にご参加いただいてありがとうございます。ですねちょっと冒頭の僕の方からいくつかまたご案内させていただければと思いますのでちょっと5分ほど少々お付き合いいただいてですね、からあの講義の方を釣っていければなというふうに思っておりますので、どうぞ本日もよろしくお願いします。

[0:00:26 - 0:00:57]
はいアナウンス事項がいくつかございましてですね、まずこれまでのコミュニティの中での交流について皆様活発的に交流いただいてありがとうございます質問対応にに関してもですね他の受講者の方々が回答いただいたりとか、運営に対する問い合わせに対してもご対応いただいたりとかしててかなり助かっており、おりますというところで、改めて御礼申し上げます。

[0:00:57 - 0:01:16]
引き続き、なんかコミュニケーション取りやすいようなコミュニティを作っていければなというふうに思ってますので、どうぞ活発に交流いただければというふうに思ってますのでよろしくお願いいたします。はいですね次のアナウンス事項としてもくもく会を実施できればなと思います。

[0:01:16 - 0:02:07]
ちょっと今リンク貼ったんですけども、6画面共有しようかな少々お待ちください。今リンクに貼っておりますですね、講座のもくもく会をやろうかなというふうに企画してます9月26日木曜日の20時から20時半、2時、21時半までこの場にオンライン上で集まっていただいて各自自分の課題だったりとか演習宿題みたいなところに取り組んでいただくっていう形で実施できればなというふうに思ってますちょっと初回なんでいろいろ試行錯誤しつつやっていくことになるのかなと思うんですが、ぜひ皆さんご参加いただければというふうに思ってます。

[0:02:07 - 0:03:27]
はい。ちょっとこのコンパスのリンクから申し込みができますので、ぜひ皆さんご参加よろしくお願いします。というのが二つ目で、三つ目としてはですね。こちらの同じ同じようにリンクを貼ります。よいしょはいLM講座のwikiを開設しましたっていう案内をさせていただいたんですけども早速一部の方々記事作っていただいていただきましたありがとうございますGENIACというプロジェクトでタヌキモデルたぬきのたぬきモデルというLLMを開発したんですけどそれをDay2の演習で使いましたっていう無料で使いましたっていうようなMさんGENIACの小滝チームの開発メンバーの1人でもありますけどもこちらにまとめていただいてるというような記事を作ったりとか、していただいてます他にもですね、複数にも複数のあの記事と立ち立てていただいてる方々K山田さんとかですかね、あのありがとうございますちょっとこういう形で皆さんで何て言うんすかね、DBを充実させていければなというふうに思ってますのでどなたでも編集追記可能なので、どうぞこちら活用いただければというふうに思いますので、よろしくお願いします。

[0:03:28 - 0:03:54]
はい続きましてチャットポットのご案内になります講座の質問用のボットとしてですね、チャットボット提供しておりますが、ちょっと初回にお伝えした通り我々も何て言うんすかねアップデートしながら皆さんの声を聞いてアップデートしながら開発を並行して進めておりますがちょっと新機能というか、一部回収しましたのでそちらのご案内させてください。

[0:03:55 - 0:04:21]
はい新機能というか新しい部分としてはですね今日このここまでは一緒で、質問範囲のところに運営業務の項目を追加しましたこれまで何か事務系のあの質問ですよね例えば開催日いつですかとか手引きに書かれているような情報、あの全部URLなんですかとか、そういったように、そういったような質問に回答できるようになりました。

[0:04:23 - 0:05:29]
ちょっと試しにやってみましょう。はい。こんな感じではいますまだ全然完璧じゃないんですけども、これまでの回答できなかった部分だったりとかに関しては、あの、対応できるようになったのでちょっとあの皆さんお使いいただければなと思うのと、また何か気になる部分もあると思うので、ぜひそちら金受講後のアンケートですかねそちらに項目があると思うんで、ぜひコメントいただけますと幸いですよろしくお願いします改めてのご案内なんですけどもこのチャットボット質問基本的に1スレッド1、1問の質問で使っていただければと思いますというのもあのこれまではデータをどんどん蓄積してですねちゃんと今後も改善するようなサイクルを組みたいと思っているので、基本的に一つの質問室スライドに対して質問一つの質問が収まってるっていうな形を保ちたいので異なる質問をする場合はこのこちらからですね新しい質問というのを実施をしても質問し直していただけるとありがたいです。

[0:05:30 - 0:06:03]
ていうのと解決しましたかしなかった明日家下しないか、関わらず質問、使い終わった後はこのフィードバックをしていただくようにお願いしますグッド押すと、そのままこの質問はチケットを閉じられますし、バットを押すと運営側に通報というか連絡が行ってそこから可能な限り個別で対応できればというふうなフローを進んでおりますので極力極力極力という必ずこちらどちらかの選択いただくようにお願いいたします。

[0:06:04 - 0:06:37]
はい。というようなところでございます。あとですねちょっと再出国告知なんですけどもまだ計画中なんですが近々LM講座のに参加されているあの方々を招いたオフラインイベントを実施できないかなというふうに計画してますまだ計画中なんで、固まり次第できればなというふうに思っておりますが、より密なコミュニケーションとって楽しく有意義な米講座にできればなというふうに思ってますので、引き続き皆さんよろしくお願いします。

[0:06:37 - 0:07:35]
はい。ということで講座の方に移っていければと思います本日はですねPretraining会というところで事前学習なかなかあの経験できない部分なのかなというふうに思いますがそちらの講座をやっていきたいと思います講義パートからですね川西さんGENIACのプロジェクトでは、開発支援チームのメンバーとしてですね、開発各各チームがちょっとちょっと詳細は割愛しますけども、フェーズフェーズ1のLLM開発コンペティションにおける各チームの開発の行動の事前の環境だったりとかスターターキットみたいなところのコードだったりとかを用意していただいたあの管理士さんが講師をさせていただいてその後演習は前回のRAGPromptingの講義の主講義の講師をやられた松尾研博士の原田さんにお願いしたいというふうに思ってますので、よろしくお願いします。

[0:07:36 - 0:08:04]
はい。そうしましたら川西さん、準備よろしければ始めていただければと思いますがいかがでしょうか？はい、大丈夫ですよろしくお願いしますよろしくお願いします。では画面共有させていただきます。それ今、スライドショーの画面って見えていますでしょうか？はい、見えてます。

[0:08:04 - 0:08:18]
ちょっとページを送ってもらってもいいですか。変わるかどうかだけ大丈夫ですねはいよろしくお願いします。はい、ありがとうございますよろしくお願いします。はい本日前半の工期の交渉を担当させていただきます川西と申します。

[0:08:18 - 0:08:52]
後半の演習の方を原田さんが担当していただきます私は松尾岩沢研究室では、LLMの学習に使用するCoTの開発などを行っていました。例えばはい例えば先ほどの川崎さんにご紹介していただいた通りGENIACというプロジェクトによって、各チームがLLMの学習のに使用する際の行動のスターターキットとなるような行動の開発などを行っていました。

[0:08:53 - 0:09:50]
簡単ですが自己紹介は一緒にさせていただいてちょっと今回分量が多いので早速内容にやっていこうと思います。はい今回Day3Pretrainingの会事前学習の会議でして今回の講義のときは、このLLM大規模言語モデルの主要なモデル構造であるTrasnformerとその事前学習の仕組みを理解するっていうことを目的に、Qをいたしますこの講義が終わった後の目標はまず言語モデルにおけるTrasnformerの位置づけについて説明できるようになること次にLLMで主流となっているTrasnformerのモデル構造についても説明できるようになることそしてLLMの事前学習のパイプラインについて説明できるようになると最後にLLMの事前学習を実際に公道で実装できるようになること、これらを目標としていきます。

[0:09:57 - 0:10:13]
講義全体の流れは大きく四つの章にわかれていまして、まず最初に言語モデルとは何かということを説明した後にその中で重要なモデル構造でありますTrasnformerの具体的な細かいモデル構造について細かく見ていこうと思います。

[0:10:14 - 0:10:36]
次にTrasnformerの構造を知った上ででは事前それを学習するための事前学習はどのような流れで行うかを解決し最後に発展的話題のセクションに移っていくことを思います最後、演習の方は実際に牌トーチを用いて、Trasnformerを実装し、さらに実際に学習してみようというような内容になっております。

[0:10:41 - 0:11:25]
では早速内容の方に入っていこうと思います。まず最初に言語モデルとは何かっての復習も兼ねて解説していこうと思います。まず言語モデルとよく聞くTrasnformerという言葉の関係なんですけれども、この図で表したように、まず大きく言語モデルというのがあってその中で、ディープラーニング、つまりニューラルネットを用いて言語モデルを構築しようという試みが、その中でもニューラル言語モデルと呼ばれるものでして、さらにその中でも昨今一番うまくいってるニューラルネットのモデルが、Trasnformerと呼ばれるモデルとこのような位置関係になっております。

[0:11:26 - 0:11:44]
Trasnformerってのは近年の大規模言語モデルでもほぼ全て一般的に使用利用されている非常に有名なモデル構造となっています。ですので、このTrasnformerの利点や高めの構造などを今回、細かく見ていこうという感じです。

[0:11:50 - 0:12:10]
これは復習になるんですけれども言語モデルとは何だったのかっていうのを押さえも兼ねて解説したこと思います。まずこの単語の系列単語の並びですねそれはつまり文章ですけどもその生成確率を予測するものをモデル化したものが言語モデルといいます。

[0:12:11 - 0:12:49]
ただし、この各単語間と同時に、単語1勝単語2勝ってとこ全部の単語を同時に発生する確率ってのをいきなり求めるのは難しいので、連鎖率条件付き確率での掛け算で分解したモデルを自己回帰言語モデルと呼ぶようになりましたので、一気に求めるんではなくて、まず単語1単語名の確率は何かとその1単語目が、来たら2単語目はこれだろうという確率を掛け算してってそれをどんどんどんどん繰り返していくとこういう分の掛け算で分解していくというようになったしました。

[0:12:50 - 0:13:16]
このように分解してこの条件付き確率がそれぞれわかると、今度は文章の生成をすることができるようになります例えば、入力に2本のシュートはこの4単語入ってきたら単語メールに次何の単語枠なのかっていうと、東京パリ帰ろこのような選択肢があった中では東京が一番最もらしい確率が高いだろう。

[0:13:16 - 0:13:51]
というふうな計算ができるようになります。そうしたならば、では日本の人はBERT来たら次東京と出力したら、文が完成するそのような感じで文章を作成するができるという流れになっています。この仕組みを数式で書くとこのような形になるんですけれども、単語1単語目に単語Meta…ってこういう単語ついた次に単語が来る最も高い確率となるような単語はどれでしょうっていう、そんなような問題を解くというのが言語モデルのものです。

[0:13:53 - 0:14:26]
この条件付き確率を、いろんな方法で推測したいわけですけれどもその中でもニューラルレッドを用いて近似した。という方法をとったのが、ニューラル言語モデルと呼ばれます。ニューラルの言語モデルを見ていく前に、ディープラーニングよりも前にあった代表的な言語モデルっていうのも見ていこうと思います。

[0:14:27 - 0:15:41]
これはどのように行っていたかといいますとこの条件付き確率を統計的に文章の出現回数をもとに求める方法がありました。これは大規模コーパスな大量の文章のデータセットの中の単語率の出現頻度から単純に出現回数から割り算して確率を求めていくという方法です出演回数をカウントする関数をシャープ((っていう関数でおいたとすると、この日本の首都は時田次に、東京都来る確率はどのように求めるかというと、単純に日本の首都はっていう文章がこのデータセット内にあった回数とする分母にして、次は日本の首都は東京と実線ついた文書を実際にカウントしてそれを分子に打っており刺したこのような単純な内部な形で確立を求めるという方法がありました例えば日本の首都はが旋回出てきて、その次に日本の首都は東京と実際についたのは200回やったら1,000分の200で20%、0.2というふうに確率を出そうという考え方のやり方です。

[0:15:43 - 0:16:11]
このような内部の形で5モデルの構成することもできるんですけれども、この方法に大きな課題が二つありまして、一つ目がデータSparseNESS問題これはどういう問題かといいますと、単語列が長くなると、その出演回数が急激に減少し、条件付きかけたTwitterこんなになってくると全く同じ文章がどんどんどんどんアイス減ってきますので、それってSparseDense問題で問題です。

[0:16:12 - 0:16:35]
もう一つが、類義語問題という問題でして、これは意味が同じなんですけれども、ちょっと形は違う類義語が別の事象として扱われ、扱われてしまうんでちょっと言い方を変えた微妙に変えただけで、もうあの異なる出現頻度としてカウントされてなので、正しく確率を条件付き確率を推定できないという問題になってしまいます。

[0:16:36 - 0:16:54]
例えば、日本の首都はという文章と、日本国の首都はこの二つ言ってることは意味は一緒ですけれども別々としてカウントされてしまいましたので正しく条件付き確率をそれぞれ求めることができなくなってしまうという問題がありました。

[0:16:57 - 0:17:19]
次にディープラーニング以前の代表的な言語モデルでもう一つN-gram言語モデルというものがありまして、これは直近のN-1個の単語を使って次の単語を予測するという方法です。これもどのように確率を計算するかというと先ほどと同じです。

[0:17:19 - 0:17:45]
出現頻度で推定します。ただ出現の頻度を推定する範囲を全部の単語、これまで入力してきたZoomの単語ではなくて直近の数単語を使って、次の単語を予測するとこの点が違います。例えば、Nイコール3の場合は3g言語モデルAと言いまして、直近の2単語から次の単語を予測するという仕組みになります。

[0:17:45 - 0:18:23]
ですので、先ほど使っていた日本の首都は、次に東京が来る確率を求める場合はこの3g、言語モデルでは直近の2単語、つまり、首都はから次東京っていう単語が来る確率を予測しようというような手法です。このように需要予測に使うこの単語の範囲、単語数の範囲を狭めることで、ある程度先ほどの処方の問題であったこのデータSparseDense問題をある程度は回避できるようになりました。

[0:18:23 - 0:19:14]
ただ、これでも解決できない大きな問題がありましてそれは長距離間の単語間の関係性を把握しづらい。つまり、直近の2単語からだけを見て、その次東京っていうのを推定するのは難しい。面も多々ありますのでそういったことが難しいと、この場合は、もう数単語前の日本のっていうところがないと、次東京って予測しづらいですので、このような範囲を狭めたことでデータSparseのS問題を回避できることはできたけれどもその裏側としてもっと前単語のことを無視してしまうということ問題がありますとか、ですのでこれらの問題を、後ほど紹介しますこのTrasnformerで解決しようっていうのがこれまでの流れでした。

[0:19:19 - 0:19:51]
ここからディープラーニングなどが発達してきてニューラル言語モデルっていうのが提案されるようになってきました。これは先ほどから求めようとしているこの条件付き確率を、これ何らかのニューラルネットを用いて推定したモデルとなりますこれは他の機械学習と同様にこの誘導を最大化するように訓練させますつまり、誤差逆転版を用いてのニューラルネットワークを訓練させます。

[0:19:51 - 0:20:19]
なので、この下の図のように下から日本の首都はこの4単語ニューラルネットに入力したときに、次に来る単語がこの東京てくる確率を他の単語よりも高く予測する京都って予測する確率よりも東京というか、予測する確率をより高くなるようにニューラルネットワークを訓練しようというのがこのニューラル言語モデルのアイディアです。

[0:20:21 - 0:20:54]
そうすると次の問題は、ではどのようなネットワーク構造が最適なのかという問題になってきます。ニューラル言語モデルってのはですね歴史的背景的に機械翻訳の分野で大きく発展してきましたのでここからは機械翻訳のタスクを、に見ていきます具体的には吾輩は猫であるこの4単語を入力したときにこれを英語に翻訳するCatに翻訳したいというときの例を考えてみます。

[0:20:56 - 0:21:19]
この2一旦骨髄愛amキャット、このように出力したいという、この詩を達成するためにどのようなニューラルネットワークが適切なのかというのを見ていこうと思います。ここで徒歩二つ用語の定義を紹介しようと思います。

[0:21:19 - 0:21:37]
一つ目はEncoderというものでしてこれは入力文入力テキストを入力として受け付ける左側の部分のことをEncoderと言います。もう一つ、もう一つがDecoderと呼ばれるもので、これは文章を出力する。

[0:21:37 - 0:22:04]
場所です。ただし、これただ出力するだけではなくて自分が出力した単語をさらにもう1回入力に持ってきて、そして2次の単語を予測するという最適な入力を受け付ける機構も持っています。なんでこのEncoderとDecoderこの言葉、後ほど出てきますので、この二つ覚えていっていただきたいと思います。

[0:22:09 - 0:22:42]
ニューラル言語モデルの中でまず最初にRNN型リカレントニューラルネットワークを使った言語モデルというのが達成しました。この代表的なモデルにはSeektoSeekといったモデルなどがあります。これは冒頭の単語から1単語ずつゆらネットワークに入力して、このニューロンを逐一更新すると、ただしそのときパラメータは使いまわしで行っていくという方法ですこのネットワーク講座を使うことによって、原理的には単語いくつでも入力してかつ出力することはできます。

[0:22:42 - 0:23:02]
実際に吾輩は猫であるのを用いて例にとって見て、この流れを見ていきましょう。まず最初に、1単語目の吾輩が、入力されます理論が構築されます。同じパラメータを使って今度は2単語目を入力します。入力が更新されます。

[0:23:02 - 0:23:19]
この猫入力します日本こうしてますっていうのをごめんなさいボタンを入れて湯いうのが構築されますと次Decoderに移っていきます。Encoder値の潜在表現をDecoderに渡して、Decoderがファイルを出力します。

[0:23:19 - 0:23:53]
そしたら、Decoderが出力した1単語目のI、これをもっとさらにもう1回入力が2の最後にくっつけて、入力すると、次の2単語目のであるアームが取得されて、そのまま流れでどんどん次々の単語出力し、CatはいこのようにRNN型の言語モデルで翻訳ができるということはできそうだということが確認できました。

[0:23:57 - 0:24:19]
ただし、このRNN型でも二つ課題が残っていまして、一つ目はこのニューロンが低調ですので長文になればなるほど、全ての情報を覚えきれない。結局このRNN型でも、先ほどもあった、先ほどのような手法で全くの単語間の長距離依存性の把握が困難だという問題がありました。

[0:24:19 - 0:24:46]
これは、言い換えますと、ちょうど入ってくると最初の直近の単語覚えてるけど最初の方の単語が何だったっけな。あんまり覚えていられないという問題がありました。二つ目の課題は、これ、長文入れれば入れるほどネットワークは横ほぼ単語方向にどんどんどんどん伸びてって深くなっていくため、この学習が不安定かつ学習が遅いということになります。

[0:24:47 - 0:25:03]
これは、誤差逆伝播でアルバックプロパーレーションをするときに、一番後ろの出力層から一番最初に6層まで戻っていくのは非常に何ステップもあって大変でありその間に学習が不安定かつ各種遅くなるという問題がありました。

[0:25:07 - 0:25:40]
これらのような二つの問題をうまく解決したモデル構造として提案されたのがこのTrasnformerというモデルことだったんですよこれは詳しいモデル講座、後ほど紹介しますが、まず結論から先に紹介しますと、このTrasnformerの中にあるテンション事故という仕組みを最大限活用することで、先ほど述べたRNN型の問題を解決できました。

[0:25:41 - 0:26:03]
つまりこのアテンション機構というのを最大限引きそうすることで、単語間の長距離依存性を把握できるようになり、かつバックプロポレーション逆を誤差逆伝播のこのステップ数が単語数に依存しなくなるつまり短くなってそのおかげで、学習の安定化と高速化これ両方を達成することができました。

[0:26:08 - 0:26:48]
ですので受賞分になっても14ヶ所を超えていて且つ学習も早いという。長文に強くなったということです。ということでこれからそんなすごいTrasnformerの細かい内部情報を次の章で見ていこうと思います。TrasnformerってのはattentionisAllUnitっていう非常に有名な論文で初めて提案されたモデルとなりますこれ論文は2024年現在、10万件のEOSを超える非常に有名なものとなっています。

[0:26:48 - 0:27:08]
これは最初に1017年にGoogleを中心にした研究チームが発表したものでして、アテンション機構というのを最大限活用することで、この単語さんへ言い換えますと空間の直立補正の関係性を効率的に学習することができるようなったってのが大きな特徴です。

[0:27:08 - 0:27:50]
かつ、この学習時の並列計算も効率できたことで、大規模化、もちろん大規模化つまり分散学習たくさんのコンピュータを使って一気に並列計算しやすくモデルの学習をしやすくなったというメリットがあります。細かいモデル構造を見ていく前にこのTrasnformerというモデルのすごさここで紹介しようと思うんですが、この2017年に発表されて以降、モデルの改良やスケール化つまり、より大規模化させることによって、数多くのベンチマークで、当時の再構成の達成し続けています。

[0:27:51 - 0:28:16]
例えばGPTシリーズの1から4までありますけれども、これ全てTrasnformerモデルが採用されていまして、次、特にその中でもGPT-4が有名だと思いますが、GPT-4は米国の医師免許試験や相試験に合格したり、日本の医師免許試験や処方試験にも合格できたりするような高い能力を持っている報告がされています。

[0:28:17 - 0:28:57]
また、つい先週、OpenAIから発表された大湾モデル、こちらも非常に数学などの論理的思考が必要なベンチマークにおいて非常に高い。パフォーマンスを叩き出していまして例えば米国数学オリンピックの予選にて高い点数を出して本戦出場資格を入れるくらいの高いスコアを出したりとか、化学、物理学、生物学などにおける専門性の高いベンチマークにて人間のPHPソフトの回答を出したりといったぐらいハイパフォーマンスを叩き出しているというような報告がされています。

[0:29:02 - 0:29:36]
そんなすごいTrasnformerの細かな構造、中身まで見ていきましょう。Trasnformerって検索するとよくこの左側の図が出てくる、見覚えのある方もいらっしゃるかもしれないんですけれどもこの中身をこれから細かく見ること思いますこれ図はTrasnformerを構成する最小単位でありますけど、ブロックと呼ばれるものとして、このブロックを縦に、N数を積み重ねて作るというのがTrasnformerモデルです。

[0:29:37 - 0:29:59]
この中の特に左側をEncoderブロックおよび右側をDecoder6と呼びます先ほども出てきた。営業後ですね。ではまずこのTrasnformerの処理の全体の流れっていうのを、イメージを持つために見ていきましょう。

[0:30:01 - 0:30:20]
先ほども就労した例吾輩は猫であるこの4単語を英語に翻訳するという翻訳タスクを見ていく例に見ていこうと思います。まず、吾輩は猫であるこの4単語一騎5Trasnformer1層目に入力します。いいっすよね。

[0:30:21 - 0:30:46]
2層目に入力が渡って3染谷納…そうめんぱで伝わりますDecoder側も同じく縦側にNそう。あります。ですのでこのEncoder側の情報をDecoder側に渡して、翻訳、つまり出力分が吐き出されます。はい。

[0:30:47 - 0:31:18]
今度さっき自分Decoderが出力したAIをさらに自分自身も1回、最終的に入力して、やっとこのように出力、つまり翻訳文が出力されていきます。このとき注意していただきたいのが、このブロックの縦に積み上がるこの層が遠くの数だけ横方向、単語の数だけ横方向に増えていっているということに注目していただきたいです。

[0:31:19 - 0:31:39]
この横同士のブロックは後ほど紹介します。アテンション機構という機構によって、横同士のブロックが繋がります。つまり同氏にも情報の伝達が行われているということになります。これがTrasnformerの全体の処理の流れです。

[0:31:43 - 0:32:05]
実はEncoder、左側のEncoder部分じゃなくても、Decoderだけでもテキスト生成自体は可能となります。これはなぜなら出力だけではなくてDecoder側最終的に入力、自分が吐き出したしする子もいっぱい入力に入れるという機構も持ち合わせているからです。

[0:32:06 - 0:32:56]
実際にやってみましょう。このDecoderだけにニューラルネットにこの春は0歳春はという負担を入力していく例を見ていこうと思います。これ春は実際入力してみて、そうすると、桜で単語が次に行くのは最もらしいとDecoderが予測した場合、今度は桜ってのをまた入力に持ってきて、春は桜ときたら次は画が出力されて、ということは春は桜が来たら綺麗と出力されると、これって結局春は入力したら、春は桜が綺麗、このように文章が生成DecoderOnlyだけでも文章生成ができそうだってことはわかりますね。

[0:32:57 - 0:33:33]
実際GPTシリーズってのは、のDecoderOnlyの形式をとっています。これ背景なんですけれども、このTrasnformerでモデルが当初提案された領域は、機械翻訳の領域だったためその機械翻訳の先行研究でよく使われていたこのEncoderDecoder形式に習う形を取る構造が当初提案されたので、Trasnformerも英語だけ講座を持っている形でしたが、テキスト生成という意味においてはDecoderOnlyだけでもできるという話です。

[0:33:39 - 0:34:04]
はい。ではここからここまでがTrasnformer全体像全体の処理の流れを見ていきましたここから各パーツに分けて細かくその仕組みを見ていこうと思います。ここでは四つ大きく四つにパーツを分解して一つ一つ見ていこうと思います一つ目がエンベディング、次にマルチ伝承次にフィードフォワードと最後その他の順番で見ていこうと思います。

[0:34:04 - 0:34:19]
ただこれ図左側の図ではこれ下の方から入力入って上から出力が出ていくという流れになってますのでこの順番は上下逆転して、下からエンベディング、あるいは発電所フィードフォワードその他そういう順番になっています。

[0:34:23 - 0:35:09]
では最初、エンベディングというそうだよ何をやってるかを解説していこうと思いますこれは一言で言うと単語のベクトル変換を行っています。単語のベクトル変換ってのはどういうことかって言いますと、これは色をRGBの3次元ベクトルに変換しているのと非常に考え方は似ています例えば、格子色をRGBのこの3次元ビクターに変換することで、このコンピュータでもこの青紫とか青に近いんだなということが、コンピュータ、人間は目で見てすぐわかりますけどコンピュータとしても数値的にわかるようになるこのような利点がありますこれと全く同じこと単語にも行うというのはこのエンベディング層でやっていることです。

[0:35:14 - 0:35:40]
このテキストをどうやってTrasnformerに取り込むかとかっていう、いうところを担当したのがここのエンベディングをして、実際に見ていこうと思います。このテキスト文字データをそのままではニューラルネットは入力として受け付けることができないので何とかこれを数値化していきたいという流れです。

[0:35:41 - 0:36:06]
まずテキストを遠くない座と呼ばれる後ほど紹介しますプログラムを通して、トークンという最小単位に分割します。ここでは単語単位に分割されると考えていただいて大丈夫ですのである訳ものっていう一つの文章があるはあけぼのっていう3単語にわかれます。

[0:36:08 - 0:36:28]
こうやって分割された単語を今度はまた同じく遠くない座の内部で持っている辞書に沿ってこの1単語を取るとある個別に割り当てられたIDという数字に変換していきます。例えば、春っていうのは戦後10番目の参考です。

[0:36:28 - 0:37:07]
和は80番目の単語ですというような形で、各単語一対一で対応されてる数字にIDに変換していきます。このIDを今度はワンCoTベクトルっていう特殊な形のPEFTに変換しますここは機械的な自動変換でして、単純な変換をしておりまして1050という数値TalkIDを変換するときはこのたくさんある要素の中でも、戦後10番目の要素だけが1になったベクトル、そして他の要素全部ゼロというような特殊なベクトルに変換します。

[0:37:08 - 0:37:44]
ですので80っていう場合は、この80番目だけ1あって他は全部ゼロというようなベクトルに変換します。最後このような形に変換したワンCoTベクトルをマルチLayerSEPという1900万を用いて、ワードエンベディングという最終的なベクトルに変換していきますこの九重のマルチLayerSEPとのパラメータここもニューラルネットですので、パラメータ学習対象となります。

[0:37:45 - 0:38:14]
こうすることで、000000で1個だけ一応当たっていたって、単純なWベクトルが、いろんな実数値が敷き詰められたもう数字の羅列となるようなPEFTに変換されますこれをワードエンベディングと呼びます。このワードエンベディングってのはこの単語の分散要件とか単語埋め込みの中の他の呼び呼ばれ方もされます。

[0:38:15 - 0:39:13]
多分単語埋め込みってこのエンベディングっていう言葉に、直訳したものでしてこの単語の意味この数値化してPEFTの中に埋め込んだそのようなイメージを行っています。このような変換してどういった利点があるかといいますと、これは単語っていうこのSparseの情報をベクトルというDenseな密な表現に変換していると捉えることができますこうすることでまず一つ目に、数値化されたことによって、Trasnformerなどのこのニューラルモデルの入力中として扱えるようになりますさらに、内科したベクトルという形で数値化したのであればそれらの点をプロットすることでこの各単語のこの意味を可視化することはできるという。

[0:39:14 - 0:40:32]
そしてその店同士のこの値差近さによって距離の近さによって、この意味の類似度を測ることができるというメリットがあります例えば、左側の図、これ万という単語とウーマンっていう単語これは似たような単語ですけれどもそれが実際プロットしてみると、この近くにあるとだからこのトゥルーマンは二つの単語は意味が近いんだなっていうことがわかる数値的にコンピュータでもわかるようになるという話ですただこの単語同士の誓いがわかるだけではなくて、同じくこのキングとクイーンも、この同じ近いところにあるんですけれども、面白いのはこのマントウーマンと同じような位置関係にキングとクリームをプロットされるというような形で、この性別の関係をコンピューターも理解できるようになるというそういったメリットがありますこの人間はこれ耳で聞いてすぐわかるんですけれども、それをしっかりコンピュータでもわかってもらうようになれるという、そういうメリットがありますこれは真ん中の図はこの動詞の活用形ですねリング系と過去形がいろんな単語で同じ方向の関係性にあると。

[0:40:32 - 0:41:04]
三つ目はこれ、国名とその国の人目もやはり同じような関係性にいるというのがわかるというそのような面白い研究もあったりしますこれはまさに一番最初の方に説明しましたこの色をRGBの3次元PEFTに変換すると非常に似ていてこうすることでこの紫ってのは赤と青の中間にあって、赤紫はどちらかというと赤寄りなんだなっていうことがコンピュータもわかるようになるっていう大きなメリットがあります。

[0:41:12 - 0:41:38]
ただ、エンベディングはここで終わらずに、さらにですねこのエンベディングワードエンベディングっていうの書く単語そのものの意味を数値化するという作業だったんですけれども、これだけだとその単語はこの文章全体の中で何単語目にあるのかっていうこの単語の位置情報っていうのは含まれてませんので、ここでその子の単語は何単語名ですっていう情報をここで最後に追加します。

[0:41:41 - 0:42:04]
これをポジションあるエンコーディングと呼びますやり方は30でしてこのワードエンベディングで出したベクトルに対して同じ次元数のベクトルに、この、これは一番目のトークンですという情報を表したベクトルを足し込む足し算をするという。

[0:42:05 - 0:42:26]
やり方です具体的にはこの補助者なるエンコーディング、どのようにしてこの一番目のトークンですという情報をベクトルの数値化するかというと、この下の方に式にありますこのサインコサイン以下ベクトルの次元数によって周期の違う。

[0:42:27 - 0:42:54]
サインコサインか用意してそれをただ足し込むということをします。この数式で実際に算出した値を可視化したのがこの右側の長方形の図なんですけれども、この縦側がベクトルの次元数の軸で、横側が単語の何番目かっていうのを表している。

[0:42:54 - 0:44:04]
単語方向の軸です。ですので、一番左のこの水色のこの長方形の枠は一番目のトークに足し算するポジションのエンコーディングでして、最後、右側が最後のトークのポジションなりDecodingですこれ明るいこの赤白っぽい明るい色と黒っぽい暗い色がありますがこの明るい色が高い値で暗い色が低い値を表していましてなので、上の方はは周期の早いこの明るい暗い明るい暗い明るい暗いという主義の速い社員箱山屋を使用していまして、下の方に行くにつれてこの明るい暗い明るいライトの周期がなだらかな、戦後前半を使用してるんだけど、そういうふうな見方ですねこれをすることによって、各番目のトークンによって独自のユニークな値を作れますのでこれを足し込むことで、このワードエンベディングこの単語は、文章全体中の何単語目ですっていうのを表現情報を加えた状態で、そこでTrasnformerに入力していく、このような流れになっています。

[0:44:09 - 0:44:27]
はいここまでがエンベディング層の仕組みでした。まとめますと、テキストを遠くに分割し、それをワードエンベディングというPEFTに変換します次にポジションあるエンコーディングっていう位置情報を足し合わせて、その上でTrasnformerに入力していく。

[0:44:27 - 0:45:02]
このような役割を持っているのがエンベディングそうです。次ここが核となるパーツだと僕は思っているんですけれども、マルチヘッドアテンションっていうパーツについて入っていこうと思います。このアテンション機構っていうのは何かといいますと一言で言うところ、全トークン間の類似を図ることによって、長距離のトークン間の依存関係を把握すること野々下稀子といえます。

[0:45:04 - 0:45:42]
例えば具体的な例を用いて説明しますと、春はあけぼの、夏はできたら、次どんな出力単語が来るかを予測するときに、この今まで入力してきた単語の中でどれが一番、次の単語を予測するのに一番効いてくるのかなっていうのを判別する、自動的に一番良いものを取捨選択できるような機構がこの伝承ですアテンションとはこの注目度とかっていう役ができますどこの単語に注目するのかというのを計算する場所となります。

[0:45:48 - 0:46:13]
このアテンションっていうのは数式で書くと論文にはこのような数式やその数式を可視化したこの右側のこういったグラフ失礼しました図があったりするんですけれどもこの組織やこの図を見てもちょっとぱっとわかりづらいところもあると思いますので、今からこの数式をアニメーション形式で説明していこうと思います。

[0:46:13 - 0:46:32]
ここで三つのアニメーションの中で三つのベクトルが登場人物として出てきます。これは一つ目QueryベクトルKeyベクトルValueベクトルだこれらを中心にこの組織はどのように動いているのかってのアニメーションで見てること思います。

[0:46:36 - 0:46:58]
まず先ほども使っていたこの春はあけぼのという3単語3トークンを入力するところを例に見ていこうと思いますこれらのトークンはそれぞれ全て先ほどエンベディングすをもう通過した後で、この単語がワードエンベディングというベクトル化された状態であるというところからスタートします。

[0:47:02 - 0:47:22]
まず、それぞれのトークンのワードエンベディングベクトルを用いてそれらを線形変換、マルチLayerperSEPってのを用いて、二つのベクトルに分けて作ります一つがKeyベクトルとValueベクトルこの二つにそれぞれ分けます。

[0:47:22 - 0:47:46]
2単語目もKeyベクトルValuePEFT第3解けもKeyベクトルValueベクトルそれぞれ作成します。だいいちトークンから見ていくんですけれども、ダイイチトークンのこのワードエンベディングベクトルをもとに、さらに三つ目のベクトルであるこのQueryベクトルっていうのを、同じく線形変換バッチLayerperSEP殿を用いて作成します。

[0:47:47 - 0:48:15]
このように作成したQueryベクトルを用いて、今度は各単語のKeyベクトルとの内積を取って、類似度を図ります。この第1解けのQueryPEFTを用いて、まず第1トークン自分自身のKeyベクトルと内積取って自分自身との類似度を図りますこれは先ほどの図でいうとここに相当しますね。

[0:48:15 - 0:48:32]
QueryベクトルとKeyベクトルを使って内積をとることに相当します。これを今度はダイイチ東北のQueryPEFT今度は第2党区のKeyベクトルと内積をとって、第2と第1特と第2党区の間でのルイス類似度を図ります。

[0:48:33 - 0:49:01]
同じことを今度は第1党区のQueryベストと第3党区のKeyベクトルでない席を取って同じく第1トークンと、今度は第3トークンとの類似度を図っていきます。このようにして、第1単語とその他のトークンとのそれぞれの類似動画在籍で取れた後に、これをの合計の値を1に正規化するためにSoftmaxをかけます。

[0:49:04 - 0:49:41]
これは左側の全体像というとここに相当します。こうすることで値が大きい方がこの類似度が高い。つまり、単語間の依存度が高いということがわかりますこの例で言うと、第1単語と自分自身のスコアは0.0、第1単語と第2、すいません第1トークンと第2党区の類似度が0.5で最後、第1トークンと第3トークンの間のリージョン0.5ってことなので、第1トークンはこの場合、第2トークン第3トークにこの依存度が高いということがわかります。

[0:49:42 - 0:50:10]
この類似度を用いて最後にこの類似度をそれぞれのトークのホンダ、最後に残ったValueベクトルと掛け算していきます。ですので第1トークンと自分自身のとの類似度これスカラーですのでこれを第1トークン自分自身のValueベクトルと掛け算の掛け算して、新たなベクトルを作成します。

[0:50:10 - 0:50:29]
これ同じことを第2トークンとも行います。第1解けと第2トークンとの類似度を使って、第2等分のValueベクトルと掛け算します。これを第3特に対しても同じことをやります。第1と第3トークン類似と第3部のValueベクトルを計算して、新たなベクトルを作ります。

[0:50:29 - 0:50:58]
そして、これらが出てきたオレンジのコミットメントろそ取ったものを最後、ダイイチベクトルのすみません第1トークンのアテンション機構の出力とします。これは何をやってるかと言いますと、この第1トークンが、それぞれの各トークンその他のトークンとのこの類似度を使って、加重平均をとった。

[0:50:58 - 0:51:24]
と捉えることができきます。これは全体像で言うとここ最後ここに相当しますね、この第1党区のQueryベクトルと他のKeyベクトルとの内積とって、類似度をとったものを最後書くべくValueベクトル泊トークのValueベクトルと、加重平均を行っている、そういうのような流れになります。

[0:51:26 - 0:52:02]
こうすることで第1トークンのに対するアテンション基本出力は、その他の単語との関係性を含めた新たな表現として出力変換されるという流れになります。これと今のちょっと同じ流れを今度第2党区に対しても行います第2トップにもQueryベクトルを最後新たに作ってこれをその他のトークンとの内積取ってる時どう図ってそれを用いて、各トークンてのValueべくとの荷重平均を取る。

[0:52:02 - 0:52:21]
これが第2党区におけるパーテーション機構の出力をします。第3トークも同じですねQuery代さんトークのクリプト作って、各トークンと内積とってスコア、類似度を測って最後各トークンとのValuePEFTの荷重平均をとったものが第3トップのアテンション機構の施策とします。

[0:52:23 - 0:52:54]
この流れを今高速で復習しますとこのようなアニメーションなります。それぞれの各トークンでKeyWebとValueベクターを作って、各ベクトルのQueryベクトルをそれぞれ他のベクトルたちと掛け合いのKeyベクトルと掛け合わせてなにせ体積を取って類似度を測って、最後そうやって出した類似類似度を用いて、各ベクトルのValueベクトルとの加重平均を取る。

[0:52:54 - 0:53:24]
これはこれを、この作業をどんどん繰り返していきます。はい。これは一歩引いてみると何を行ってるかといいますと、これはですね、ワンステップで全単語と繋がることで、これトークのトークの情報を効率よく取り込むようにできたと捉えることができます。

[0:53:26 - 0:53:58]
このアテンション機構を用いることでこの1本ある単語がその他の単語との類似度を測って加重平均を取ってってやることによってこの各トークンが必要なトークの情報だけをこの柔軟に取捨選択して重み付けをして、新たな表現に変わることができるという、これはこの時系列に沿ってこの単語の順番に沿ってどんどんどんどん入力していくトークトークンを入力していくこのRNN型では実現できなかったことですね。

[0:54:00 - 0:54:51]
こうすることで、1単語目が非常に遠くにある。10単語先のトークンともも重要だとここのトークには関わりがあるかどうかってのを長期にわたって関係性を、構築することができるこのような大きな特徴があります。これは次のトークンを予測するときに、直近のトークンだけ役に立つときにはこの投稿を見る必要はないですよねそういうときはそっちはいらないと、逆に次のトークンを予測するときに遠くのトークンの情報が必要なときに、逆に言えば近くを見る必要ないですよねそういうような形で適宜重み付けを柔軟に設定できるというのがこのアテンション機構の良いところです。

[0:54:54 - 0:55:17]
ええ。言い換えますと、ワンステップでこれ全単語と繋がることで、このRNN間の課題であった1単語間の長距離依存関係を把握できるようになった。そして、にワンステップで全単語と繋がりますので、今度は逆誤差逆伝播がステップ数が減り、安定かつ高速になることができました。

[0:55:21 - 0:55:47]
つまり、このアテンション機構によって、この各トークンのベクトルが、その他の全トークンと述べ、関係性を取り込んでより良い形に表現に変換されたトランスフォームされた。と捉えることもできる。ます。Trasnformerというのは、モデル名となった由来はもしかしたらここにあるのではと僕は思っています。

[0:55:52 - 0:56:12]
このようにアテンションってのが各単語でできますので、このいうことは今、アテンションのこの重みを可視化することで、各単語はどの単語にも紐付いているのか多く関係性が高いのかっていうのを可視化することもできます。

[0:56:12 - 0:56:41]
例えば左側の例文リーダーにもあるdtypeクロスター3Bコースいきます中体連こういう文章があったときのこの真ん中にあるキッドが、結局何を指し表しているのかっていうのを可視化したのがこのオレンジの太線と細い線なんですけどもこの太線であればあるほど高いアテンションが張られているということを表すんですが、しっかりフィットってのは、リニューアルに対して強くテンションがかかっていることはわかります。

[0:56:41 - 0:56:57]
このイット!ってのはこのストリートに対しては低くて正解のアニマルちゃんと強くアテンションかかっているってことはこのTrasnformerが文章ちゃんと理解できているってことが可視化してわかることができます。

[0:56:58 - 0:57:31]
これは1と1単語に対してのこのアテンションを可視化したものですけれどもこれアテンションKeyCoTは同じことを全ての単語に対して行っていますので、ということは、これを右図のように、アテンションマップで各単語がどれどの単語に対してどれほどアテンション当たってるかっていうヒートマップを作ることができますこれはそうですねスポーツでいうとリーグ戦のようにですね、この各単語がそれぞれどのようにアテンションがかかってるかっていうのを2次元のマップとして見ることもできます。

[0:57:36 - 0:58:16]
はいここまでがアテンション機構でした先ほどのアニメーション見た後ですとこの数式や、この論文で掲載されているこの図が先ほどよりかはちょっとわかりやすくなったのではないかなと思います。はいこのアテンション機構っていうのはですねEncoder側とDecoderがそれぞれにあるんですけれどもちょっとEncoder側とDecoder側で違いがありますのでそこを解説しようと思いますまずN講座側は入力テキスト内だけでのアテンションをかけるセルフアテンションという呼ばれる機構となっています。

[0:58:17 - 0:58:38]
一方で、Decoder側のアテンション機構が入力テキストだけではなくてテキスト出力側のテキストにもまたがったアテンション、これクロスアテンションというのをなっています。Decoder側では、出力テキスト内だけでの検証、政府アテンションも存在します。

[0:58:41 - 0:58:57]
ただし注意点は、この出力テキストについては自分より未来のトークン、これから出力していくところのトップに対してはアテンションされないようにマスクをかける人が、をかけますこれ講座るアテンションマスクと呼びます。

[0:58:58 - 0:59:40]
これはどういう目的かと言いますとこのDecoderってのがこのこれから未来のこれからどんどんし単語を予測していきますので、カンニングを防ぐ未来に対してカンニングを防ぐ目的でこれを行います。これ例えばこの図がその講座らテンションマスクを可視化した図なんですけれどもこれ行がQuery列がKeyベクトルだった場合に、AIというトークにとっては自分自身の愛にはテンション掛けますけどこの後出てくるはずのamaCatにはアテンションかけれないようになってると。

[0:59:40 - 1:00:12]
一方でamは、amとその前まで自分が出力したAIまでには、アテンションかけれるけども、その先のCatやパーテーションをかけられないとそのような仕組みをしますこれはプログラム実装上はテンションアップのSoftmax直前にこの非常に該当要素に非常に大きな負の値、中のマイナス-1の中の中条はなどを足したりする、そのような実装をすることがあります。

[1:00:22 - 1:00:52]
このようなアテンション機構なのですが、これをさらに複数個同じようなアテンションの仕組みを複数個並列で行いますその後出てきたそれぞれのテンションで出てきた、すごい一つのベクトルに統合して最後次の層に渡すんですけども、これは何をやってるかといいますと、一つのトークンが様々なトークンに異なる形式を与えのアテンションを当てることが可能にさせるという仕組みです。

[1:00:54 - 1:01:14]
これは平たく言うとこの人の文章でも、読み方によっては捉え方がいくつかあると思うんですけれどもそのようなことをここでもアテンションTrasnformerもわかる中でも同じようなことを達成できるようになる仕組みなんのではないかと私は理解しています。

[1:01:18 - 1:01:40]
はい。ここまでが、マルチェッロアテンションでしたおさらいしますと、アテンション機構にて全トークンのベクトルを、各トークンのベクトルをその他の全トークンとの関係性を取り込むことで、より良い表現に変換するそのような役割を果たしてるのはここのLayerです。

[1:01:43 - 1:02:03]
次、フィードフォワード尊入っていこうと思います。フィードカードは一言で言いますこの巨大な2階建てのマルチLayerforSEPtになっています。ただし、特徴として中間層が非常に大きいという。形をとっています。

[1:02:03 - 1:02:39]
これ入力層と出力層は同じ次元数なんですけれども、その間の中間層が入力総出力層の次元の4倍もあるというこのような非常に大きなうちLayerパーフェクトになっています。これ巨大なと言ったんですけどどれほど巨大かと言いますと、実際にパラメータ数をargを割り出してみると、全体の子役3分の2、66%のパラメータ数を占めているということがGPT3ではそれほどたくさん占めているということがわかります。

[1:02:39 - 1:04:08]
これGPT3を例にとって細かく計算していきますとこの出力入力総出力層ともに、1万2000次元ほどありまして、中間層とはその4倍ということなので、4万9000円次元ほどありますそんなブロック96ブロックありますので、つまりフィードフォワードのパラメータ数はどれくらいになるかといいますと、この入力層から出力層に対しても全ニューロンがお互い繋がり合っていますのでそこでのパラメータ数は入力層の次元×1万2000掛ける中間層の4万9000円でそれが今度は中間層から出力層に対しても同じようにたくさんの量が確認が全部繋がっていますので、それが2個に染まるとそんなのが96ブロックもありますこれ全部で116BillonからMetaから1,160億個のパラメータがあるフィードファーストフィードフォワード層だけであるということになりますこれGPT3の総パラメータ数ってのは175Billonですので1750億個のパラメータですので、フィードフォワード層の全体のパラメーターに占めるパラメータ数は結局66%、3分の2ほどあるということになります。

[1:04:11 - 1:04:27]
ということで非常に多く、全体の非常に大きな部分の割合を、ここのフィードフォワード数をパラメータを主占有しているということになりますので、パラメータ数が多い大きいだけに、非常に重要な何か重要なことをしているんだろうと推測することができます。

[1:04:28 - 1:05:07]
これ実際何をやる出るかと言いますと、後続の研究によりますと、これ最後の下の方にまとめて書いてありますが、知識を蓄える場所と考えられているという解釈がされています。これはどういうことかといいますと第1外のパラメータを入力のパターンを把握する箇所のKeyといて、第2層のパラメータをその方が何を意味しているかを表すValueといった場合に、これはKeyValueを用いて知識を抽出するニューラルMemoryを模倣してるんだではないかと解釈できるという研究があります。

[1:05:08 - 1:05:38]
ですのでMeta知識を行うKeyを使って抽出するそのような場所なんでないかと考えられています。はい。フィードフォワード指導これは日まとめますと、巨大な2階建てのバッチLayer羽セットKeyValueでの形式で蓄積した知識を抽出する機器として考えられています。

[1:05:40 - 1:06:15]
最後その他の部分に入っていきます&なんだろうね。ここへも重要なんですけれどもさっと紹介しようと思います。まず同これは山接続っていう呼ばれるところなんですが、深い層の学習をするときのテクニックです。フィールド外アテンションその前後でザ接続このを行っています二つ目にルームこれLayer席かというところなんですがこれは学習を効率化するテクニックでして、各社の受験数で平均と分散を取り、正規化するような処理を行っています。

[1:06:16 - 1:06:43]
最後すごくそうですがこれは線形変化を挟んだ後にSoftmaxをすることで、次の単語の正規確率ってのを出力する層になっています。はい。これが単相なモデルの全体像でした。まとめますと、おさらいしますと、テキストとエンベディング層では、テキスト特に爆発してベクトルに変換し、ポジションのエンコーディングを足し合わせてTrasnformerに入力します。

[1:06:44 - 1:07:05]
次にマルチ減ったアテンションこれは、アテンション機構にて獲得のベクトルを全部オープンとの関係性を取り込むことで、より良い表現に変換する場所です。最後巨大フィードフォワードそう、巨大な2階建ての1Layer%のKeyValue形式で蓄積した知識を抽出する機構として考えられています。

[1:07:05 - 1:07:43]
最後、その他の部分では学習をうまく行うテクニックと最後出力層のこのような、大きく分けて四つのパーツで構成されています。はい。ここまでが相場の中身についてでした。今日ここで一旦休憩を挟むと思います休憩後には事前学習の具体的な流れに入っていこうと思います。

[1:07:43 - 1:08:30]
そうですね、3分ほど持って休憩をしましょう。なんで次10分に再開しようと思います。ちょっとすいません下赤根さんお疲れ様ですちょっと皆さんの休憩がてらちょっと質疑応答の時間にもできればなというふうに思うので、ちょっと管理者画面共有していただいて、回答できそうなところ、あの、時間の許す限りお願いできればなと思っているんですがちょっと今チャットの方で内部でフィルタリングかけたURLを貼ったんですが、ちょっとこちらアクセスできそうであれば穴開いていただいて質問できそうなもの、あのピックアップしてご回答いただけるとありがたいかなと思います。

[1:08:30 - 1:08:59]
はい、わかりました。少々お待ちください。ちょっと時間がごめんなさいあの後5分ぐらい、ちょっと休憩の時間あった方がいいと思うので、再開13人ぐらいでも大丈夫です。大丈夫そうですかちょっと押すかもしれないんですが大丈夫ですはい13分で大丈夫すいませんはい皆さん13分再開という形でよろしくお願いしますちょっとあの手が空いてる方休憩がてらあの質疑応答にも耳を傾けていただければ良いのかなと思いますよろしくお願いします。

[1:09:07 - 1:10:06]
管理する管理士さん側で画面共有とかってできそうですかします少々、はい、はい。ええそうです。ペンション機構の関する質問が多いです。はい、そうですねこのテンションテンション機構におけるKeyとValueとは何を示すかのイメージがわきませんでした。

[1:10:12 - 1:10:54]
これはそうですね僕の理解ではまずValueベクトルの方から説明しようと思うんですけれども、このValueベクトルが、各単語の意味を、の本質的な意味をを確認しているを数値化しているものだと私は理解していまして、Keyベクトルの方は、その単語の意味そのものというよりかは、その単語が他の単語とどのような関係性を持っているのかっていう文脈に相当する。

[1:10:55 - 1:12:06]
情報格納しているのではと私は理解しています。ですので、各単語例えば、ダイイチトークンが、その他の台にトークン退散特大4トークンとどのような関係性を持っているかっていうのを見るために、ダイイチトークのQueryベクトルとその他のトークンとのKeyベクトルの内積を取っていくという話だったんですけれどもこのValueベクトルの方は、その下、各トークンのそれぞれの単体の意味だけを持って、いるので、そっちと内積を取るよりかは、このその他の単語、この単語に対してどんな関係を持ってるかっていう、そっちの情報持ってるKeyベクトル法と、内積を取っていくとそのときには、そのときには今ダイイチ徳野Keyベクトルではなくて、このQueryベクトルってのを内積使ってない席とってましたよね、それは今度はですねこのダイイチトークンが絡みた。

[1:12:07 - 1:12:50]
その他のトークンとの関係性はどういうものなのかっていうのを情報持ってるのがQueryベクトルなのではないかなと私は理解しますそれを使って、その他の単語とのこのダイイチトークンと関係性のあるトークンは何だっていうのを探し当てていくのをやっているのがそのQueryPEFTを用いて、その他の単語とのKeyベクトルと内積を取っているそういった作業に処理になっているんではないかなと思いますそうすることで、この第1党県ってのは、代理とかこういう関係性なんだあんまり関係ないんだとか、第1相くんと大山東くんと非常に関係が強いんだって言ったことがどんどんどんどんわかっていく。

[1:12:51 - 1:13:20]
わかったならば今度はそのダイイチ総研とその他特にこういう関係性なんであれば、そのトークン自体が持っている本質的な意味Valueベクトル法をその類似に従って加重平均した出して出そうっていうような仕組みなんではないかなと私は理解してます。

[1:13:21 - 1:13:46]
はい。ということですいませんもう実際、7時13分にRZ83ページになりましたので、回答講義の続きをしていると思います。はい。ちょっと時間がすみませんを知っていますので、ここからは、少しすみません。秋吉君だってしまうと思います。

[1:13:48 - 1:14:05]
はい川崎さんも休憩終了で再開で大丈夫ですか大丈夫ですよろしくお願いします。はい、ありがとうございます。では事前学習のここまではTrasnformer内部の仕組みも細かく見てきましたので実際にそれをどうやって学習していくかっていう事前学習の流れについて解説していこうと思います。

[1:14:08 - 1:14:40]
はいまず事前学習に入る前にこのLLM全体の学習フローってのは大きくする三つのステップに分けることができます第1ステップはこの事前学習でこれから見ていくものです。ただしそれが終わった後にも二つのステップがありましてこのFinetuningとRLHFの二つのステップがありますがこれは後日講義で詳しくありますのでそちらの方で詳しい解説を譲りたいと思いますまずこの会では第1ステップである事前学習の意向と思います。

[1:14:43 - 1:15:48]
事前学習の具体的な回数に入る前にまず、LLM以前はどのようなか言語モデルの学習をしていたかってのを見ていると思います時実はそのLLM以前はあまり事前学習というのはありませんでして各モデルがそれを北井佑そのものに、の中学習だけ行っていましたので、翻訳モデルとして使う作られたモデル翻訳だけの学習を行いようやくモデルはようやくどっかまでの読解の学習だけそのように書くも各タスクそれぞれわかれた別個の学習だけをしていましたが、LLMが誕生してからは、まず一旦大規模コーパス大量のテキストデータセットを使って事前学習を行って行って汎用的な能力を確保獲得してから、各細かいタスクに分岐してそれぞれ翻訳の学習、追加で要約の修正に追加でどっかの各種のように行う。

[1:15:48 - 1:16:26]
このように、学習のフェーズを大きく2段階に分けるようになりましたこの二、三回目のFinetuningとかRLHFだとかを事後学習と呼ぶようになりました。第1フェーズのこの事前学習の目的はどのような目的かと言いますとこれは後続タスクに共通して必要な汎用的な知識、言い換えますと読み書きそろばん、いろんなこの基礎知識を学習させやって学習させた基礎知識を後続のタスクに転移するというような目的があるのではないかと解釈されています。

[1:16:26 - 1:16:51]
関連する研究も分野ではこのトランスファーラーニングやアダプテーションた分野があります。高速タスクのための、これは良いパラメータを食すKey値が得られるとも解釈することができます高速タスクって呼んでるのは最終的に起きたタスクのことでして、さっきのスライドで言うと、要約翻訳読解などのことを指します。

[1:16:54 - 1:17:32]
これ下にファンデーションモデルに関する論文のところを引用しているんですけどここでもこの事前学習というのは、の目的が書かれていますのでもし興味がある方は、こちらの論文を参照していくことをせします。はい事前学習のパイプラインってそのものも細かく四つにさらに分解することができまして、一つ目はデータの収集、次、データの前処理で滑って綺麗にしたデータを訓練し、最後評価を行う、このような4ステップになっています。

[1:17:32 - 1:17:47]
これはこっからは概要だけ御説明しますが、この本資料の最後の方にあります発展的話題の方のセクションでも詳細の解説をしていますので興味のある方はそちらをご参照ください。では一つ目のデータの収集から見ていこうと思います。

[1:17:50 - 1:18:18]
事前学習用のデータを一般的にこのWebから大規模にクロールして集めてきたデータを用いることはあり、多いです。夢よくあるのは一般的なWebサイトニュースやブログ、ホームページなどから集めてきたデータやプログラム言語で、であるGitHub百科事典Wikipedia、あとは小説などを含めたボックス、あと論文のアーカイブと最後技術的な話題のQ&A知恵袋みたいなサイトであります。

[1:18:19 - 1:19:23]
スタッフExchangeといったところから集めてきたデータを用いて学習させることです。これは次どれくらいの量を集めてくるか。なんですけれどもかこれGPT3の亡霊にしてみますと、約5000億トークンのテキストを利用しているとこれは書籍5000億トークンってのを書籍に換算しますと、約500万冊に相当すると言われています参考までに東大図書館の約130万冊国会図書館が4700万冊ありますので、これ東大図書館よりも、ほぼイーブン量をGPTするように見込んでいるという学習されたということになりますこれはリンク情報によりますと今度はGPT-4さらに多い1.3億冊相当13兆トークンを使用して学習されたといわれていますこれもはや国会図書館よりも多い量を準備Day4読み込んでいると言い出すことができる。

[1:19:24 - 1:20:03]
と言えます。ただ最近ではモデルサイズは小さめのモデルサイズなんですけれども、なんだけれどもこれより多くのトークンを収集させてパフォーマンスの改善を試みるという事例もよく見られます例えばLlama2が70Billonして、175BillonであるGPT3よりもモデルサイズはちっちゃいんですけれども、仮にGPTするにも多く、この日商と逆4倍のトークン数を学習させることでパフォーマンスの改善を試みるという事例もあります。

[1:20:09 - 1:21:07]
そのように大量に集めてきたデータセットなんですけどそれをそのまま使うわけではなくてそのデータは玉石混淆品質は玉石混合ですのでちゃんと前処理を行って綺麗なデータセットにするという作業をここで行います。データセットによって毎週の仕組みもそれぞれちょっとずつ異なるんですけれどもここでは代表的な前処理のステップを紹介しようと思いますまず最初に、クオリティフィルタリング、これはより品質の低いデータを何とか判別してデータを取り除くということを行います次に日アプリケーションDeepと略されることも多いんですけれども、これは文章に重複があるとそれが学習への悪影響が多いため、これは文文章+様々な粒度で重複同じようなこと書いてあるところを排除するっていう処理があります。

[1:21:08 - 1:21:29]
次にこれも14なんですけども、プライバシーリアクション個人を特定できる個人情報などは取り除くとのことを行います。最後に次のページで説明しますけれども、遠くないぜションというのを行いますこれは文章最小単位単語単位のような最小単位で分割する作業になります。

[1:21:33 - 1:21:50]
この遠くない税所得税ション先ほどからよく出ている単語言葉なんですけどこれ何を行ってるか。っていうのをここで説明しようと思いますこれはテキスト、一つの文章長い文章をこのトークンと呼ばれる最小単位に分割する。

[1:21:51 - 1:22:15]
という処理を行う場所ですトークンとはイメージでは1解け1単語のような形で単語単位のような形で細かく分割していくという作業ですそんなトークン化を行うためのプログラムを遠くないTheと呼んでいまして、例えば有名なのはバイトペアDecodingやSentencePIECEなのか有名なものがいくつかあります。

[1:22:15 - 1:22:49]
これは一般的に5位、その文章の中で出てくるこの合意の日、出現頻度と関係したアルゴリズムで、このどこで区切るかってのを決めるというようなアルゴリズムになっていることが多いです。ですのでこのコーパスこの文章をテキストデータセットの中から遠くない座が決めたアルゴリズムに従ってここで区切るっていうような語彙の語彙一覧の辞書を作成し、その辞書に従って、文章を分割していくというような作業を行います。

[1:22:53 - 1:23:09]
そのように綺麗にされたデータされてるかつトークン単位で分割されたデータセットをどのように学習するかなんですけれども、これはネクストトークンプロジェクションという自己教師あり学習の一種を用いて、学習を行います。

[1:23:10 - 1:23:39]
これは具体的にはどういう学習方法かといいますと、この文章を入力して、その文章の次にくるトークンが何かっていうのを、そこの確率をひたすら予測するというものです。なので例えば、吾輩は猫であるっていう文章を学習データセットとして使った場合、どのようなことが起きるかって言いますと、まず一旦我輩を入れた後に、次に来る。

[1:23:40 - 1:24:03]
正解の単語はなんですけれども、実際にその我輩を入れた後に、はが出てくると予測する確率を高めるようなふうにこのモデルを訓練するということです。次も一緒で吾輩はが来たら次猫正解ですのでちゃんと猫が出てくるよ足が確率が高くなるようにモデルを訓練します。

[1:24:03 - 1:24:34]
吾輩は猫って切ったら次は次はあるんまるっというのを予測していくと実際にモデルが予測した。確率が入ってきたら次側が来るだろうっていう即した確率を、実際の世界が終わった場合のこの予測と正解との誤差、交差エントロピーという講座を小さくなるように、世界との差に企画成果により近づくようにLLMを学習させていくっていうそのような学習方法です。

[1:24:35 - 1:25:13]
学校することで、このテキストデータをそのまんま特にラベルづけとか、人間がアノテーション高をしなくてもある程度そのままデータセットが使えるというメリットがあります。XとCoTくんプロジェクションってのがこの次のトークンの生成確率をひたすら予測する方法なんですけれどもその予測と生活の誤差、具体的には交差エントロピーってのを小さくするように学習しますスーパー交差エントロピーの数式で書くような形になります。

[1:25:16 - 1:25:38]
このネクストトークプロジェクションLLMの学習において特徴的な他の機械学習ディープラーニングに比べて特徴的なのがこの1エポックのみっていう少ないボックスのみで各種させるっていう点が非常に特徴かなと思っています多くても1から3サイクルっていう非常に少ない範囲で学習を行うことが多いです。

[1:25:38 - 1:26:19]
例えばFalconと呼ばれる一番上、Falconと呼ばれるこのUAEが発表した開発したLLMでは、全てのデータセットで1億しか学習していませんで、Llama2でもMeta社が開発したLlama2というモデルでも右腕パックのみ1本のみをトレーニングさせていますLlamaOneの方ではもう各データセットによってフォックスがちょっとずつ違いますけれども、多くても一、二本ぐらいってことで、やはり少ないボックスだけ学習させるっていうことが多いです。

[1:26:21 - 1:26:59]
これは何でかといいますと、考えられる理由として、この複数エポックたくさん学習しすぎると、各学習だってこの性能が落ちるデグラデーションするか、もしくはあまり下がらない、こっちなくてもあまり差がないじゃない、やる意味があんまりないということがわかっていますのでこれが大きな理由なのではないかなと考えますこのさらに悪いことに、このモデルサイズを大きくすればするほど性能が高まるとわかっていますが、大きくすればするほど、複数PoCしたときのこのパフォーマンスが落ちる傾向が強くなるというより、悪影響が大きいということがもうわかっています。

[1:27:00 - 1:28:21]
そういった理由があって、少ないポップスだけ行うっていうことが多いです。このちょこう聞くと単にこの次の単語を予測して後々答え合わせをしていくだけと聞くと、後なんか簡単そうに聞こえるんですけれども、これ実際やってみると非常に難しくてこの他どのように難しいかというと、小さなモデルの訓練だと発生しないようなも問題エラーが大きな大規模なモデル学習すると途端によく発生するようになるという非常に厄介な性質があります例えばどういうのがあるかといいますと、具体的にはこの交差エントロピーこのロス、この下がれば下がれば、下がれば下がるほどいいという値なんですけどこれが学習途中で順調に下がってると思いきゃいけないパートかね上がって、その後一向に下がらないみたいな事象が起きたり、他にも、てLayerとハードウェアGPU周りたり、ネットワーク周りでこんなにますこんなない赤を着るっていうようなこともよくありますこういう難しさがありますので、事前学習を行う際には、1PCとかインフラ周りだとかに詳しい方と一緒に協力して行うということが多いです。

[1:28:23 - 1:28:51]
この数のスパイプロサーファーて跳ね上がる。ってしまう問題を軽減するためにいろいろな作ってなるんですけれどもその一つハイパーパラメータを設定しましてこれもハイパーパラメーターでたくさんの設定がありますけれどもごくその中でもごく一部紹介しようと思いますまず一つ目はプチいざの設定例えばアダムやアダムWだよなMOMENTUMベースのオプティマイザーを使ったりすることが多いですね。

[1:28:51 - 1:29:18]
あとは、ランニングレートも一定の同じ値をずっと使い続けるわけではなくてスケジューラーといいましてこの合宿進むにつれてこの後、ランニングレートの値をちょっと使えていくっていうのを行ったりもします具体的にはこの右の図のように、ちょっと最初の方はだんだん0からちょっと通をワーキングアップさせて最後少しずつ減らしていくDKさせていくというようなことを行ったりもします。

[1:29:19 - 1:29:40]
あと、浮動小数点制度FP32とか、FP16とかといったものが一般的ですがFP16だとロススパイクがよく起きるって各種不安定になるっていうことはわかっていてその上DF16が安定的に学習しやすいってことがが報告がよく見られたりします。

[1:29:41 - 1:30:07]
最後論文を読んでいるとこのバッチサイズの単位が何かちょっとわかりづらいときがあると思うんですけどこれ実はこの単位バッチサイズもこのいろいろ調整できたりしてロススパイクの低減に繋がったりするんですけれども、この論文ではこの場サイズってのがサンプル数ではなくて、トークン数の単位であるときがよくあります。

[1:30:07 - 1:30:41]
例えば4Billonって書かれていますが、400万サンプルではなくて、この400万トークンを使用したということになります。これを右図のような形でサンプルすると、そのサンプラスの中でのサンプラーの最大トークン超過決断したこの長方形の面積を、が400ミリオンでそれをバッチサイズとして論文に記載しているということが多いですねこのこの長方形のこの縦の長さが400ミリをあるというわけではないということを注意していただければなと思います。

[1:30:43 - 1:31:12]
最後評価のところなんですけどまず定量評価ではまず最初に見るべきはバリエーションロスですね。ロスがちゃんと下がっているかっていうのをモニタリングしていきます。他の機械学習では、データセット取れ三つに分けてトレーニングをするリレーションズテストのステップの三つのロスを見ることが多いと思うんですけれどもここにLLMではテストロスのログ上ではあまり見かけないってのが特徴的ですね。

[1:31:12 - 1:31:43]
これはおそらく理由としてはこの全学者もJ-POPぐらいしか学習しないでオーバーフィットさんしないだろうという前提だからなのではないかなと思われます。たまにたまに場合によってはトレーニングロスだけで完結してることもあります例えば、左のLlamaではトレーニングの数だけ見ていますし、こちらはGPTのスキル不足を確認したときの論文なんですがそこではバリエーションの数だけを見てみます。

[1:31:47 - 1:32:07]
補足ですがこの交差エントロピーってのを主なロスの指標として使うんですけれどもこれは黒線とPとかCロスとか他の呼び方があったりししますと、そしてさらにクロス交差エントロピーだけではなくてこの周期変形した実質交差エントロピーと実質同じ指標なんだけど名前が違う指標っていうのも使われたりしています。

[1:32:07 - 1:32:30]
例えばPerplexityPMと略されてるものや、Bits-per-Character、PPC、Bits-per-wordPPWといった、このような他の指標もあったりします例えば右上の図であるGPTC4のテクニカルレポートはBits-per-word仕様が使われてますし、Llama2ではPerplexityPPIってのが使われたりしています。

[1:32:30 - 1:33:09]
この、この辺の試験系のこの下の方の絵に書いてありますリンク先にここから詳しい解説ありますので、もし興味のある方、そちらのサイトを参考していただければなと思っています。はい。事前学習ってのは後続高須くん特化したFinetuningなどまだ行っていない状況なんですけれども、一応そこの事前学習済みモデルに対しても一応下流のタスク最終的に飛び立つ圧縮での評価をすることがあります具体的なコンテキストLoRAにZero-ShotFew-Shotなどの絵を使って評価することはあります。

[1:33:10 - 1:33:28]
これもちろんまだFinetuningなどの事前事後学習を行ってない状態ですのでそれをさらに行えば、さらに下流タスクの性能さらに上がるってのはわかっています。ここまでは定量評価での定性評価を行うことがあります。

[1:33:28 - 1:34:03]
これは学習終わったLLMを使って実際にテキストを入力して、出力させてみるという。どんな文書が出力されるかってのをちゃんと見てみるっていうやり方ですね。サンプルレベルでの評価を行いますこの取得することってコードっていうんですけれども、このDecoderには実は面白くて、今いろんな方式が存在していまして、例えばグリーDecodingBeamサーチ、ランダムサンプリングなどの方法がありましてこれはそれぞれ特徴があってこれもこれで重要ですここからそのDecoderの方法について少し紹介させていただければなと思います。

[1:34:06 - 1:35:04]
一つ目のグリーンDecodingのDecoder方式なんですけれども、これは各単語においてそのときそのときで一番高い正規確率が高いやつを選んでいくという方法ですねこのグリーンって日本語で言うと同玉っていう意味ですよね一番高いのをどんどんどんどんどんどん選び続けていくという方法ですね例えばここの枝は野津5例にとってみますとまずTheでは最初の1単語目だったとしたら次、ドック次ありえなさん3択あると独が0.4、ライスが0.5、Carが0.1であった場合には、ドックではなくて一番高い、直近で確率が一番高い内装を選ばれると永安が選ばれた後にさらにその先に山とかっていうBloomハウスが言ってあるんですけども、その中でも一番確率の高いBloomを選んでって、ザライス有無ってこんな感じで選んでいくってのがアプリDecodingです。

[1:35:08 - 1:36:49]
これグリーDecoding若干、必ずその一番高い確率を選んできまましたこの決定的な出力になるんですけれどもそれだけだと必ずしもいい文章ができないという問題もありましてそこを解決するためにビームサーチっていうか、新たな手法が、また別のDecoding手法がありますとこれはですねブリーDecodingといって先だけしか見ないんですけどもこれはそうじゃなくてもうちょっと先まで見て二転三転先まで見てトータルで見てこの確率が高くなるようなあの組み合わせを探していくという方法ですね例えば先ほどの例で言うと、フリーDecodingatTherice万と1万1000冊しか見ないでやっていますけれども、これ、今度はそれだと0.5掛ける0.40だな通愛翔0.2、トータル0.2の確率なんですが、もうちょっと2.先まで見てみると、今度はザロックはずの方がロックが0.4で、外が0.9なので、0.4掛ける0.9で0.36、さっきのTheniceウーマンの0.2のトータル確率よりかも、ざっと区外の方がトータルで確率が高いこっちの方がきっとより良い文章を作れるだろうはずだっていう考え方で、やっぱりこっちを選ぶというそういう方法がPMSearchとなりますこれ右側はこの日Mサイズ3のとき、つまり3手先まで見て考えるときの例なんですけどもちょっと時間の都合上、すいません割愛させていただきますこちらにリンクがありますので、興味のある方はそちらのピンクをたどって見ていただければなと思います。

[1:36:51 - 1:37:19]
最後ランダムサンプリングという決定的ではない。方法のDecodingの方等もあります。これは次のトップの正規確率の分布に従ってランダムに選択していくという形ですこのやり方には重要なパラメータが三つありまして、一つ目はトップPトップ系最後Tempalayちゃんと三つあります。

[1:37:19 - 1:37:44]
これはそれぞれです。最後紹介していこうと思いますトップPってのは全部の単語中に確率分布があるんですけれどもその中の上位1%はトークンだけから選択するようにするっていうことですねなので確実に低いところはもう選択肢にそもそも入らないと、確率の高い中からこれとランダムで選ぶとなんで、完全ランダムではなくてある程度それっぽい多くの中から選ぶという方法ですね。

[1:37:45 - 1:38:05]
トップ系も似たような話なんですけれどもこれは確率で式足切りをするんではなくて、ランキング確率でランキング取ったときの上位稽古1時からKまでの所上位傾向のトーク中から選択する例えば、10セッター設定した1位から10位までの間の受候補の中から選ぶという話ですね。

[1:38:06 - 1:38:42]
最後に、Temperatureだけはちょっと性質が違うパラメーターでして、これはLlamaDAMさを調整するとパラメータですこれどういうことかといいますと、これ数式で書くとこの左側のようにこのSoftmaxをここの分母に掛ける数字をちょっと調整するって話なんですけれども、1だと普通のSoftmaxと同じで、1よりも少なくするとどうなるかといいますと、この右図のようにこれ真ん中がTA1のときだったときに、普通のSoftmaxと同じこれが通常場合で1より少ない。

[1:38:43 - 1:39:17]
値にするとこのようなシャープな方1位のやつがより1位に突出して、必ずこいつがどんどん選ばれて他のやつらにはもうチャンスが回ってこないというような分布になります。逆に1より大きい値2.0そういうの値を設定するとその他の確率分布がシャープになると反対に今度はなだらかになって一応分布に近づいて是善候補にこのチャンスが回ってくるとどれが選ばれるかよりわからなくなるということでランダム性が高くなるという性質を持っています。

[1:39:18 - 1:39:45]
このようなパラメータを駆使していろいろないろんな広報の文書を作成するといったことができます。結局この中でどうやって講座方式を使えばいいのかっていうとその答えは著状況次第でありまして、例えば分類問題を解く場合はこの決定的な回答を好まれますので、グリーDecodingというのが好まれて、ビームサーチってのは機械翻訳のタスクを解くときに見ることが多いです。

[1:39:45 - 1:40:13]
これはおそらくこの自然な文章を作るためには何何てかさて3.4手先まで見て文章を作る方がいいだろうというのが理由だと思われます。最後長文生成をするときはランダムサンプリング例えば、クリエイティブな小説を作るってときには、なんかずっと同じような回答してるとつまらないのでちょっとランダム性を使って振ってみるっていうのが目的にランダムサンプリングが使われることは多いということです。

[1:40:13 - 1:40:31]
はい。はい。ここまで事前学習の話でしてここから先発展的話題でたくさんいろんな話題があるんですけれどもこれ結構分量が多くてそうですね。こちらも面白いんですけれども時間とかすごい今回はすいません、割愛させていただきます。

[1:40:35 - 1:40:53]
はいということで、本日おまとめに入ろうと思います今日は、大規模言語モデルLLMの事前学習について紹介しました主に四つ業務を使っていただきたいことがありまして、一つ目が、言語モデルにおけるTrasnformerの位置づけについて説明しました。

[1:40:54 - 1:41:16]
Trasnformerってのはニューラル言語モデルの一つなんですけれども、その前の時代に使われていたRNN型の言語モデルが抱えていた解決を課題をうまく解決しました。そんなLLMで主流となっているTrasnformerモデルの構造について、次に説明しました。

[1:41:17 - 1:41:35]
まず、アテンション機構への活用してモデル構造でして、これはのおかげで、ワンステップで、前段互換との情報をが接続できるようになりました。こうすることで、RNN方が抱えていた二つの課題を解決することができました。

[1:41:35 - 1:42:01]
具体的には、一つ目単語間の長距離依存性が把握できるようになった二つ目、誤差逆伝播の計算ステップアップ町に依存しなくなり、つまりステップ数が短くなり、学習が安定化かつ高速化することができた。次、三つ目にそんなTrasnformerモデルどのように学習するかということで、LLMの事前学習について説明しました。

[1:42:02 - 1:42:21]
大規模コーパス大規模でテキストデータセットによる学習を行うことで、トレイルの汎用性を高めていると捉えることができます。具体的には、ネクスト、東プレで行く所という自己教師あり学習により学習させているということを説明しました。

[1:42:21 - 1:42:41]
最後に発展的課題については時間の都合上割愛しましたがこちらにデータモデル、各種評価分析について大きく四つのセクションに分けてそれぞれ細かいいろいろな面白いトピックを網羅していますので、時間興味のある方はを見ていただければなと思います。

[1:42:41 - 1:43:07]
では、お疲れ様でした以上で講義を長くなりましたがここで終了させていただきたいと思います。ありがとうございました。幹事さんありがとうございましたちょっと時間の都合上発伝的話題のところ触れられなかったんですがちょっとコミュニティのイベントでもらってぜひまた改めてお話聞かせていただけると嬉しいなと思ってます思ってますので、ちょっとまた別途相談させてください。

[1:43:07 - 1:43:25]
ありがとうございましたお疲れ様です。はい、ありがとうございました。はい、では続きまして、ちょっと時間ないのですみませんこのまま生かしてください演習の方に移っていきたいと思うので原田さんお願いできますでしょうか？はい。

[1:43:26 - 1:43:48]
お願いします。ごめんなさいちょっと時間があれなんですがよろしくお願いします。さすがに15分で説明はできないので、延長させていただくと思います。申し訳ないですがよろしくお願いしますすいませんちょっと皆さん次の予定がある方ももしかしたらいらっしゃるかと思うのでその方はちょっと途中で抜けていただいてまたアーカイブで追ってあの事故いただければと思います。

[1:43:48 - 1:44:18]
なのでちょっと延長する想定で進めさせていただければと思いますので、よろしくお願いいたします。お願いしますはいはい講義でPretrainingの話題が使えましたと、練習で学んだことを実装レベルで理解しましょうとTrasnformerの言語モデル自作したことがあるよと胸を張って言えるようになるのがこの演習の目的ですとただ分量としてもすごい詳しめに書いたので多いのではしょるところもあります。

[1:44:20 - 1:44:43]
そのためにですねちょっとGPT図を作ってみて、こんな感じで公道を投げれば、詳しめに解説してくれるボットみたいなのも作ってみたので、そちらも使ってみてください。というところで始めていきたいと思います今回GPU使う想定なのでGPUで動かすことを忘れずにお願いします。

[1:44:45 - 1:45:08]
まず皆さん実装で理解してほしいのが、その言語モデルっていうのはどう実装するのかそれをニューラルネットを使わずにどう実装して、生成であったり学習をするのかその言語モデルをニューラルネットワークを使って実装するってどういうことだろうそのニューラルネットワークをTrasnformerにするっていうことがどういうことだろうかっていうところをお話できればと思います。

[1:45:09 - 1:45:35]
今回使うデータセット事前学習講義でもあった通り事前学習データセットを大量にいろんなところから持ってくる必要がありますと。これのどういうデータを含めるかで性能も変わってきますとそのデータを集める際にWebから集めたりもするんですがそのときにどうフィルタリングするのかっていう話題もいろいろありますと僕が読んで面白かった論文とかこちらにまとめてるので見てみてください。

[1:45:36 - 1:45:56]
今回日本人の方がたくさん受けてるので日本語のコーパス使いたいと思います。LMJPっていうアカデミアの人たちが中心となって作ってる団体が整備したコーパスというものがあって、こちら合計1.7とBillonトークンぐらいあるデータセットの一部を使いました。

[1:45:56 - 1:46:28]
こちら見てみると英語のデータセットだったり日本語のデータセットだったりあるのでもし、何か自分でPretrainingがつりやるぞっていうときには一つ参考になるものかなと思います。フィルタリングの話どう実装されてるんだっていうところもわりと詳しめにそのどういうライブラリ使ってどういうフィルタリングでやりましたみたいなところも書いてあるのでぜひこのPretrainingデータをどう集めるかっていうところも結構エンジニアリングの見せ所で面白い分野であるので、気になる人は見てみてください。

[1:46:29 - 1:46:53]
今回は結構ミニサイズでやりますと、TRAINデータのwikiで、Wikipediaのページのトレイの一つのデータを使ってバリエーションをちょっと準備してやりました。実際に見てみるとこのような形でビッグエリアの子線、1000ページ分ぐらい持ってきてありますわ最初の100文字見てみるとこんな感じでやってました。

[1:46:53 - 1:47:08]
学習してるときに学習とは別のデータでモデルがちゃんと学習うまくいってるのかなというのを確認するんですがバリエーションデータとしても同じようなWikipediaで別のページ学習に使ってない別のページで見ていきたいと思います。

[1:47:09 - 1:48:10]
ここまでが早いんですが学習で事前学習データを集めたものですと実際に言語モデルや実装していきましょうまずはニューラルネットのネットワークを使わないでどう実装するかっていうところで抗議デモN-gram言語モデルというものが紹介されてましたとNが何個かNイコール1だと過去の文脈踏まえずに1単語1単語の出現確率でモデル化するものですとこのNが増えていくと過去見ていくコンテキストっていうのが増えていきますと今回はWikipedia1000ページ分、どれぐらい文字数があるかっていうと、156万文字ぐらいありますね文字の種類ユニークな文字の種類っていうのが3807文字ですとこれで156万文字ぐらいでトレイにデータがありますと簡単なゆ2gモデル1文字1文字の出現確率でモデル化する。

[1:48:11 - 1:48:30]
物にさらにどうするかっていうと、この全体の文字数の中で対象とする文字が何回出てきてるかで確率を定義します。なので日本の日という文字っていうのが、確率どれぐらいですかっていうのが出現回数を全体の文字数で割った値なりました。

[1:48:30 - 1:48:54]
なので日本とか日曜日の日曜の確率って言うのが湯2gモデルだと、日本のその質、確率と本の確率を掛け合わせた値で出てきます日曜って確率も同じように出てきますとここで見てみるとこの日本の確率の方が高いのでこのユニーBloomモデルでは日本という方が最もらしいだろうというふうに判断されるというのが流れでした。

[1:48:57 - 1:50:27]
このunigモデルそのそれぞれどれぐらい出現してくるかっていう確率が出てくるモデルなんですけどこのどれぐらいの確率で出てくるか、トップ何個か見てみたものがこちらになっていて、一番多いものっていうのが空白文字でこれぐらいの確率で出てきますとそれをその分効果しかしたのがこちらの図ですなのでここからランダムにサンプリングするっていうときはこの確率に従ってサンプリング生成していきますという流れになります講義で使ったんですがグリーDecodingっていうものがどういうことか見てみると、この分布主の中の一番最大のものMaxを持ってきて取ってくるので今回のunigとくに文脈考慮しないので空白文字ここのトップの空白文字を持ってくることで20分を生成させてるんですけど、全てそれが選択されますとなのでこのような先生になりますっていう流れになりましたトップPサンプリングというものもあったんですが、これがその子の合計して割合がトップPになる語彙数5位の中からサンプリングするっていうのがこちらにあって、そうするとその生成ごとに出てくるものが違うというところが確認できるかなと思いました。

[1:50:27 - 1:51:18]
場面場面に応じてどういうサンプリングを使って生成するかっていうところが、デザインの余地になるかなと思います。簡単に言う2gを説明して、したんですが次遠くないTheっていうのが重要ですとこちらもいろいろライブラリもあって、あってどういう設計にするかってのも重要ですと、今回は1文字1トークンの宮内座を作ってみましたと長々と書いてあるんですが、要するにしたいのは、この文字が与えられたときに、どのトークン番号になるかっていうのを返してますと、これ一文字一文字、一文字1トークンでしたよねこの限定言葉が3310番のトークン番号になりますよ。

[1:51:18 - 1:52:18]
5っていうのが、3354番のものになりますみたいな形でテキストが与えられたときに、番号を振るっていうのが宮内座ですと都区内座で気をつけないといけないところがその設計によってですねこの1文字1トークンの場合は、作った辞書のときは、同じように出てくるんですが、ないもの、学習用データになくて、辞書に登録してないものっていうのがどういうふうになるかっていうと、この1文字だとしても4トークンぐらいで出てきましたそれがどうされてるかっていうと、UPFのっていうエンコーディング公式があるんですけどそれの番号でトークン化されて、未知語に対応するためにはこのように、工夫してますという話があって遠くないTheも奥が深いんですがこちらぜひ見てみてください今回は1文字、大体1トークンでやりました。

[1:52:20 - 1:52:42]
それをもとに倍gモデル過去一つだけ考慮して言語モデルを作ってみるN-gramモデルを作ってみましょうとしてみると、どのペアがどの2文字のペアが出てきますかっていうところで、トーク後258番と259番の組み合わせってのが1万5000回ぐらい出てきます。

[1:52:43 - 1:53:26]
0これが526番と改行文字、258番のトークン番号ってのが1万回ぐらい出てきますと、倍gもこのように数え上げでどのペアが出てくるかっていうところが計算できますとそれをもとに生成したときに勝つかっていうのが入れたもので、勝川の次は裏が大きい7次あれが多いみたいな形でこちらグリーDecodingで一つ前から、次何が一番出てくるかっていうところをモデル化するとこの先ほどは空白文字BERT出てきたんですがなんか、ちょっと日本語っぽくなってきましたという形ですね。

[1:53:29 - 1:54:01]
こちらはちょっと割愛するんですがこのN-gramを増やせば増やすほど文脈増えていくんですが、考慮できる文脈が増えていくんですが、こちらNを増やすと、このパラメータ数というか、その数え上げ、その組み合わせ表みたいなのが膨大になっていきますよっていうのもこちらで値を変えていくとわかるかなと思いますと。

[1:54:01 - 1:54:29]
それでそのうまく学習用データにない組み合わせとかがすごい低い確率になったりするのが課題でした。いう形になりました。ただ、4四つぐらいまで考慮すると学習用データっぽい言葉を話すようになるっていうのが、N-gramモデルですなのでニューラルネットワーク使わなくてもこういうふうに言語モデル実装できますよというところを確認しました。

[1:54:31 - 1:54:51]
本来のニューラルネットワークを使用しましょうそのときのモチベーションが、その単語同士の類似その類義語とかが考慮できないというところがありましたとしそれをどう実現しますかねっていうところで単語ベクトルトークンベクトルWordエンベディングっていう話があったと思います。

[1:54:52 - 1:55:58]
それをどう実装していくかなんですけどのようにトークン番号を付与するところまでは今までと一緒ですと、学習するときに、536番の次は1032番、この鍵括弧の津谷勝勝の数が来て、勝野翔理っていう字の次篤人ネクストトークプリティクッションが、インプットがこれでターゲットがこれですという形でデータセットを作ってあげますとこれらのトークン番号っていうのがどういうふうになりますかっていうのがこの単語ベクトルの取得というところで、準備した後椅子×その単語ベクトルの次元で、単語ベクトルのその、何でしょう、でっかいテーブルみたいなのを準備してあげますとそのテーブル番号のこの営業を持ってきますよっていうところで536番を持ってきますよ。

[1:55:58 - 1:57:08]
536と1032と581番の行持ってきますよってすると、このように定義してあげたよ次元のベクトルがそれぞれ出てきますとそうすることで単語ベクトルを準備してあげてこれをニューラルネットワークのこの短単語IDに応じトークンIDに応じてこのベクトルを入れていきますと生成する際にはトークンValue5みたいなのが出てきますと流れをまたまとめて整理するんですけど、入力鍵括弧書きますと、トークンIDが536番ですと、これに対応するトークンエンベディング単語ベクトルっていうのがこちらですとそれをニューラルネットに入れて、次の単語当てないといけないのは1032番が1っていう確率他はゼロっていう確率を、合うようにこの確率分布を近づけていきたいですと何も学習してないモデルだと1032番がほぼゼロ、200280077番が0.01みたいな確率分布の絵が出力されますと、それを近づけますよという流れになります。

[1:57:09 - 1:57:37]
ニューラルネットワークの構造どうなってるかというと、このベクトル化したものをここでは一層ニューラル戦型相があってレベルの非線形関数があって最後にもういっそあって、忘却細部サイズその遠くない座で登録したトークン番号分出力確立するこす出力するための出力層がありました。

[1:57:38 - 1:58:56]
なのでエンベディングを得て、その確率のスコアのものを出力してあげて、それがターゲットと合ってるかどうかっていうのをこの黒線とロッピーによってロスを出してあげますというのが流れになります。こちらがその学習のループを書いたものなんですけどちょっとはしょるんですが流れとしては仁藤くん番号を得て、ロスをその次の単語当たったか当たってないかを得てモデルの重みを更新していきますとここで注意してほしいのが、このニューラルネットワーク全体を学習するというところで、このワードエンベディング最初はランダムに初期化したトークンベクトルなんですが、語彙数分それがあるんですけどそれもランダムに初期化してるんですけど、次単語予想をひたすらすることで、そもそもこの単語ベクトルっていうのがどういうエレクトロであるべきかみたいなところも学習が進んでいて、だからこそこのトークンとこのトークンの表現っていうのは知覚するべきだ同じような主語を表す単語だしみたいな形で、学習によってこの単語ベクトルも獲得されていきますと、これがend-to-endで学習されていくっていうのが流れになります。

[1:58:58 - 1:59:39]
実際に学習させてみるとトレーニング直すとバリエーションますが下がっていって、同じようにサンプリングしてみると、何かしら出てきますという形ですね。このロスっていうのがありますとここのロスどれぐらいまで下げられるといいのかなっていうところで、何も学習してない当てずっぽうモデルのロスというか、登録したエゴイストからランダムに4,064個あるものからランダムに出してきたとき、ロスが8.3ぐらいですと。

[1:59:39 - 2:01:32]
なので、学習すると何か下がって、最初は何も学習できてんでないんでほぼほぼランダムなんですけど、ロスが下がってるっていうのが、その当てずっぽうさっていうのが下がってるかなと思います。どこを目指すべきかっていうもので一つ言われてるのが、このロスの2を切ると、タスクを途端に解けるようになってくるみたいなところもあるので、モデルを大規模化するなりして、ここの実を切れるようにみんな頑張ってるかなという形ですというところで演習少し準備してるのが実際にこのMLPの層増やしたり、学習率とかを変更して、次の単語予測当たるかなと見てくださいと倍gニューラルネットワーク使わぬ使わずに出した性能が5.08ぐらいバリエーションでいくっていうのはわかってるのでとりあえずそこまでいけるかどうかみたいなのをちょっと頑張って実装してみてくださいとここまでがニューラルネットワークで言語モデルを実装するときに、まずワードエンベディングと単語ベクトルっていうものにを入力して、次の単語の予測確率を出力するようなネットワークを組んで、それが実際に準備したターゲットと合ってるかどうかっていうところから学習して、end-to-endでモデルを更新しますとただ過去の文脈今回は今組んだモデルは1トークン前しかというか、今のトークンから次のトークンを予測するっていうところしか見てなかったので、より過去の文脈踏まえたいという課題がありますとRNNリカレントニューラルネットワークでどういう流れになるかみたいなのもちょっと書き出してみたので確認してみてください。

[2:01:34 - 2:01:57]
注目してほしいのが、このRNNの過去の文脈踏まえられるんですけど、ただその過去の状態っていうのが一つのこの隠れ状態のベクトルでしか表現されないんで、今までの所のままの表現が一つのベクトルでしかずっと伝わってこないので、過去の情報というのが忘れられちゃう。

[2:01:57 - 2:03:11]
そしてネットワーク方向が広くなっちゃって学習が安定しないみたいなところが、実装からもわかるかなと思います。本題のTrasnformerでモデル化しましょうと過去の文脈もうまく踏まえつつ、学習も並列化することによって高速にやりたいですよねっていうモチベーションからTrasnformer、特にアテンションを実装していきましょうとアテンションの式抗議デモ出ましたが、QKVアテンションっていうのが、Softmaxの中でQっていう表現行列とKの表現行列の計算した後に、ある値で割ってあげますとそれでSoftmaxで出てきたこのスコアをもとにValue行列かけて上げるっていう処理がテンションですとこれをただ単に取り出してみると、ここはランダムに初期化してますとバッチサイズ1でトークン数が四つほど、アテンションヘッドの次元数が2次元ですというところで1掛け4掛け2の表現行列みたいなのを作ってあげます。

[2:03:12 - 2:03:45]
それをQKVそれぞれ準備してあげて、この式QとQAのトランスポーズしたものの掛け算このように定義できますとそれをある値で割ってあげますとその後にSoftmaxをかけてSoftmaxをかけてあげて、スコアが出ますとこれがテンションスコアですね、このアテンションスコアをにValueの強烈かけてあげると、アテンションの計算が行われますと形ですね。

[2:03:48 - 2:04:14]
ここで何でこれで終わる必要があるのかっていうの講義でも少し触れられていたんですけど、簡単な例で見てみると、スケールしない割り算をしないとき、このようになります割り算したらこうなりますと、二つ例を出してるんですが、割り算してあげないとちょっとこの分布っていうのがちょっとPというか山が尖った。

[2:04:15 - 2:04:47]
ふうになりますと、これが学習初期からそうなってると、特定のところしかテンション張ってないようになってしまうので、それよりももうちょい広くアテンションいろいろなところに最初は注目してそこから学習によってうまくいい感じにテンションかけれるようにしたいので、そこのそのなだらかな山を、最初の時点では初期化の時点では担保しておきたいっていうところもありまして、スケールというかここで待ってあげるっていうのがありますというちょっとした話でした。

[2:04:49 - 2:05:17]
フィードフォワードそうもう一つありましたとMax0でここで一つ重みとの掛け算がありまして、それをもとにまた別の重み行列で足してあげるってのがフィードフォワードそうでした。MLPでの実装でも見てみたんですが、次元が入力が入ってきて、掛け算料率の掛け算があって、ここのレール関数というのがこのMaxオペレーションですね。

[2:05:17 - 2:06:49]
これでMax取ってあげて、その次にもう1回重みEOSとかけてあげるっていうのが、フィードフォワードそうですとこの二つの主に二つのテンションとフィードフォワードネットワークでTrasnformerっていうのが実装されますとこれがどれぐらい大規模化、大規模に積み重なってるかっていうところで、こちらがGPTtoスモールのパラメータ数とか出してみたんですけどまず、トークンされる番号というのが全部で5万個ぐらいありますと遠くのワードエンベディングの広さってのが768ぐらいありフィードフォワードは大体4倍するのでやっぱりですとアテンションヘッド1個のTrasnformerの一層のアテンションで何個ヘッドを準備しますかっていうので12個ありますとそれを12層重ねますというところで計算してみると、GPT2Small124Billonぐらいのパラメータ数モデルサイズなんですけどそのうちにどれぐらいパラメーター書くコンポーネントであるかっていうのはこちらでし見てみますと、ここで見てみるとセルフアテンションとフィードフォワードそ大体1対2ぐらいですよねっていうのが確認できて、ここがパラメーターすこれもっと大きくすればもっと割合増えていくんですがここに知識が入ってるんじゃないかみたいなことが言われてたりします。

[2:06:50 - 2:07:47]
はい。一旦ここまででセルフアテンションとフィードフォワードそうやりましたと。ただここここだけ言われても多分わかんないですとこれが系列が入ってくるときに実際どうなるんですかっていうところをまず書き下しましたこれが並列化してない1トークン1トークンずつ処理してるものですとこれを追ってみると、確かにセルフアテンションというかTrasnformerはPの事情分系列地方の事情分で計算が必要になってるんだなっていうのが、こちらの実装を見てみるとわかると思いますさらにメモリが結構あの、今までの計算結果みたいなのを保持しておかないといけないトークンの保持しておかないといけないっていうのがあるのですごいメモリもくんだけ並列系列町が増えるとメモリも組んだなっていうところが実感できるかなと思います。

[2:07:47 - 2:08:18]
簡単に説明するんですが、まずトークンIDを持ってきますと、トークンIDかこの系列も保存しておきますとそのトークンIDに応じてトークンベクトルを得ますと、ここで石部1のベクトルも足すことが多いですここで渡してないんですが簡単なため出してないですと入力のベクトルを単語ベクトルを得ました次にテンションかけます。

[2:08:18 - 2:08:40]
ここで、この入力からの入力の単語ベクトルにQueryに変換する重み行列をかけてあげます。こうするとQueryベクトルが出ますQueryが何してるかって簡単に説明すると、何かを探してますというのを表現するようなくベクトルでした。

[2:08:40 - 2:09:09]
これちょっと気持ちがお気持ちで説明してます。例えばトークン0ってのは自分主語ですと、目的語とか上司とか同士とか探してますみたいな感じでQueryベクトルがこの入力ベクトルから作られます。次にKeyベクトルが同じ入力からだValue系っていう重め行列を経て作られますとこれが作られると、このKeyベクトルって何を表してるかっていうと何々を持ってました。

[2:09:11 - 2:09:43]
いうところが出てきますね。あるトークンでは自分同士ですと、あるトークンで自分を主語ですと、私はこういう属性を持ってますっていうところがKeyベクトルで出てきます。次にまた同じ入力ベクトルからWVっていうのが出てきますValueベクトルを出しますこちらが何してるかっていうと、私ってこういう中身の詳細してますというところが、そのValueなのであったり、このKeyが対応する値が何かっていうところでValueが何かっていうところが出てきました。

[2:09:46 - 2:10:28]
それぞれQueryとKeyとValueがそれぞれのトークンで出てきます使ってる重みは全て共通ですただ入力が違うのでそれぞれ違うQueryKeyValueが入力に応じて出てきます。過去のものも保存しますとそれをもとにアテンションスコアを計算しますと、QueryとKeyの値から、注目すべきトークンがどこかアテンションかけるべきトークンがどこかっていうところでスコアが出てきましたそれをもとに、Valueベクトル、どこに注目した上で表現を次に伝えましょうというところで、出てきました。

[2:10:28 - 2:11:11]
なので、ここのアテンションスコアどういうことをしてるかっていうと、第2回でRetrieverるを決めた時とジェネレーションで類似度分を探すみたいなところで、類似度計算みたいなことをしたことをしたかと思うんですけど演習でそのときのようにQueryとKeyの類似度があるのをそこで計算してるので、関係してそうなところ、ここで取ってきてるわけですねそれがそれぞれのトークンで出てきてるので、関係性が強い、つまりアテンションスコアが高いものっていうのが今注目すべき表現でしょうとそれをもとに、その重みづけをもとに表現を更新していきますという流れです。

[2:11:13 - 2:11:52]
その出力が得た後にフィードフォワード層にその表現を入れてあげることで計算しますと最後に出てくるのは、そのTrasnformer全部の処理を終えて出てくるのは、次の単語がどれか、その単語分用意した単語トークンID分どの確率が一番高いですかっていうところ、ええたりしますとそうすると、過去の文脈が増えても、次がいいな次のがあった次の確率みたいな形でそれが計算されるようになってます。

[2:11:52 - 2:12:34]
それを同じようにさ、MLPでの実装と同じように学習によって、この出てくる各率を近づけていきますそれをend-to-endでやりますっていうのがTrasnformerになります。アテンションスコアアテンションの可視化っていうところで講義でもあったと思うんですけど、最初のが入ったときに自分しか見れないので、注目するのは自分だけなので1ですと2文字目が入ったときに、アテンションスコアっていうのが、合計すれば1になりますと、その地域に注目してるのが62%自分自身には37%注目しましょう。

[2:12:35 - 2:12:57]
3文字目入れたときは今までのそれぞれこれぐらい注目しましょう。最後に入れたときにはこれぐらい注目しましょうみたいなのが出てきます。なのでテンションアップ、この値をもとにヒートマップを作っていてここが高いほど赤色であったり低いほどその色が薄かったりみたいなのを可視化したのが、アテンションマップです。

[2:12:58 - 2:13:21]
このアテンションを見ることで、ここのこことここの繋がりが強いから、何かしらこういう処理をしてるんじゃないかみたいな形で、その解釈性を見るときに使うのがテンションアップだったりしますその本当にアテンションマップで解釈性が高まるかみたいな話は、第10回中1回ぐらいで詳しく話があると思います。

[2:13:23 - 2:13:49]
時間がちょっと押しすぎなので、最後、並列化するために、先ほどは1トークンずつやってたんですよねただこれだけだと、RNNみたいにRNNも1トークン1トークン入れてないと次の層の計算ができないので処理としては1トークン進むごとに次のゾーンを走るっていう感じなんですけど、その並列化RNNとこれしかできませんと。

[2:13:49 - 2:14:15]
ただアテンションのすごいところはそれを並列化できますと並列化って言ってんのが入力本入れたらその分計算が一度にできるので、層が積み重なったとしても一気にどんどんどんどんできるので並列化行列の計算で全て計算一緒にできますというので、それを実装したのがこちらで詳しくはちょっと後で見てみてください。

[2:14:15 - 2:15:08]
ただ、処理のときに重要なのが、この下三角行列で未来の情報をマスクすることで、一気に計算ができるようになりますというところで、その未来の情報見れないこれが最初の1単語を入れたときに見た、2回目入れたときっていうところで、最初未来の情報見れないようにした三角行列ですって導入しますと計算したこの行列のところに未来の情報見れないように、未来の情報をマスクしてあげますとこれすごい小さい値を取ってあげるとSoftmaxかけてあげるとこのスコアっていうのがゼロになるので、なので講座るアテンション未来の情報見れないものを担保しつつ、学習のときに並列化するためにこのような工夫をしていますというところであります。

[2:15:10 - 2:15:35]
はい。ていうのをπ統治っぽく書き換えたのがこちらになっていますし、処理としては今までのMLPでの実装と変わらずに、トークン番号を得ますと、トークン番号に応じてトークンのワードエンベディングをいます。入れましたワードエンベディングとそのトークンの場所を表す。

[2:15:36 - 2:17:03]
1ベクトルを入れてあげて、それをもとにアテンション、入れてあげますとこれ一層のトランスアテンションだけのLanguageモデル作ってみたんですが、そこから出てきた表現を元に語彙数分の確率分布を出してあげてって今までと同じく、その確率分布が将来の次の単語と当たってるかその分布が合ってるかっていうのを、クロスエントロピーロストって上げて学習してあげますというので、他は最初にMLPで実装したときと同じですとこれが一掃でのTrasnformerテンションなんですけど、それを深くするために、深くすると学習が安定しなかったりしますと、そのためにデジタルコネクションとかLayerNormalizationとか追加することでテンションFOMAのブロック深さ方向へ積み重ねていけますとそれを実装したのがこちらGPT通の再現実装こちらになりますと、大体今までの話を理解すると読み解けるんですが違うところとしてそのLayerのその学習を安定させるLayerのルームとか入れてますとレジであるコネクションれず3れずネットとかで使われてるデジタルコネクションはただ単純に元の入力とアテンションの表現を足してあげることでできますと。

[2:17:03 - 2:18:02]
なので差分としてはここだけでずネットとLayerのもありますとこのLayerのルもかけてあげて実装してますというのがGPT通です。いう形でなりましたと、ちょっと駆け足になって申し訳なかったんですが、これで言語モデルがどういうふうに学習するか、それをどう生成するか、先生に使うかっていうところを説明しましたTrasnformerテンションで学校のトークをどう踏まえることができるかっていうのを実装でも確認しましたと実際にその学習を安定させるためには、そのアテンション機構を持つことで、過去のトークが見れるんですけどそれをどう深く深い方向にするかっていうところでれずネットとかレジでコネクションとかLayerのルームをすることで学習の安定性を保ったまま、深くすることができるので表現力高く巨大化していきましょうといういう話に繋がっていきます。

[2:18:02 - 2:18:19]
なので、実際にですね、自分でTrasnformerモデル講座をいじってみて性能向上をどこまでできるのかGPT2で報告されてるようなモデル講座を使うと、自分が作ったモデルとどれぐらい性能が変わってくるかっていうところをご自身で確かめてもらえればと思います。

[2:18:20 - 2:18:48]
さらにTrasnformerの内部実装どうなってるのかっていうところで、詳しめの資料も参考として載せてるので見てみてください。はい。ここまでで円周部分は以上になりますんで、最後、質問あったら、一、二問ぐらい演習の部分ではないのか、なさそうですかね。

[2:18:51 - 2:19:36]
ちょっと量が多くて大変だったかもしれませんが、適宜、ChatGPTとかにこの実装解説してみたいな形で聞きながら、実装してもらえればなと思いますと僕も何回もTrasnformer写経したりしてるんですが社協その実際にこの行動を見ながら自分でも真似して書いてみるっていうところ、するとまた理解が深まるかなと思うので、ぜひてくださいというところで演習では、実際に言語モデルN-gram使ってみて、ニューラルネットワークを買ってそれをどう表現するかで、そのニューラルネットワークをTrasnformerにするとはどういうことかを扱いました。

[2:19:36 - 2:19:57]
次、はい市は以上になります。はい、原田さん、上石さんありがとうございました。皆様ご受講お疲れ様でした。小栗キャンパスにログインして出血アンケートに回答してください。提出締切は本日から1週間後の水曜日17時です。

[2:19:57 - 2:20:12]
出欠アンケートの提出をもって出席といたします。また本日の宿題を公開しました。宿題のファイルのピークの先や出席宿題の提出方法は受講の手引きをご覧ください。受講の手引きへのリンクは受講のお知らせメールに記載されています。

[2:20:14 - 2:20:18]
それでは講義を終了いたします。本日はご受講いただき、ありがとうございました。
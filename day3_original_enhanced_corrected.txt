[0:00:00 - 0:00:25]
皆さんこんばんは松尾研の川崎と申します本日は の大規模言語モデル講座Day3Pretraining会にご参加いただいて りがとうございます。ですねちょっと冒頭の僕の方からいくつかまたご案内させていただければと思います。のでちょっと5分ほど少々お付き合いいただいてですね、から の講義の方を釣っていければなというふうに思っておりますので、どうぞ本日もよろしくお願いします。

[0:00:26 - 0:00:57]
はいアナウンス事項がいくつかございましてですね、まずこれまでのコミュニティの中での交流について皆様活発的に交流いただいて りがとうございます質問対応にに関してもですね他の受講者の方々が回答いただいたりとか、運営に対する問い合わせに対してもご対応いただいたりとかしててかなり助かっており、おりますというところで、改めて御礼申し上げます。

[0:00:57 - 0:01:16]
引き続き、コミュニティでコミュニケーションを取りやすい環境を作っていきたいと考えていますので、活発な交流をいただければ幸いです。よろしくお願いいたします。また、次のアナウンス事項として、もくもく会を実施できればと思います。

[0:01:16 - 0:02:07]
ちょっと今リンクを貼りましたが、6画面共有しようかと思います。少々お待ちください。今リンクに貼っておりますので、講座のもくもく会を企画しています。9月26日木曜日の20時から20時半まで、オンライン上で集まっていただき、各自自分の課題や演習に取り組んでいただける形で実施できればなと考えています。ちょっと初回なのでいろいろ試行錯誤しつつ進めていくと思いますが、ぜひ皆さんご参加いただければ幸いです。

[0:02:07 - 0:03:27]
はい。ちょっとこのコンパスのリンクから申し込みができますので、ぜひ皆さんご参加よろしくお願いします。というのが二つ目で、三つ目としてはですね、こちらの同じようにリンクを貼ります。GENIACというプロジェクトでタヌキモデルというLLMを開発したんですけど、それをDay2の演習で使いましたというような無料で使いましたというようなMさん、GENIACの小滝チームの開発メンバーの1人です。でも、こちらにまとめていただいてるというような記事を作ったりとか、していただいてありがとうございます。他にも、複数のK山田さんとか、他の方々も記事を立てていただいてるので、ありがとうございます。ちょっとこういう形で皆さんで、DBを充実させていければなというふうに思ってますので、どなたでも編集追記可能なので、どうぞこちら活用いただければというふうに思います。ので、よろしくお願いします。

[0:03:28 - 0:03:54]
はい続きましてチャットポットのご案内になります講座の質問用のボットとしてですね、チャットボット提供しておりますが、ちょっと初回にお伝 した通り我々も何て言うんすかねアップデートしながら皆さんの声を聞いてアップデートしながら開発を並行して進めておりますがちょっと新機能というか、一部回収しましたのでそちらのご案内させてください。

[0:03:55 - 0:04:21]
はい新機能というか新しい部分としてはですね今日このここまでは一緒で、質問範囲のところに運営業務の項目を追加しましたこれまで何か事務系の の質問ですよね例 ば開催日いつですかとか手引きに書かれているような情報、 の全部URLなんですかとか、そういったように、そういったような質問に回答できるようになりました。

[0:04:23 - 0:05:29]
ちょっと試しにやってみましょう。はい。こんな感じではありますが、まだ全然完璧じゃないんですけども、これまでの回答できなかった部分に関しては、対応できるようになったので、皆さんお使いいただければなと思います。また何か気になる部分もあると思うので、ぜひアンケートに項目があれば、そちらにコメントいただけますと幸いです。改めてのご案内なんですが、このチャットボットは基本的にワンスレッド1問の質問で使っていただければと思います。というのも、これまではデータをどんどん蓄積して、ちゃんと今後も改善するようなサイクルを組みたいと思っているので、基本的に一つの質問室に一つの質問が収まっているという形を保ちたいので、異なる質問をする場合はこちらから新しい質問を実施していただけると助かります。

[0:05:30 - 0:06:03]
解決しましたか、しなかった場合は明日家に下さないか、関わらずに質問してください。使い終わった後は、このフィードバックをしていただくようにお願いします。グッドを押すと、そのままこの質問はチケットを閉じられます。バッドを押すと運営側に通報というか連絡が行って、そこから可能な限り個別で対応できればというようなフローを進んでおりますので、必ずこちらかどちらかの選択をしていただくようにお願いいたします。

[0:06:04 - 0:06:37]
はい。というようなところでございます。 例えば、再出国告知なんですが、まだ計画中なんですが、近々にLM講座に参加されている方々を招いたオフラインイベントを実施できないかと考えています。まだ計画中なので、具体的な日程が決まり次第、お知らせできると思います。より密なコミュニケーションを取り、楽しく有意義な講座にできればと思いますので、引き続きよろしくお願いします。

[0:06:37 - 0:07:35]
はい。ということで、本日の講座に移っていきたいと思います。本日は、Pretraining会というところで、事前学習の経験がなかなかできない部分があるのではないかと思います。そこで、その講座を進めていきたいと思います。講義パートからですね。川西さん、GENIACのプロジェクトでは、開発支援チームのメンバーとして、フェーズ1のLLM開発コンペティションにおける各チームの開発の事前の環境やスターターキットのようなコードを提供していただいた管理士さんが講師をさせていただきます。その後の演習は、前回のRAG Promptingの講義の主講義を担当された松尾・岩澤研の原田さんにお願いしたいと考えていますので、よろしくお願いします。

[0:07:36 - 0:08:04]
はい。そうしましたら川西さん、準備よろしければ始めていただければと思います。がいかがでしょうか？はい、大丈夫ですよろしくお願いしますよろしくお願いします。では画面共有させていただきます。それ今、スライドショーの画面って見 ていますでしょうか？はい、見 てます。

[0:08:04 - 0:08:18]
ちょっとページを進めてもいいですか。内容が変わるかどうかだけ確認させていただきます。はい、ありがとうございます。よろしくお願いします。はい、本日は前半の工期の交渉を担当させていただきます。川西と申します。

[0:08:18 - 0:08:52]
後半の演習の方を原田さんが担当していただきます私は松尾岩沢研究室では、LLMの学習に使用するCoTの開発などを行っていました。例 ばはい例 ば先ほどの川崎さんにご紹介していただいた通りGENIACというプロジェクトによって、各チームがLLMの学習のに使用する際の行動のスターターキットとなるような行動の開発などを行っていました。

[0:08:53 - 0:09:50]
簡単ですが自己紹介をさせていただき、ちょっと今回の分量が多いので早速内容に入っていこうと思います。はい、今回の講義はDay3の事前学習の講義で、今回は大規模言語モデル（LLM）の主要なモデル構造であるTransformerとその事前学習の仕組みを理解することを目的としています。この講義が終わった後の目標は、まず言語モデルにおけるTransformerの位置づけについて説明できるようになることです。次に、LLMで主流となっているTransformerのモデル構造についても説明できるようになること、そして、LLMの事前学習のパイプラインについて説明できるようになることです。最後に、LLMの事前学習を実際に実装できるようになること、これらを目標としていきます。

[0:09:57 - 0:10:13]
講義全体の流れは大きく四つの章にわかれていまして、まず最初に言語モデルとは何かということを説明した後にその中で重要なモデル構造で りますTrasnformerの具体的な細かいモデル構造について細かく見ていこうと思います。

[0:10:14 - 0:10:36]
次にTrasnformerの構造を知った上ででは事前それを学習するための事前学習はどのような流れで行うかを解決し最後に発展的話題のセクションに移っていくことを思います最後、演習の方は実際に牌トーチを用いて、Trasnformerを実装し、さらに実際に学習してみようというような内容になっております。

[0:10:41 - 0:11:25]
では早速内容の方に入っていこうと思います。まず最初に言語モデルとは何か、というのを復習も兼ねて解説していこうと思います。まず言語モデルとよく聞くTransformerという言葉の関係なんですけれども、この図で表したように、まず大きく言語モデルというのが、その中でディープラーニング、つまりニューラルネットワークを用いて言語モデルを構築しようという試みが、その中でもニューラル言語モデルと呼ばれるものでして、さらにその中でも昨今一番うまくいっているニューラルネットワークのモデルが、Transformerと呼ばれるモデルとこのような位置関係になっています。

[0:11:26 - 0:11:44]
Trasnformerってのは近年の大規模言語モデルでもほぼ全て一般的に使用利用されている非常に有名なモデル構造となっています。ですので、このTrasnformerの利点や高めの構造などを今回、細かく見ていこうという感じです。

[0:11:50 - 0:12:10]
これは復習になるんですけれども、言語モデルとは何だったのかというのを押さえることも兼ねて解説したと思います。まず、この単語の系列単語の並び、つまり文章の生成確率を予測するものをモデル化したものが言語モデルといいます。

[0:12:11 - 0:12:49]
ただし、この各単語間と同時に、単語1勝単語2勝ってとこ全部の単語を同時に発生する確率ってのをいきなり求めるのは難しいので、連鎖率条件付き確率での掛け算で分解したモデルを自己回帰言語モデルと呼ぶようになりましたので、一気に求めるんではなくて、まず単語1単語名の確率は何かとその1単語目が、来たら2単語目はこれだろうという確率を掛け算してってそれをどんどんどんどん繰り返していくとこういう分の掛け算で分解していくというようになったしました。

[0:12:50 - 0:13:16]
このように分解してこの条件付き確率がそれぞれわかると、今度は文章の生成をすることができるようになります例 ば、入力に2本のシュートはこの4単語入ってきたら単語メールに次何の単語枠なのかというと、東京パリ帰ろこのような選択肢が った中では東京が一番最もらしい確率が高いだろう。

[0:13:16 - 0:13:51]
というふうな計算ができるようになります。そうしたならば、日本の人はBERTを導入したら、次に「東京」と出力したら、文が完成するような感じで文章を作成する流れができるというものです。この仕組みを数式で書くとこのような形になります。単語1単語目に入力された単語Meta…といった単語の次に、最も高い確率で来る単語はどれでしょうというような問題を解くことが言語モデルの役割です。

[0:13:53 - 0:14:26]
この条件付き確率を、さまざまな方法で推測したいわけですが、その中でもニューラルネットワークを用いて近似した方法をとったのが、ニューラル言語モデルと呼ばれます。ニューラル言語モデルを見ていく前に、ディープラーニングよりも前にあった代表的な言語モデルについても見ていきたいと思います。

[0:14:27 - 0:15:41]
これはどのように行っていたかといいますとこの条件付き確率を統計的に文章の出現回数をもとに求める方法が りました。これは大規模コーパスな大量の文章のデータセットの中の単語率の出現頻度から単純に出現回数から割り算して確率を求めていくという方法です出演回数をカウントする関数をシャープ((という関数でおいたとすると、この日本の首都は時田次に、東京都来る確率はどのように求めるかというと、単純に日本の首都はという文章がこのデータセット内に った回数とする分母にして、次は日本の首都は東京と実線ついた文書を実際にカウントしてそれを分子に打っており刺したこのような単純な内部な形で確立を求めるという方法が りました例 ば日本の首都はが旋回出てきて、その次に日本の首都は東京と実際についたのは200回やったら1,000分の200で20%、0.2というふうに確率を出そうという考 方のやり方です。

[0:15:43 - 0:16:11]
この方法には大きな課題が二つあります。一つ目がデータのSparseness問題です。これはどういう問題かというと、単語列が長くなると、その単語の出演回数が急激に減少し、条件付きのTwitterのような状況になると、全く同じ文章がどんどん減ってきます。これがSparseDense問題という問題です。

[0:16:12 - 0:16:35]
もう一つは、類義語問題という問題でして、これは意味が同じものの、形が少し違う類義語が別の事象として扱われてしまい、言い方を少し変えただけで、異なる出現頻度としてカウントされてしまうので、正しく条件付き確率を推定できないという問題になってしまいます。

[0:16:36 - 0:16:54]
例えば、日本の首都はという文章と、日本国の首都はこの二つ言ってことは意味は一緒ですが、別々としてカウントされてしまいましたので、正しく条件付き確率をそれぞれ求めることができなくなってしまうという問題がありました。

[0:16:57 - 0:17:19]
次にディープラーニング以前の代表的な言語モデルの一つにN-gram言語モデルがあります。これは直前のN-1個の単語を使って次の単語を予測する方法です。確率の計算方法は先ほどと同じです。

[0:17:19 - 0:17:45]
出現頻度で推定します。ただ出現の頻度を推定する範囲を全部の単語、これまで入力してきたZoomの単語ではなくて直近の数単語を使って、次の単語を予測するとこの点が違います。例 ば、Nイコール3の場合は3g言語モデルAと言いまして、直近の2単語から次の単語を予測するという仕組みになります。

[0:17:45 - 0:18:23]
ですので、先ほど使っていた日本の首都は、次に東京が来る確率を求める場合はこの3g、言語モデルでは直近の2単語、つまり、首都はから次東京という単語が来る確率を予測しようというような手法です。このように需要予測に使うこの単語の範囲、単語数の範囲を狭めることで、 る程度先ほどの処方の問題で ったこのデータSparseDense問題を る程度は回避できるようになりました。

[0:18:23 - 0:19:14]
ただ、これでも解決できない大きな問題があります。それは、長距離間の単語間の関係性を把握しづらいということです。つまり、直近の2単語からだけを見て、その次に「東京」というのを推定するのは難しいのです。面も多々ありますので、そういったことが難しいと、この場合は、もう数単語前の「日本」という単語がないと、次に「東京」を予測しづらいです。このような範囲を狭めたことでデータのSparseの問題を回避できることはできたけれども、その裏側として、もっと前単語のことを無視してしまうという問題があります。ですので、これらの問題を、後ほど紹介します。このTransformerで解決しようというのがこれまでの流れでした。

[0:19:19 - 0:19:51]
ここからディープラーニングなどが発達してきてニューラル言語モデルというのが提案されるようになってきました。これは先ほどから求めようとしているこの条件付き確率を、これ何らかのニューラルネットを用いて推定したモデルとなりますこれは他の機械学習と同様にこの誘導を最大化するように訓練させますつまり、誤差逆転版を用いてのニューラルネットワークを訓練させます。

[0:19:51 - 0:20:19]
なので、この下の図のように下から日本の首都はこの4単語ニューラルネットに入力したときに、次に来る単語がこの東京てくる確率を他の単語よりも高く予測する京都って予測する確率よりも東京というか、予測する確率をより高くなるようにニューラルネットワークを訓練しようというのがこのニューラル言語モデルのアイディアです。

[0:20:21 - 0:20:54]
そうすると次の問題は、ではどのようなネットワーク構造が最適なのかという問題になってきます。ニューラル言語モデルってのはですね歴史的背景的に機械翻訳の分野で大きく発展してきましたのでここからは機械翻訳のタスクを、に見ていきます具体的には吾輩は猫で るこの4単語を入力したときにこれを英語に翻訳するCatに翻訳したいというときの例を考 てみます。

[0:20:56 - 0:21:19]
この2一旦骨髄愛amキャット、このように出力したいという、この詩を達成するためにどのようなニューラルネットワークが適切なのかというのを見ていこうと思います。ここで徒歩二つ用語の定義を紹介しようと思います。

[0:21:19 - 0:21:37]
一つ目はEncoderと呼ばれ、これは入力テキストを入力として受け付ける左側の部分を指します。もう一つはDecoderと呼ばれ、これは文章を出力する部分です。

[0:21:37 - 0:22:04]
場所です。ただし、これただ出力するだけでなく、自分が出力した単語をさらにもう1回入力に持ち込み、そして2次の単語を予測するという最適な入力を受け付ける機構も持っています。なんでこのEncoderとDecoderという言葉、後ほど出てきますので、この二つ覚えておいてください。

[0:22:09 - 0:22:42]
ニューラル言語モデルの中で、まず最初にRNN型（リカレントニューラルネットワーク）を使った言語モデルが達成されました。この代表的なモデルにはSeektoSeekといったモデルがあります。これは、冒頭の単語から1単語ずつユニットネットワークに入力して、このニューロンを逐一更新すると、ただしそのときパラメータは使いまわしで行っていくという方法です。このネットワークを用いることで、原理的には単語をいくつでも入力して、かつ出力することができます。

[0:22:42 - 0:23:02]
実際に吾輩は猫で るのを用いて例にとって見て、この流れを見ていきましょう。まず最初に、1単語目の吾輩が、入力されます理論が構築されます。同じパラメータを使って今度は2単語目を入力します。入力が更新されます。

[0:23:02 - 0:23:19]
この猫入力します日本こうしてますというのをごめんなさいボタンを入れて湯いうのが構築されますと次Decoderに移っていきます。Encoder値の潜在表現をDecoderに渡して、Decoderがファイルを出力します。

[0:23:19 - 0:23:53]
そしたら、Decoderが出力した1単語目のものを、これをさらにもう1回入力の2の最後にくっつけて入力すると、次の2単語目のアームが取得されて、そのまま流れでどんどん次々と単語が出力されます。Catは、このようにRNN型の言語モデルで翻訳ができるということが確認できました。

[0:23:57 - 0:24:19]
ただし、このRNN型でも二つ課題が残っていまして、一つ目はこのニューロンが低調ですので長文になればなるほど、全ての情報を覚 きれない。結局このRNN型でも、先ほども った、先ほどのような手法で全くの単語間の長距離依存性の把握が困難だという問題が りました。

[0:24:19 - 0:24:46]
これは、言い換 ますと、ちょうど入ってくると最初の直近の単語覚 てるけど最初の方の単語が何だったっけな。 んまり覚 ていられないという問題が りました。二つ目の課題は、これ、長文入れれば入れるほどネットワークは横ほぼ単語方向にどんどんどんどん伸びてって深くなっていくため、この学習が不安定かつ学習が遅いということになります。

[0:24:47 - 0:25:03]
これは、誤差逆伝播でアルバックプロパーレーションをするときに、一番後ろの出力層から一番最初に6層まで戻っていくのは非常に何ステップも って大変で りその間に学習が不安定かつ各種遅くなるという問題が りました。

[0:25:07 - 0:25:40]
これらのような二つの問題をうまく解決したモデル構造として提案されたのがこのTrasnformerというモデルことだったんですよこれは詳しいモデル講座、後ほど紹介しますが、まず結論から先に紹介しますと、このTrasnformerの中に るテンション事故という仕組みを最大限活用することで、先ほど述べたRNN型の問題を解決できました。

[0:25:41 - 0:26:03]
つまりこのアテンション機構というのを最大限引きそうすることで、単語間の長距離依存性を把握できるようになり、かつバックプロポレーション逆を誤差逆伝播のこのステップ数が単語数に依存しなくなるつまり短くなってそのおかげで、学習の安定化と高速化これ両方を達成することができました。

[0:26:08 - 0:26:48]
ですので受賞分になっても14ヶ所を超えているという、かつ学習も早いということです。長文に強くなったということです。ということでこれからそのようなすごいTransformerの細かい内部情報を次の章で見ていこうと思います。Transformerは、attention is all you needという非常に有名な論文で初めて提案されたモデルとなります。この論文は2024年現在、10万件以上の引用を超える非常に有名なものとなっています。

[0:26:48 - 0:27:08]
これは最初に1017年にGoogleを中心にした研究チームが発表したものでして、アテンション機構というのを最大限活用することで、この単語さんへ言い換 ますと空間の直立補正の関係性を効率的に学習することができるようなったってのが大きな特徴です。

[0:27:08 - 0:27:50]
かつ、この学習時の並列計算も効率できたことで、大規模化、もちろん大規模化つまり分散学習たくさんのコンピュータを使って一気に並列計算しやすくモデルの学習をしやすくなったというメリットが ります。細かいモデル構造を見ていく前にこのTrasnformerというモデルのすごさここで紹介しようと思うんですが、この2017年に発表されて以降、モデルの改良やスケール化つまり、より大規模化させることによって、数多くのベンチマークで、当時の再構成の達成し続けています。

[0:27:51 - 0:28:16]
例：ここでは、GPTシリーズの1から4までをご紹介いたしますが、これらすべてTransformerモデルが採用されています。特に、その中でも、GPT-4が有名です。しかし、GPT-4は米国の医師免許試験や相試験に合格したり、日本の医師免許試験や処方試験にも合格できたりするような高い能力を持っていると報告されています。

[0:28:17 - 0:28:57]
また、つい先週、OpenAIから発表された大規模言語モデル（LLM）は、数学などの論理的思考が必要なベンチマークにおいて非常に高いパフォーマンスを示しています。例えば、米国数学オリンピックの予選で高い点数を出して本戦出場資格を得るほどの高スコアを出したり、化学、物理学、生物学などにおける専門性の高いベンチマークで人間の専門知識に匹敵する回答を出したりといったハイパフォーマンスを叩き出しているという報告がされています。

[0:29:02 - 0:29:36]
そんなすごいTransformerの細かな構造、中身まで見ていきましょう。Transformerって検索するとよくこの左側の図が出てくる、見慣れた方もいらっしゃるかもしれませんが、この中身をこれから細かく見るつもりです。この図はTransformerを構成する最小単位で、ブロックと呼ばれるものとして、このブロックを縦にN数を積み重ねて作るというのがTransformerモデルです。

[0:29:37 - 0:29:59]
この中の特に左側をEncoderブロックおよび右側をDecoder6と呼びます先ほども出てきた。営業後ですね。ではまずこのTrasnformerの処理の全体の流れというのを、イメージを持つために見ていきましょう。

[0:30:01 - 0:30:20]
先ほども就労した例吾輩は猫で るこの4単語を英語に翻訳するという翻訳タスクを見ていく例に見ていこうと思います。まず、吾輩は猫で るこの4単語一騎5Trasnformer1層目に入力します。いいっすよね。

[0:30:21 - 0:30:46]
2層目に入力が渡って3染谷納…そうめんぱで伝わりますDecoder側も同じく縦側にNそう。 ります。ですのでこのEncoder側の情報をDecoder側に渡して、翻訳、つまり出力分が吐き出されます。はい。

[0:30:47 - 0:31:18]
今度さっき自分Decoderが出力したAIをさらに自分自身も1回、最終的に入力して、やっとこのように出力、つまり翻訳文が出力されていきます。このとき注意していただきたいのが、このブロックの縦に積み上がるこの層が遠くの数だけ横方向、単語の数だけ横方向に増 ていっているということに注目していただきたいです。

[0:31:19 - 0:31:39]
この横同士のブロックは後ほど紹介します。アテンション機構という機構によって、横同士のブロックが繋がります。つまり同氏にも情報の伝達が行われているということになります。これがTrasnformerの全体の処理の流れです。

[0:31:43 - 0:32:05]
実はEncoder、左側のEncoder部分じゃなくても、Decoderだけでもテキスト生成自体は可能です。これはなぜなら、出力だけでなく、Decoder側は最終的に入力としても機能し、自分が生成したテキストも入力に含まれる仕組みを持っているからです。

[0:32:06 - 0:32:56]
実際にやってみましょう。このDecoderだけにニューラルネットワークに春の入力をいくつか入力していく例を見ていこうと思います。これ春に実際入力してみて、そうすると、桜で単語が次に行くのは最もらしいとDecoderが予測した場合、今度は桜をまた入力に持ってきて、春に桜が来たら次は画が出力されて、ということは春に桜が来たら綺麗と出力されると、これって結局春に桜が入力したら、春に桜が綺麗、このように文章が生成されるDecoderOnlyだけでも文章生成ができそうだってことはわかりますね。

[0:32:57 - 0:33:33]
実際のGPTシリーズは、DecoderOnlyの形式をとっています。背景として、このTransformerモデルが当初提案された領域は機械翻訳の領域でした。そのため、機械翻訳の先行研究でよく使われていたEncoder-Decoder形式に習う形で構造が当初提案されました。そのため、Transformerも英語だけの講座を持っている形でしたが、テキスト生成という意味においてはDecoderOnlyだけでも可能だという話です。

[0:33:39 - 0:34:04]
はい。ではここからここまでがTrasnformer全体像全体の処理の流れを見ていきましたここから各パーツに分けて細かくその仕組みを見ていこうと思います。ここでは四つ大きく四つにパーツを分解して一つ一つ見ていこうと思います一つ目がエンベディング、次にマルチ伝承次にフィードフォワードと最後その他の順番で見ていこうと思います。

[0:34:04 - 0:34:19]
ただこれ図左側の図ではこれ下の方から入力入って上から出力が出ていくという流れになってますのでこの順番は上下逆転して、下からエンベディング、 るいは発電所フィードフォワードその他そういう順番になっています。

[0:34:23 - 0:35:09]
では最初に、エンベディングについて解説していきたいと思います。これは一言で言うと単語のベクトル変換を行っています。単語のベクトル変換とは何かというと、これは色をRGBの3次元ベクトルに変換しているのと非常に似ています。例えば、格子色をRGBのこの3次元ベクトルに変換することで、コンピュータでも青紫や青に近いといった色が、人間は目で見てすぐにわかりますが、コンピュータとしても数値的にわかるようになるという利点があります。これは単語にも全く同じことを行うというもので、このエンベディング層で行っています。

[0:35:14 - 0:35:40]
このテキストをどうやってTrasnformerに取り込むかとかという、いうところを担当したのがここのエンベディングをして、実際に見ていこうと思います。このテキスト文字データをそのままではニューラルネットは入力として受け付けることができないので何とかこれを数値化していきたいという流れです。

[0:35:41 - 0:36:06]
まずテキストを遠くない座と呼ばれる後ほど紹介しますプログラムを通して、トークンという最小単位に分割します。ここでは単語単位に分割されると考 ていただいて大丈夫ですので る訳ものという一つの文章が るは けぼのという3単語にわかれます。

[0:36:08 - 0:36:28]
こうやって分割された単語を今度はまた同じく遠くない座の内部で持っている辞書に沿ってこの1単語を取ると る個別に割り当てられたIDという数字に変換していきます。例 ば、春というのは戦後10番目の参考です。

[0:36:28 - 0:37:07]
和は80番目の単語ですというような形で、各単語一対一で対応されてる数字にIDに変換していきます。このIDを今度はワンCoTベクトルという特殊な形のPEFTに変換しますここは機械的な自動変換でして、単純な変換をしておりまして1050という数値TalkIDを変換するときはこのたくさん る要素の中でも、戦後10番目の要素だけが1になったベクトル、そして他の要素全部ゼロというような特殊なベクトルに変換します。

[0:37:08 - 0:37:44]
ですので80という場合は、この80番目だけ1 って他は全部ゼロというようなベクトルに変換します。最後このような形に変換したワンCoTベクトルをマルチLayerSEPという1900万を用いて、ワードエンベディングという最終的なベクトルに変換していきますこの九重のマルチLayerSEPとのパラメータここもニューラルネットですので、パラメータ学習対象となります。

[0:37:45 - 0:38:14]
こうすることで、000000で1個だけ一応当たっていたって、単純なWベクトルが、いろんな実数値が敷き詰められた数字の羅列となるようなPEFTに変換されます。これをワードエンベディングと呼びます。このワードエンベディングは、この単語の分散要件や単語埋め込みの他の呼び方とも呼ばれます。

[0:38:15 - 0:39:13]
多分単語埋め込みってこのエンベディングという言葉に、直訳したものでしてこの単語の意味この数値化してPEFTの中に埋め込んだそのようなイメージを行っています。このような変換してどういった利点が るかといいますと、これは単語というこのSparseの情報をベクトルというDenseな密な表現に変換していると捉 ることができますこうすることでまず一つ目に、数値化されたことによって、Trasnformerなどのこのニューラルモデルの入力中として扱 るようになりますさらに、内科したベクトルという形で数値化したので ればそれらの点をプロットすることでこの各単語のこの意味を可視化することはできるという。

[0:39:14 - 0:40:32]
そして、その単語同士の値の近さによって、距離の近さによって、この意味の類似度を測ることができるというメリットがあります。例えば、左側の図で、「マン」という単語と「ウーマン」という単語は似たような単語です。しかし、実際にプロットしてみると、このトゥルーマンは二つの単語が意味が近いことがわかります。数値的にコンピュータでもわかるようになるという話です。ただ、この単語同士の関係がわかるだけでなく、同じく「キング」と「クイーン」も、この同じ近いところにあります。面白いのはこのマン・ウーマンと同じような位置関係に「キング」と「クイーン」をプロットされるというような形で、この性別の関係をコンピューターも理解できるようになるというメリットがあります。この人間は耳で聞いてすぐわかるんですが、それをしっかりコンピュータでもわかってもらうようになれるという、そういうメリットがあります。真ん中の図は、この動詞の活用形ですね。リング系と過去形がいろんな単語で同じ方向の関係性にあるということです。

[0:40:32 - 0:41:04]
三つ目はこれ、国名とその国の人目もやはり同じような関係性にいるというのがわかるというそのような面白い研究も ったりしますこれはまさに一番最初の方に説明しましたこの色をRGBの3次元PEFTに変換すると非常に似ていてこうすることでこの紫ってのは赤と青の中間に って、赤紫はどちらかというと赤寄りなんだなということがコンピュータもわかるようになるという大きなメリットが ります。

[0:41:12 - 0:41:38]
ただ、エンベディングはここで終わらずに、さらにですね、このエンベディングを数値化するという作業だったんですけれども、これだけだとその単語はこの文章全体の中で何単語目に出てくるのかという、この単語の位置情報は含まれませんので、ここでその単語の位置情報を最後に追加します。

[0:41:41 - 0:42:04]
これをポジション るエンコーディングと呼びますやり方は30でしてこのワードエンベディングで出したベクトルに対して同じ次元数のベクトルに、この、これは一番目のトークンですという情報を表したベクトルを足し込む足し算をするという。

[0:42:05 - 0:42:26]
やり方です具体的にはこの補助者なるエンコーディング、どのようにしてこの一番目のトークンですという情報をベクトルの数値化するかというと、この下の方に式に りますこのサインコサイン以下ベクトルの次元数によって周期の違う。

[0:42:27 - 0:42:54]
サインとコサインを用意して、それらを足し込むという作業を行います。この数式で実際に算出した値を可視化したものが、この右側の長方形の図です。この縦側がベクトルの次元数の軸で、横側が単語の何番目かを表しています。

[0:42:54 - 0:44:04]
単語方向の軸です。ですので、一番左のこの水色のこの長方形の枠は一番目のトークに足し算するポジションのエンコーディングでして、最後、右側が最後のトークのポジションなりDecodingですこれ明るいこの赤白っぽい明るい色と黒っぽい暗い色が りますがこの明るい色が高い値で暗い色が低い値を表していましてなので、上の方はは周期の早いこの明るい暗い明るい暗い明るい暗いという主義の速い社員箱山屋を使用していまして、下の方に行くにつれてこの明るい暗い明るいライトの周期がなだらかな、戦後前半を使用してるんだけど、そういうふうな見方ですねこれをすることによって、各番目のトークンによって独自のユニークな値を作れますのでこれを足し込むことで、このワードエンベディングこの単語は、文章全体中の何単語目ですというのを表現情報を加 た状態で、そこでTrasnformerに入力していく、このような流れになっています。

[0:44:09 - 0:44:27]
はいここまでがエンベディング層の仕組みでした。まとめますと、テキストを遠くに分割し、それをワードエンベディングというPEFTに変換します次にポジション るエンコーディングという位置情報を足し合わせて、その上でTrasnformerに入力していく。

[0:44:27 - 0:45:02]
このような役割を持っているのがエンベディングです。次にここが核となるパーツだと僕は思っているんですけれども、マルチヘッドアテンションというパーツについて入っていこうと思います。このアテンション機構というのは何かといいますと一言で言うところ、全トークン間の類似を図ることによって、長距離のトークン間の依存関係を把握することです。野々下稀子と申します。

[0:45:04 - 0:45:42]
例 ば具体的な例を用いて説明しますと、春は けぼの、夏はできたら、次どんな出力単語が来るかを予測するときに、この今まで入力してきた単語の中でどれが一番、次の単語を予測するのに一番効いてくるのかなというのを判別する、自動的に一番良いものを取捨選択できるような機構がこの伝承ですアテンションとはこの注目度とかという役ができますどこの単語に注目するのかというのを計算する場所となります。

[0:45:48 - 0:46:13]
このアテンションというのは、数式で書くと、論文にはこのような数式や、その数式を可視化した右側のグラフが描かれていることがあります。しかし、この組織やこの図を見ても、ぱっとわかりにくいところもあると思います。そのため、これからこの数式をアニメーション形式で説明していきたいと思います。

[0:46:13 - 0:46:32]
ここで三つのアニメーションの中で三つのベクトルが登場人物として出てきます。これは一つ目QueryベクトルKeyベクトルValueベクトルだこれらを中心にこの組織はどのように動いているのかってのアニメーションで見てること思います。

[0:46:36 - 0:46:58]
まず先ほども使っていたこの春は けぼのという3単語3トークンを入力するところを例に見ていこうと思います。これらのトークンはそれぞれ全て先ほどエンベディングすをもう通過した後で、この単語がワードエンベディングというベクトル化された状態で るというところからスタートします。

[0:47:02 - 0:47:22]
まず、それぞれのトークンのワードエンベディングベクトルを用いてそれらを線形変換、マルチLayerperSEPってのを用いて、二つのベクトルに分けて作ります一つがKeyベクトルとValueベクトルこの二つにそれぞれ分けます。

[0:47:22 - 0:47:46]
2単語目もKeyベクトルValue PEFT 第3解もKeyベクトルValueベクトルそれぞれ作成します。まずはトークンから見ていくんですけれども、ダイイチトークンのこのワードエンベディングベクトルをもとに、さらに三つ目のベクトルでこのQueryベクトルというものを、同じく線形変換バッチLayer per SEPを通じて作成します。

[0:47:47 - 0:48:15]
このように作成したQueryベクトルを用いて、今度は各単語のKeyベクトルとの内積を取って、類似度を図ります。この第1解けのQueryPEFTを用いて、まず第1トークン自分自身のKeyベクトルと内積取って自分自身との類似度を図りますこれは先ほどの図でいうとここに相当しますね。

[0:48:15 - 0:48:32]
QueryベクトルとKeyベクトルを使って内積をとることに相当します。これを今度はダイイチ東北のQueryPEFT今度は第2党区のKeyベクトルと内積をとって、第2と第1特と第2党区の間でのルイス類似度を図ります。

[0:48:33 - 0:49:01]
同じことを今度は第1党区のQueryベストと第3党区のKeyベクトルでない席を取って同じく第1トークンと、今度は第3トークンとの類似度を図っていきます。このようにして、第1単語とその他のトークンとのそれぞれの類似動画在籍で取れた後に、これをの合計の値を1に正規化するためにSoftmaxをかけます。

[0:49:04 - 0:49:41]
これは左側の全体像というとここに相当します。こうすることで値が大きい方がこの類似度が高い。つまり、単語間の依存度が高いということがわかりますこの例で言うと、第1単語と自分自身のスコアは0.0、第1単語と第2、すいません第1トークンと第2党区の類似度が0.5で最後、第1トークンと第3トークンの間のリージョン0.5ってことなので、第1トークンはこの場合、第2トークン第3トークにこの依存度が高いということがわかります。

[0:49:42 - 0:50:10]
この類似度を用いて最後にこの類似度をそれぞれのトークのホンダ、最後に残ったValueベクトルと掛け算していきます。ですので第1トークンと自分自身のとの類似度これスカラーですのでこれを第1トークン自分自身のValueベクトルと掛け算の掛け算して、新たなベクトルを作成します。

[0:50:10 - 0:50:29]
これ同じことを第2トークンとも行います。第1解けと第2トークンとの類似度を使って、第2等分のValueベクトルと掛け算します。これを第3特に対しても同じことをやります。第1と第3トークン類似と第3部のValueベクトルを計算して、新たなベクトルを作ります。

[0:50:29 - 0:50:58]
そして、これらが出てきたオレンジのコミットメントろそ取ったものを最後、ダイイチベクトルのすみません第1トークンのアテンション機構の出力とします。これは何をやってるかと言いますと、この第1トークンが、それぞれの各トークンその他のトークンとのこの類似度を使って、加重平均をとった。

[0:50:58 - 0:51:24]
と捉えることができます。これは全体像で言えば、ここに相当しますね。この第1党区のQueryベクトルと他のKeyベクトルとの内積を取って、類似度を計算したものを最後にValueベクトルと加重平均を行っている、そのような流れになります。

[0:51:26 - 0:52:02]
こうすることで第1トークンのに対するアテンション基本出力は、その他の単語との関係性を含めた新たな表現として出力変換されるという流れになります。これと今のちょっと同じ流れを今度第2党区に対しても行います第2トップにもQueryベクトルを最後新たに作ってこれをその他のトークンとの内積取ってる時どう図ってそれを用いて、各トークンてのValueべくとの荷重平均を取る。

[0:52:02 - 0:52:21]
これが第2党区におけるパーテーション機構の出力をします。第3トークも同じですね。Query代さん、トークごとにクリプトを作成し、各トークンと内積をとり、スコアと類似度を測定して、最後に各トークンとのValue PEFTの重みを平均したものが第3トップのアテンション機構の施策となります。

[0:52:23 - 0:52:54]
この流れを今高速で復習しますとこのようなアニメーションなります。それぞれの各トークンでKeyWebとValueベクターを作って、各ベクトルのQueryベクトルをそれぞれ他のベクトルたちと掛け合いのKeyベクトルと掛け合わせてなにせ体積を取って類似度を測って、最後そうやって出した類似類似度を用いて、各ベクトルのValueベクトルとの加重平均を取る。

[0:52:54 - 0:53:24]
これはこれを、この作業をどんどん繰り返していきます。はい。これは一歩引いてみると何を行ってるかといいますと、これはですね、ワンステップで全単語と繋がることで、これトークのトークの情報を効率よく取り込むようにできたと捉 ることができます。

[0:53:26 - 0:53:58]
このアテンション機構を用いることでこの1本 る単語がその他の単語との類似度を測って加重平均を取ってってやることによってこの各トークンが必要なトークの情報だけをこの柔軟に取捨選択して重み付けをして、新たな表現に変わることができるという、これはこの時系列に沿ってこの単語の順番に沿ってどんどんどんどん入力していくトークトークンを入力していくこのRNN型では実現できなかったことですね。

[0:54:00 - 0:54:51]
こうすることで、1単語目が非常に遠くに る。10単語先のトークンともも重要だとここのトークには関わりが るかどうかってのを長期にわたって関係性を、構築することができるこのような大きな特徴が ります。これは次のトークンを予測するときに、直近のトークンだけ役に立つときにはこの投稿を見る必要はないですよねそういうときはそっちはいらないと、逆に次のトークンを予測するときに遠くのトークンの情報が必要なときに、逆に言 ば近くを見る必要ないですよねそういうような形で適宜重み付けを柔軟に設定できるというのがこのアテンション機構の良いところです。

[0:54:54 - 0:55:17]
。言い換 ますと、ワンステップでこれ全単語と繋がることで、このRNN間の課題で った1単語間の長距離依存関係を把握できるようになった。そして、にワンステップで全単語と繋がりますので、今度は逆誤差逆伝播がステップ数が減り、安定かつ高速になることができました。

[0:55:21 - 0:55:47]
つまり、このアテンション機構によって、各トークンのベクトルが他のすべてのトークンとの関係性を取り込み、より良い形で表現に変換される、つまりトランスフォームされると捉えることができます。Transformerというのは、モデル名となった由来はもしかしたらここにあるのではないかと僕は思っています。

[0:55:52 - 0:56:12]
このようにアテンションが各単語間で行われるので、このことにより、アテンションの重みを可視化することで、各単語がどの単語と多くの関係を持つのか、高い関係性があるのかを可視化することもできます。

[0:56:12 - 0:56:41]
例えば左側の例文リーダーにもdtypeのクロースター3Bコースに入ります。このような文章ができたときの、この真ん中に表示されるキッドが、結局何を指し示しているのかというのを可視化したのがこのオレンジの太線と細い線なんですけど、この太線があればあるほど、高いアテンションが張られているということを表します。しっかりフィットってのは、リニューアルに対して強くテンションがかかっていることはわかります。

[0:56:41 - 0:56:57]
このイット!ってのはこのストリートに対しては低くて正解のアニマルちゃんと強くアテンションかかっているってことはこのTrasnformerが文章ちゃんと理解できているってことが可視化してわかることができます。

[0:56:58 - 0:57:31]
これは1と1単語に対するこのアテンションを可視化したものですけれども、このアテンションKeyCoTは同じことを全ての単語に対して行っていますので、ということは、これを右図のように、アテンションマップで各単語がどれどれの単語に対してどれほどアテンションが当たっているかというヒートマップを作ることができます。そうですね、スポーツでいうとリーグ戦のようにですね、この各単語がそれぞれどのようにアテンションがかかっているかというのを2次元のマップとして見ることもできます。

[0:57:36 - 0:58:16]
はい、ここまでがアテンション機構でした。先ほどのアニメーションを見ていただいた後ですと、この数式や、この論文に掲載されているこの図が先ほどよりも少しわかりやすくなったのではないかなと思います。はい、このアテンション機構というのはですね、Encoder側とDecoder側それぞれにありますが、ちょっとEncoder側とDecoder側で違いがありますので、そこを解説しようと思います。まずEncoder側は入力テキスト内でのみのアテンションをかけるセルフアテンションと呼ばれる機構となっています。

[0:58:17 - 0:58:38]
一方で、Decoder側のアテンション機構が入力テキストだけではなくてテキスト出力側のテキストにもまたがったアテンション、これクロスアテンションというのをなっています。Decoder側では、出力テキスト内だけでの検証、政府アテンションも存在します。

[0:58:41 - 0:58:57]
ただし注意点は、この出力テキストについては自分より未来のトークン、これから出力していくところのトップに対してはアテンションされないようにマスクをかける人が、をかけますこれ講座るアテンションマスクと呼びます。

[0:58:58 - 0:59:40]
これはどういう目的かというと、このDecoderはこれから未来の単語を予測していくので、未来に対してカンニングを防ぐ目的でこれを行います。例えば、この図がその講座で紹介したテンションマスクを可視化した図なんですが、この行がQuery列がKeyベクトルの場合に、AIにとっては自分自身のトークにはテンションを掛けますが、この後出てくるはずのamaCatにはアテンションを掛けられないようになっています。

[0:59:40 - 1:00:12]
一方で、AMは、AMとその前まで自分が出力したAIまでには、アテンションを掛けられますが、その先のCatやパーテーションをかけられないという仕組みをします。これはプログラム実装上では、Softmax直前に、該当要素に非常に大きな負の値、例えばマイナス-1の値などを足すような実装をすることがあります。

[1:00:22 - 1:00:52]
このアテンション機構では、複数の同じようなアテンションの仕組みを並列で行います。その後、出てきたそれぞれのアテンションで得られた情報を統合して、一つのベクトルにまとめ、次の層に渡します。これは、一つのトークンが様々なトークンに異なる形式のアテンションを当てることが可能になるという仕組みです。

[1:00:54 - 1:01:14]
これは平たく言うと、この人の文章でも、読み方によっては複数の解釈が考えられると思います。そのようなことをここでも、アテンショントランスフォーマーを理解している中で、同じようなことを達成できる仕組みなんではないかと私は理解しています。

[1:01:18 - 1:01:40]
はい。ここまでが、マルチェッロアテンションでしたおさらいしますと、アテンション機構にて全トークンのベクトルを、各トークンのベクトルをその他の全トークンとの関係性を取り込むことで、より良い表現に変換するそのような役割を果たしてるのはここのLayerです。

[1:01:43 - 1:02:03]
次、フィードフォワード尊入っていこうと思います。フィードカードは一言で言いますこの巨大な2階建てのマルチLayerforSEPtになっています。ただし、特徴として中間層が非常に大きいという。形をとっています。

[1:02:03 - 1:02:39]
これ入力層と出力層は同じ次元数なんですけれども、その間の中間層が入力層と出力層の次元の4倍も大きいという非常に大きなLayerになっています。巨大と言いましたが、どれほど巨大かというと、実際にパラメータ数を計算してみると、全体のパラメータ数の3分の2、つまり66%を占めていることがわかります。

[1:02:39 - 1:04:08]
これGPT3を例にとって細かく計算していきますとこの出力入力総出力層ともに、1万2000次元ほど りまして、中間層とはその4倍ということなので、4万9000円次元ほど りますそんなブロック96ブロック りますので、つまりフィードフォワードのパラメータ数はどれくらいになるかといいますと、この入力層から出力層に対しても全ニューロンがお互い繋がり合っていますのでそこでのパラメータ数は入力層の次元×1万2000掛ける中間層の4万9000円でそれが今度は中間層から出力層に対しても同じようにたくさんの量が確認が全部繋がっていますので、それが2個に染まるとそんなのが96ブロックも りますこれ全部で116BillonからMetaから1,160億個のパラメータが るフィードファーストフィードフォワード層だけで るということになりますこれGPT3の総パラメータ数ってのは175Billonですので1750億個のパラメータですので、フィードフォワード層の全体のパラメーターに占めるパラメータ数は結局66%、3分の2ほど るということになります。

[1:04:11 - 1:04:27]
ということで非常に多く、全体の非常に大きな部分の割合を、ここのフィードフォワード数をパラメータを主占有しているということになりますので、パラメータ数が多い大きいだけに、非常に重要な何か重要なことをしているんだろうと推測することができます。

[1:04:28 - 1:05:07]
これ実際何をやる出るかと言いますと、後続の研究によりますと、これ最後の下の方にまとめて書いて りますが、知識を蓄 る場所と考 られているという解釈がされています。これはどういうことかといいますと第1外のパラメータを入力のパターンを把握する箇所のKeyといて、第2層のパラメータをその方が何を意味しているかを表すValueといった場合に、これはKeyValueを用いて知識を抽出するニューラルMemoryを模倣してるんだではないかと解釈できるという研究が ります。

[1:05:08 - 1:05:38]
ですのでMeta知識を行うKeyを使って抽出するそのような場所なんでないかと考 られています。はい。フィードフォワード指導これは日まとめますと、巨大な2階建てのバッチLayer羽セットKeyValueでの形式で蓄積した知識を抽出する機器として考 られています。

[1:05:40 - 1:06:15]
最後その他の部分に入っていきます&なんだろうね。ここへも重要なんですけれども、さっと紹介しようと思います。まず、これは山接続と呼ばれるところなんですが、深い層の学習をするときのテクニックです。フィールド外アテンションの前後にこの接続を行っています。二つ目に、ルームセクションというところなんですが、これは学習を効率化するテクニックでして、各社の受験数で平均と分散を取り、正規化するような処理を行っています。

[1:06:16 - 1:06:43]
最後すごくそうですがこれは線形変化を挟んだ後にSoftmaxをすることで、次の単語の正規確率ってのを出力する層になっています。はい。これが単相なモデルの全体像でした。まとめますと、おさらいしますと、テキストとエンベディング層では、テキスト特に爆発してベクトルに変換し、ポジションのエンコーディングを足し合わせてTrasnformerに入力します。

[1:06:44 - 1:07:05]
次にマルチ減ったアテンションこれは、アテンション機構にて獲得のベクトルを全部オープンとの関係性を取り込むことで、より良い表現に変換する場所です。最後巨大フィードフォワードそう、巨大な2階建ての1Layer%のKeyValue形式で蓄積した知識を抽出する機構として考 られています。

[1:07:05 - 1:07:43]
最後、その他の部分では学習をうまく行うテクニックと最後出力層のこのような、大きく分けて四つのパーツで構成されています。はい。ここまでが相場の中身についてでした。今日ここで一旦休憩を挟むと思います休憩後には事前学習の具体的な流れに入っていこうと思います。

[1:07:43 - 1:08:30]
そうですね、3分ほど休憩しましょう。それでは、10分後に再開しようと思います。ちょっとすいません、下赤根さんお疲れ様です。皆さんの休憩の合間に質疑応答の時間を設けられると良いなと考えていますので、管理者画面を共有していただければ、時間の許す限り回答できそうなところをピックアップして、質問できそうなものがあったらご回答いただけると助かります。チャットの方で内部でフィルタリングをかけたURLを貼りましたが、アクセスできそうであれば、穴を開けていただいて質問できそうなもの、ピックアップしてご回答いただけると助かります。

[1:08:30 - 1:08:59]
はい、わかりました。少々お待ちください。ちょっと時間がかかりますが、5分ほど休憩を取った方が良いと思いますので、再開は13分後にしましょう。大丈夫でしょうか？もし何か問題があればお知らせください。はい、皆さん13分後に再開という形でよろしくお願いします。手が空いている方は、休憩がてら質疑応答にも耳を傾けていただければ幸いです。よろしくお願いします。

[1:09:07 - 1:10:06]
管理する管理士さん側で画面共有とかってできそうですかします少々、はい、はい。 そうです。ペンション機構の関する質問が多いです。はい、そうですねこのテンションテンション機構におけるKeyとValueとは何を示すかのイメージがわきませんでした。

[1:10:12 - 1:10:54]
これはそうですね、僕の理解ではまずValueベクトルの方から説明しようと思います。このValueベクトルは、各単語の本質的な意味を数値化しているものだと私は理解しています。一方、Keyベクトルは、その単語が他の単語とどのような関係性を持っているかという文脈に相当するものです。

[1:10:55 - 1:12:06]
情報格納しているのではと私は理解しています。ですので、各単語例えば、ダイイチトークンが、その他のトークンとどのような関係性を持っているかというのを見るために、ダイイチトークのQueryベクトルとその他のトークンとのKeyベクトルの内積を取っていくという話だったんですけれども、このValueベクトルの方はその下、各トークンのそれぞれの単体の意味だけを持っているので、そっちと内積を取るよりかは、このその他の単語、この単語に対してどんな関係を持ってるかという、そっちの情報を持っているKeyベクトル法と、内積を取っていくとそのときには、そのときには今ダイイチトークのKeyベクトルではなくて、このQueryベクトルってのを内積を使っていなかった席とってましたよね。それは今度はですねこのダイイチトークンが絡みた。

[1:12:07 - 1:12:50]
その他のトークンとの関係性はどういうものなのかという情報を持っているのがQueryベクトルではないかと私は理解します。それを使って、その他の単語との関係性をこのダイイチトークンと探し当てていく作業をしているのがQuery PEFTを用いているんだと思います。その他の単語とのKeyベクトルと内積を取る作業に処理されているんではないかなと思います。そうすることで、この第1党県ってのは代理とかこういう関係性なんだ、関係ないんだとか、第1相くんと大山東くんと非常に関係が強いんだということがどんどんわかっていく。

[1:12:51 - 1:13:20]
わかったならば今度はそのダイイチ総研とその他特にこういう関係性なんで れば、そのトークン自体が持っている本質的な意味Valueベクトル法をその類似に従って加重平均した出して出そうというような仕組みなんではないかなと私は理解してます。

[1:13:21 - 1:13:46]
はい。ということですいません、もう実際、7時13分にRZ83ページになりましたので、回答講義の続きをしていると思います。はい。ちょっと時間が過ぎてしまいましたので、ここからは、少しお待ちください。秋吉君が登場すると思います。

[1:13:48 - 1:14:05]
はい川崎さんも休憩終了で再開で大丈夫ですか大丈夫ですよろしくお願いします。はい、 りがとうございます。では事前学習のここまではTrasnformer内部の仕組みも細かく見てきましたので実際にそれをどうやって学習していくかという事前学習の流れについて解説していこうと思います。

[1:14:08 - 1:14:40]
はい、まず事前学習に入る前に、このLLM全体の学習フローは大きく三つのステップに分けることができます。第1ステップはこれから見ていくこの事前学習です。ただし、これが終わった後にも二つのステップ、すなわちファインチューニングとRLHFのステップがありますが、これらは後日講義で詳しく説明しますので、そちらの方で詳しい解説を譲りたいと思います。まずこの会では第1ステップである事前学習についてお話していきます。

[1:14:43 - 1:15:48]
事前学習の具体的な回数に入る前にまず、LLM以前はどのようなか言語モデルの学習をしていたかってのを見ていると思います時実はそのLLM以前は まり事前学習というのは りませんでして各モデルがそれを北井佑そのものに、の中学習だけ行っていましたので、翻訳モデルとして使う作られたモデル翻訳だけの学習を行いようやくモデルはようやくどっかまでの読解の学習だけそのように書くも各タスクそれぞれわかれた別個の学習だけをしていましたが、LLMが誕生してからは、まず一旦大規模コーパス大量のテキストデータセットを使って事前学習を行って行って汎用的な能力を確保獲得してから、各細かいタスクに分岐してそれぞれ翻訳の学習、追加で要約の修正に追加でどっかの各種のように行う。

[1:15:48 - 1:16:26]
このように、学習のフェーズを大きく2段階に分けるようになりましたこの二、三回目のFinetuningとかRLHFだとかを事後学習と呼ぶようになりました。第1フェーズのこの事前学習の目的はどのような目的かと言いますとこれは後続タスクに共通して必要な汎用的な知識、言い換 ますと読み書きそろばん、いろんなこの基礎知識を学習させやって学習させた基礎知識を後続のタスクに転移するというような目的が るのではないかと解釈されています。

[1:16:26 - 1:16:51]
関連する研究分野では、このトランスファーラーニングやアダプテーションの分野が重要です。高速タスクのための良いパラメータを効率的に得られるとも解釈できます。高速タスクとは、最終的に起きたタスクを指し、さっきのスライドで言及された要約翻訳や読解などのことを指します。

[1:16:54 - 1:17:32]
以下は修正されたテキストです。

これ下にファンデーションモデルに関する論文のところを引用しているんですけど、ここでもこの事前学習の目的が書かれていますので、興味がある方はこちらの論文を参照してください。はい、事前学習のパイプラインも細かく4つに分解することができます。一つ目はデータの収集、次にデータの前処理で滑らかにしたデータを訓練し、最後に評価を行う、という4ステップになっています。

[1:17:32 - 1:17:47]
これはこっからは概要だけ御説明しますが、この本資料の最後の方に ります発展的話題の方のセクションでも詳細の解説をしていますので興味の る方はそちらをご参照ください。では一つ目のデータの収集から見ていこうと思います。

[1:17:50 - 1:18:18]
事前学習用のデータを一般的にこのWebから大規模にクロールして集めてきたデータを用いることは り、多いです。夢よく るのは一般的なWebサイトニュースやブログ、ホームページなどから集めてきたデータやプログラム言語で、で るGitHub百科事典Wikipedia、 とは小説などを含めたボックス、 と論文のアーカイブと最後技術的な話題のQ&A知恵袋みたいなサイトで ります。

[1:18:19 - 1:19:23]
スタッフExchangeといったところから集めてきたデータを用いて学習させることです。これは、どれくらいの量を集めてくるかという問題なんですが、これ、GPT-3の亡霊にしてみますと、約5000億トークンのテキストを利用しているとされています。これは、書籍に換算すると約500万冊に相当すると言われています。参考までに、東大図書館の約130万冊、国会図書館の約470万冊がありますので、これ、東大図書館よりもほぼイーブンな量を学習しているということになります。リンク情報によると、今度はさらに多い1.3億冊相当、13兆トークンを使用して学習されたといわれています。これもはや国会図書館よりも多い量を準備していると言い出すことができるでしょう。

[1:19:24 - 1:20:03]
と言います。ただ最近では、モデルサイズは小さめのモデルサイズですが、これよりも多くのトークンを収集させてパフォーマンスの改善を試みる事例もよく見られます。例えば、Llama2は700億で、1750億のトークンを学習させ、GPT3よりもモデルサイズは小さいにもかかわらず、この日商と逆4倍のトークン数を学習させることでパフォーマンスの改善を試みる事例もあります。

[1:20:09 - 1:21:07]
そのように大量に集めてきたデータセットなんですが、それをそのまま使うわけではありません。そのデータは品質がばらつきがあるため、ちゃんと前処理を行って、綺麗なデータセットにするという作業をここで行います。データセットによって毎週の仕組みもそれぞれ少しずつ異なりますが、ここでは代表的な前処理のステップを紹介します。まず最初に、クオリティフィルタリングを行います。これは、より品質の低いデータを判別して取り除く作業です。次に、Deep Applicationと略されることも多いんですけれども、これは文章に重複があると学習への悪影響が多いため、文章+様々な粒度で重複している部分を排除する処理を行います。

[1:21:08 - 1:21:29]
次にこれも14なんですけども、プライバシーに関わる個人を特定できる個人情報などは取り除くとのことを行います。最後に次のページで説明しますけれども、遠くないうちにワンスレッド1というのを行います。これは文章最小単位、単語単位のような最小単位で分割する作業になります。

[1:21:33 - 1:21:50]
この遠くない税所得税ション先ほどからよく出ている単語言葉なんですけどこれ何を行ってるか。というのをここで説明しようと思います。これはテキスト、一つの文章長い文章をこのトークンと呼ばれる最小単位に分割する。

[1:21:51 - 1:22:15]
という処理を行う場所ですトークンとはイメージでは1解け1単語のような形で単語単位のような形で細かく分割していくという作業ですそんなトークン化を行うためのプログラムを遠くないTheと呼んでいまして、例 ば有名なのはバイトペアDecodingやSentencePIECEなのか有名なものがいくつか ります。

[1:22:15 - 1:22:49]
これは一般的に5位、その文章の中で出てくるこの合意の日、出現頻度と関係したアルゴリズムで、このどこで区切るかってのを決めるというようなアルゴリズムになっていることが多いです。ですのでこのコーパスこの文章をテキストデータセットの中から遠くない座が決めたアルゴリズムに従ってここで区切るというような語彙の語彙一覧の辞書を作成し、その辞書に従って、文章を分割していくというような作業を行います。

[1:22:53 - 1:23:09]
そのようにきれいにされたデータで、かつトークン単位で分割されたデータセットをどのように学習するかなんですけれども、これはネクストトークン予測という自己教師あり学習の一種を用いて、学習を行います。

[1:23:10 - 1:23:39]
これは具体的にはどういう学習方法かというと、この文章を入力して、その文章の次にくるトークンが何かという確率をひたすら予測するというものです。例えば、「吾輩は猫である」という文章を学習データセットとして使った場合、どのようなことが起きるかというと、まず一旦「吾輩は猫である」を入れた後に、次に来るトークンの確率を予測します。

[1:23:40 - 1:24:03]
正解の単語はなんですが、実際にその我輩を入れた後に、はが出てくると予測する確率を高めるようなふうにこのモデルを訓練するということです。次も同様で吾輩が来たら次猫が正解ですので、ちゃんと猫が出てくる確率が高くなるようにモデルを訓練します。

[1:24:03 - 1:24:34]
吾輩は猫って切ったら次は次は るんまるっというのを予測していくと実際にモデルが予測した。確率が入ってきたら次側が来るだろうという即した確率を、実際の世界が終わった場合のこの予測と正解との誤差、交差エントロピーという講座を小さくなるように、世界との差に企画成果により近づくようにLLMを学習させていくというそのような学習方法です。

[1:24:35 - 1:25:13]
学校することで、このテキストデータを特にラベル付けや人間によるアノテーションをしなくても、ある程度そのままデータセットとして使用できるというメリットがあります。XとCoTくんプロジェクションは、次のトークンの生成確率を予測する方法ですが、その予測と実際の生成の誤差、具体的には交差エントロピーを小さくするように学習します。スーパー交差エントロピーの数式で表すことができます。

[1:25:16 - 1:25:38]
このネクストトークプロジェクションLLMの学習において特徴的なのは、他の機械学習のディープラーニングと比較して、非常に少ないエポック数で学習を行うことが多い点です。多くの場合、1から3サイクルという非常に限られた範囲で学習を行います。

[1:25:38 - 1:26:19]
Falconと呼ばれる、UAEが発表した開発したLLMでは、全てのデータセットで1億しか学習していません。Llama2でも、Meta社が開発したLlama2というモデルでも、右腕パックの1本のみをトレーニングさせています。LlamaOneの方では、各データセットによって少し違いますが、多くても1、2本ぐらいしか学習させていません。やはり、少ないボックスだけ学習させることが一般的です。

[1:26:21 - 1:26:59]
これは何でかといいますと、考 られる理由として、この複数エポックたくさん学習しすぎると、各学習によってこの性能が落ちるデグラデーションするか、もしくはあまり下がらない、こっちなくてもあまり差がないじゃない、やる意味があまりないということがわかっていますのでこれが大きな理由なのではないかなと考 ます。さらに悪いことに、このモデルサイズを大きくすればするほど性能が高まるとわかっていますが、大きくすればするほど、複数PoCしたときのこのパフォーマンスが落ちる傾向が強くなるという、悪影響が大きいということがもうわかっています。

[1:27:00 - 1:28:21]
そういった理由で、少ないポップスだけを行うということが多いです。このちょうどう聞くと、単に次の単語を予測して後で答え合わせをしていくだけと聞こえるんですけれども、実際やってみると非常に難しくて、この他にどのように難しいかというと、小さなモデルの訓練では発生しないような問題エラーが大きな大規模なモデル学習すると途端によく発生するようになるという非常に厄介な性質があります。例えばどういうのが難しいのかといいますと、具体的にはこの交差エントロピーのロス、この値は下がれば下がれば、下がれば下がるほどいいという値なんですけど、学習途中で順調に下がってると思いきゃいけない部分が、途中で上がって、その後一向に下がらないような事象が起きたり、他にも、レイヤーとハードウェアのGPU周り、ネットワーク周りでこんなに大きな赤を着るというようなこともよくあります。こういう難しさがあるため、事前学習を行う際には、1台のPCとかインフラ周りに詳しい方と一緒に協力して行うことが多いです。

[1:28:23 - 1:28:51]
この数のスパイプロサーファーが跳ね上がるという問題を軽減するためにいろいろな方法がありますが、その一つとしてハイパーパラメータを設定する方法があります。これも多くの設定がありますが、ごく一部を紹介したいと思います。まず一つ目は、例えばアダムやアダムWのようなMOMENTUMベースのオプティマイザーを使うことが多くなっています。

[1:28:51 - 1:29:18]
とは、ランニングレートは一定の同じ値をずっと使い続けるわけではなく、スケジューラーによって進むにつれて、この後、ランニングレートの値を少しずつ調整していくことが行われます。具体的には、この右の図のように、最初は少しずつ0からワーキングアップさせて、最後は少しずつ減らしていくような調整が行われたりします。

[1:29:19 - 1:29:40]
と、浮動小数点制度のFP32とか、FP16とかといったものが一般的ですが、FP16だとロススパイクがよく起きるという各種不安定になることや、その上DF16が安定的に学習しやすいことが報告されることが多いです。

[1:29:41 - 1:30:07]
最後に論文を読んでいると、このバッチサイズの単位が少しわかりにくいときがあると思います。しかし、この単位のバッチサイズはいろいろ調整できたりして、ロススパイクの低減につながったりします。この論文では、このバッチサイズというのはサンプル数ではなく、トークン数の単位で表現されていることが多いです。

[1:30:07 - 1:30:41]
例 ば4Billonって書かれていますが、400万サンプルではなくて、この400万トークンを使用したということになります。これを右図のような形でサンプルすると、そのサンプラスの中でのサンプラーの最大トークン超過決断したこの長方形の面積を、が400ミリオンでそれをバッチサイズとして論文に記載しているということが多いですねこのこの長方形のこの縦の長さが400ミリを るというわけではないということを注意していただければなと思います。

[1:30:43 - 1:31:12]
最後に評価のところなんですが、まず定量評価では最初に見るべきはバリエーションロスですね。ロスがちゃんと下がっているかをモニタリングしていきます。他の機械学習では、データセットを三つに分けてトレーニングをすることが多いですが、LLMではテストロスのログ上ではあまり見かけないのが特徴的ですね。

[1:31:12 - 1:31:43]
これはおそらく、全学者がJ-POPぐらいしか学習しないでオーバーフィットしないという前提からくるのではないかなと思われます。たまにたまに、場合によってはトレーニングロスだけで完結することもあります。例えば、左のLlamaではトレーニングの数だけ見ていますし、こちらはGPTのスキル不足を確認したときの論文なんですが、そこではバリエーションの数だけを見てみます。

[1:31:47 - 1:32:07]
補足ですが、この交差エントロピーを主なロスの指標として使用します。これは、P損失やC損失など、他の呼び方もあります。さらに、クロス交差エントロピーだけでなく、周期変形した実質交差エントロピーという、同じ指標ですが名前の異なる指標も使われています。

[1:32:07 - 1:32:30]
例えば、Perplexity PMと略されるものや、Bits-per-Character、PPC、Bits-per-word PPWといった他の指標もあります。例えば、右上の図で紹介されているGPTC4のテクニカルレポートではBits-per-wordの仕様が使われており、Llama2ではPerplexity PPIが使われています。

[1:32:30 - 1:33:09]
この辺りの試験系のこの下の方の絵に書いてあるリンク先に詳しい解説がありますので、もし興味のある方はそちらのサイトを参考にしていただくと良いと思います。はい。事前学習は、後続の高須君特化したFinetuningなどまだ行っていない状況ですが、一応事前学習済みのモデルに対しても、最終的に飛び立つ圧縮での評価をすることが可能です。具体的なコンテキストでは、LoRAやZero-Shot、Few-Shotなどの手法を使って評価することができます。

[1:33:10 - 1:33:28]
これもちろんまだFinetuningなどの事前事後学習を行ってない状態ですのでそれをさらに行 ば、さらに下流タスクの性能さらに上がるってのはわかっています。ここまでは定量評価での定性評価を行うことが ります。

[1:33:28 - 1:34:03]
これは学習を終えた大規模言語モデル（LLM）を使って、実際にテキストを入力して、その出力を確認する方法ですね。どんな文書が出力されるかをしっかりと見てみるという方法です。サンプルレベルでの評価を行います。この評価の仕方をコードと呼びます。このDecoderには実は面白いことに、いろんな方式が存在しています。例えば、グリーデコーディングビームサーチやランダムサンプリングなどの方法があります。それぞれに特徴があり、これらも重要です。ここからそのDecoderの方法について少し紹介させていただければと思います。

[1:34:06 - 1:35:04]
一つ目のグリーンDecodingのDecoder方式なんですけれども、これは各単語においてそのときそのときで最も高い正規確率のやつを選んでいくという方法ですね。このグリーンって日本語で言うと同玉という意味ですよね。一番高いのをどんどん選び続けていくという方法ですね。例えば、この枝を野津5例にとってみると、まずTheでは最初の1単語目だったとしたら、次にドッグ、り、なさん3択になると独が0.4、ライスが0.5、Carが0.1の場合には、ドッグではなく最も高い確率の内装を選ばれると永安が選ばれた後にさらにその先に山とかというBloomハウスが言ってるんですけども、その中でも最も確率の高いBloomを選んでって、ザライスが有無ってこんな感じで選んでいくのがアプリDecodingです。

[1:35:08 - 1:36:49]
これグリーデコーディングでは、若干、その一番高い確率を選んできまして、この決定的な出力になるんですけれども、それだけだと必ずしもいい文章ができないという問題もあります。そこで、ビームサーチという新しい手法、別のデコーディング手法があります。これは、ブリーデコーディングといって、先だけしか見ないんですけど、これはそうじゃなくて、もうちょっと先まで見て、二転三転先まで見て、トータルで見て、この確率が高くなるような組み合わせを探していくという方法ですね。例えば、先ほどの例で言うと、フリーデコーディングでは1万と1000冊しか見ないでやっていますけれども、これだと0.5掛ける0.40だな、通愛翔0.2、トータル0.2の確率なんですが、もうちょっと先まで見てみると、ザロックの方が0.4で、外が0.9なので、0.4掛ける0.9で0.36、さっきのTheniceウーマンの0.2のトータル確率よりかも、ざっと外の方がトータルで確率が高い。こっちの方がきっとより良い文章を作れるだろうはずだという考え方で、やっぱりこっちを選ぶという方法がビームサーチとなります。右側はこの日のMサイズ3のとき、つまり3手先まで見て考えるときの例なんですけども、ちょっと時間の都合上、すいません割愛させていただきます。こちらにリンクがありますので、興味のある方はそちらのリンクをたどって見ていただければなと思います。

[1:36:51 - 1:37:19]
最後ランダムサンプリングという決定的ではない。方法のDecodingの方等も ります。これは次のトップの正規確率の分布に従ってランダムに選択していくという形ですこのやり方には重要なパラメータが三つ りまして、一つ目はトップPトップ系最後Tempalayちゃんと三つ ります。

[1:37:19 - 1:37:44]
これはそれぞれです。最後に紹介していきます。Top-P（トップピー）は、すべての単語の中から確率分布を持ち、そのうちの上位1%のトークンから選択するようにします。これにより、確実に低い確率の選択肢はそもそも選択肢に入らないと、確率の高い中からランダムで選ぶことで、完全ランダムではなく、多くの中から選ぶという方法ですね。

[1:37:45 - 1:38:05]
トップ系も似たような話なんですけれども、これは確率ではなく、ランキング確率でランキングを取ったときの上位10位までの受候補の中から選ぶという話ですね。

[1:38:06 - 1:38:42]
最後に、Temperatureだけは少し性質の異なるパラメーターです。これはLlamaDAMの調整パラメータです。具体的には、数式で書くと左側のように、このSoftmaxの分母に掛ける数字を調整するという話です。1だと通常のSoftmaxと同じです。1よりも少なくするとどうなるかというと、この右図のように、真ん中がTA1のときに、通常のSoftmaxと同じものが通常の場合で1より少ない場合です。

[1:38:43 - 1:39:17]
値を大きくすると、最も高い値を持つものの方がより選ばれやすくなり、他のものにはほとんど選ばれなくなる傾向が生まれます。逆に、1より大きな値、例えば2.0を設定すると、他の確率分布がよりなだらかになり、分布に近づきます。この場合、どの選択肢が選ばれるかは不明瞭になり、ランダム性が高まるという性質を持っています。

[1:39:18 - 1:39:45]
このようなパラメータを駆使していろいろないろんな広報の文書を作成するといったことができます。結局この中でどうやって講座方式を使 ばいいのかというとその答 は著状況次第で りまして、例 ば分類問題を解く場合はこの決定的な回答を好まれますので、グリーDecodingというのが好まれて、ビームサーチってのは機械翻訳のタスクを解くときに見ることが多いです。

[1:39:45 - 1:40:13]
これはおそらくこの自然な文章を作るためには何何てかさて3.4手先まで見て文章を作る方がいいだろうというのが理由だと思われます。最後長文生成をするときはランダムサンプリング例 ば、クリエイティブな小説を作るってときには、なんかずっと同じような回答してるとつまらないのでちょっとランダム性を使って振ってみるというのが目的にランダムサンプリングが使われることは多いということです。

[1:40:13 - 1:40:31]
はい。はい。ここまで事前学習の話でしたが、これから先発展的な話題がたくさんあります。しかし、内容が多くて時間が足りないようですね。こちらも面白いのですが、今回はすいません、割愛させていただきます。

[1:40:35 - 1:40:53]
はいということで、本日おまとめに入ろうと思います今日は、大規模言語モデルLLMの事前学習について紹介しました主に四つ業務を使っていただきたいことが りまして、一つ目が、言語モデルにおけるTrasnformerの位置づけについて説明しました。

[1:40:54 - 1:41:16]
Transformerはニューラル言語モデルの一つですが、それ以前に使われていたRNN型の言語モデルが抱えていた課題をうまく解決しました。そのようなLLMで主流となっているTransformerモデルの構造について、次に説明します。

[1:41:17 - 1:41:35]
まず、アテンション機構への活用してモデル構造でして、これはのおかげで、ワンステップで、前段互換との情報をが接続できるようになりました。こうすることで、RNN方が抱 ていた二つの課題を解決することができました。

[1:41:35 - 1:42:01]
具体的には、一つ目単語間の長距離依存性が把握できるようになった二つ目、誤差逆伝播の計算ステップアップ町に依存しなくなり、つまりステップ数が短くなり、学習が安定化かつ高速化することができた。次、三つ目にそんなTrasnformerモデルどのように学習するかということで、LLMの事前学習について説明しました。

[1:42:02 - 1:42:21]
大規模コーパス大規模でテキストデータセットによる学習を行うことで、トレイルの汎用性を高めていると捉 ることができます。具体的には、ネクスト、東プレで行く所という自己教師 り学習により学習させているということを説明しました。

[1:42:21 - 1:42:41]
最後に発展的課題については時間の都合上割愛しましたがこちらにデータモデル、各種評価分析について大きく四つのセクションに分けてそれぞれ細かいいろいろな面白いトピックを網羅していますので、時間興味の る方はを見ていただければなと思います。

[1:42:41 - 1:43:07]
では、お疲れ様でした以上で講義を長くなりましたがここで終了させていただきたいと思います。 りがとうございました。幹事さん りがとうございましたちょっと時間の都合上発伝的話題のところ触れられなかったんですがちょっとコミュニティのイベントでもらってぜひまた改めてお話聞かせていただけると嬉しいなと思ってます思ってますので、ちょっとまた別途相談させてください。

[1:43:07 - 1:43:25]
りがとうございましたお疲れ様です。はい、 りがとうございました。はい、では続きまして、ちょっと時間ないのですみませんこのまま生かしてください演習の方に移っていきたいと思うので原田さんお願いできますでしょうか？はい。

[1:43:26 - 1:43:48]
お願いします。ごめんなさいちょっと時間が れなんですがよろしくお願いします。さすがに15分で説明はできないので、延長させていただくと思います。申し訳ないですがよろしくお願いしますすいませんちょっと皆さん次の予定が る方ももしかしたらいらっしゃるかと思うのでその方はちょっと途中で抜けていただいてまたアーカイブで追って の事故いただければと思います。

[1:43:48 - 1:44:18]
なので、ちょっと延長する想定で進めさせていただければと思います。よろしくお願いいたします。講義でPretrainingの話題を取り上げました。練習で学んだことを実装レベルで理解しましょうと、Transformerの言語モデルを自作したことを胸を張って言えるようになるのがこの演習の目的です。ただ、分量としてもかなり詳しく書いたので、一部省略するところもあります。

[1:44:20 - 1:44:43]
そのためにですね、ちょっとGPTの図を作ってみました。こんな感じで公道を投げれば、詳しく解説してくれるボットみたいなのも作ってみたので、そちらも使ってみてください。というところで始めていきたいと思います。今回GPUを使用する想定なので、GPUで動かすことを忘れずにお願いします。

[1:44:45 - 1:45:08]
まず皆さん実装で理解してほしいのが、その言語モデルというのはどう実装するのかそれをニューラルネットを使わずにどう実装して、生成で ったり学習をするのかその言語モデルをニューラルネットワークを使って実装するってどういうことだろうそのニューラルネットワークをTrasnformerにするということがどういうことだろうかというところをお話できればと思います。

[1:45:09 - 1:45:35]
今回のデータセット事前学習講義でも、事前学習データセットを大量にさまざまなところから集める必要があります。これのどのようなデータを含めるかによって性能も変わってきます。そのデータを集める際にウェブから集めたりもするんですが、そのときにどのようにフィルタリングするのかという話題もいろいろあります。僕が読んで面白かった論文などをこちらにまとめているので、見てみてください。

[1:45:36 - 1:45:56]
今回日本人の方がたくさん受講しているので、日本語のコーパスを使いたいと思います。LMJPという学術研究者たちが中心となって作った団体が整備したコーパスというものがあって、そのデータセットの一部、合計17億トークンぐらいのものを使用しました。

[1:45:56 - 1:46:28]
こちらを見てみると、英語のデータセットや日本語のデータセットなどがあるため、自分でPretrainingを行う際に参考になるものだと思います。フィルタリングの実装方法についても、かなり詳しく説明されており、どのようなライブラリを使用してどのようなフィルタリングを実施したかなども書かれているので、Pretrainingデータの収集方法についても非常にエンジニアリングの見せ所で面白い分野ですので、興味のある人はぜひ見てみてください。

[1:46:29 - 1:46:53]
今回は結構ミニサイズでやりますと、TRAINデータのwikiで、Wikipediaのページのトレイの一つのデータを使ってバリエーションをちょっと準備してやりました。実際に見てみるとこのような形でビッグエリアの子線、1000ページ分ぐらい持ってきて りますわ最初の100文字見てみるとこんな感じでやってました。

[1:46:53 - 1:47:08]
学習しているときに、学習とは別のデータでモデルがちゃんと学習をうまく行っているのを確認するんですが、バリエーションデータとしても同じようなWikipediaの別のページを使っていない別のページで見ていきたいと思います。

[1:47:09 - 1:48:10]
ここまでが早いんですが、学習で事前学習データを集めたものですと、実際に言語モデルを実装していきましょう。まずはニューラルネットワークを使わないでどう実装するかというところで、抗議デモN-gram言語モデルというものが紹介されました。Nが何個か、N=1だと過去の文脈を踏まえずに1単語1単語の出現確率でモデル化します。このNが増えていくと、過去のコンテキストが増えていきます。今回はWikipedia1000ページ分、どれぐらい文字数があるかというと、156万文字ぐらいです。文字の種類、ユニークな文字の種類は3807文字です。これで156万文字ぐらいでトレーニングデータができます。簡単な2gモデル1文字1文字の出現確率でモデル化します。

[1:48:11 - 1:48:30]
物にさらにどうするかというと、この全体の文字数の中で対象とする文字が何回出てきてるかで確率を定義します。なので日本の日という文字というのが、確率どれぐらいですかというのが出現回数を全体の文字数で割った値なりました。

[1:48:30 - 1:48:54]
なので日本とか日曜日の日曜の確率って言うのが湯2gモデルだと、日本のその質、確率と本の確率を掛け合わせた値で出てきます日曜って確率も同じように出てきますとここで見てみるとこの日本の確率の方が高いのでこのユニーBloomモデルでは日本という方が最もらしいだろうというふうに判断されるというのが流れでした。

[1:48:57 - 1:50:27]
このユニグモデルは、それぞれの出現確率が出てくるモデルなんですが、どの程度の確率で出現するか、トップ何個かを見てみたものがこちらになっています。最も多いものは空白文字で、これぐらいの確率で出てくるとのことでした。その分効果を高めたのがこちらの図です。そのため、ランダムにサンプリングする際には、この確率に従ってサンプリング生成していくという流れになります。講義で使ったグリーデコーディングについて見てみると、この分布の主要なものの中で最大のもの（Max）を取り出して生成しているようです。今回のユニグは特に文脈を考慮していないため、空白文字のトップを持ち出してきて、20分を生成させています。全てが選択されると、このような先生になりますという流れです。トップPサンプリングについても説明しましたが、これは合計して割合がトップPになる語彙数5位の中からサンプリングするというものです。そのため、生成ごとに異なるものが出てくることが確認できます。

[1:50:27 - 1:51:18]
場面場面に応じてどういうサンプリングを使って生成するかというところが、デザインの余地になるかなと思います。簡単に言うと、2gを説明したんですが、次に遠くないTheというのが重要です。そして、今回は1文字1トークンの宮内座を作ってみました。長々と書いてるんですが、要するにしたいのは、この文字が与えられたときに、どのトークン番号になるかというのを返します。これ一文字一文字、一文字1トークンでしたよね。この限定された言葉が3310番のトークン番号になりますよ。

[1:51:18 - 1:52:18]
5というのが、3354番のものになりますみたいな形でテキストが与えられたときに、番号を振るというのが宮内座です。都区内座で気をつけないといけないところがその設計によってですね、この1文字1トークンの場合は、作った辞書のときに、同じように出てくるんですが、学習用データになくて、辞書に登録してないものというのがどういうふうになるかというと、この1文字だとしても4トークンぐらいで出てきました。それがどうされてるかというと、UPFのエンコーディング公式によって、その番号でトークン化されて、未知語に対応するためにこのように工夫していますという話です。奥が深いんですが、こちらぜひ見てみてください。今回は1文字、大体1トークンでやりました。

[1:52:20 - 1:52:42]
それをもとに、過去のN-gramモデルを考慮して言語モデルを作ってみましょう。それで、どの2文字のペアがどの程度出てくるかというところで、トーク後258番と259番の組み合わせが約1万5000回出ています。

[1:52:43 - 1:53:26]
これが526番と改行文字、258番のトークン番号が1万回ぐらい出てくると、倍gもこのように数を上げてどのペアが出てくるかというところが計算できます。それをもとに生成したときに勝つかというのが入れたもので、勝川の次は裏が大きい7次元が多いみたいな形で、こちらのグリーデコーディングでは一つ前から、次何が一番出てくるかというところをモデル化します。この先ほどは空白文字がBERTが出てきたんですが、なんか、ちょっと日本語っぽくなってきましたという形ですね。

[1:53:29 - 1:54:01]
こちらは少し割愛しますが、N-gramを増やすほど文脈が増えていきます。考慮できる文脈が増えていきますが、Nを増やすと、パラメータ数が増え、その組み合わせ表が膨大になっていきます。値を変えていくとそのことがわかると思います。

[1:54:01 - 1:54:29]
それで、学習用データにない組み合わせがすごい低い確率で出てくることが課題でした。ただ、4つぐらいまで考慮すると、学習用データに似た言葉を話すようになるというのが、N-gramモデルです。そのため、ニューラルネットワークを使わなくても、こういった形で言語モデルを実装できることを確認しました。

[1:54:31 - 1:54:51]
本来のニューラルネットワークを使用しましょうそのときのモチベーションが、その単語同士の類似その類義語とかが考慮できないというところが りましたとしそれをどう実現しますかねというところで単語ベクトルトークンベクトルWordエンベディングという話が ったと思います。

[1:54:52 - 1:55:58]
それをどう実装していくかなんですけどのようにトークン番号を付与するところまでは今までと一緒ですと、学習するときに、536番の次は1032番、この鍵括弧の津谷勝勝の数が来て、勝野翔理という字の次篤人ネクストトークプリティクッションが、インプットがこれでターゲットがこれですという形でデータセットを作って げますとこれらのトークン番号というのがどういうふうになりますかというのがこの単語ベクトルの取得というところで、準備した後椅子×その単語ベクトルの次元で、単語ベクトルのその、何でしょう、でっかいテーブルみたいなのを準備して げますとそのテーブル番号のこの営業を持ってきますよというところで536番を持ってきますよ。

[1:55:58 - 1:57:08]
536と1032と581番の行持ってきますよってすると、このように定義して げたよ次元のベクトルがそれぞれ出てきますとそうすることで単語ベクトルを準備して げてこれをニューラルネットワークのこの短単語IDに応じトークンIDに応じてこのベクトルを入れていきますと生成する際にはトークンValue5みたいなのが出てきますと流れをまたまとめて整理するんですけど、入力鍵括弧書きますと、トークンIDが536番ですと、これに対応するトークンエンベディング単語ベクトルというのがこちらですとそれをニューラルネットに入れて、次の単語当てないといけないのは1032番が1という確率他はゼロという確率を、合うようにこの確率分布を近づけていきたいですと何も学習してないモデルだと1032番がほぼゼロ、200280077番が0.01みたいな確率分布の絵が出力されますと、それを近づけますよという流れになります。

[1:57:09 - 1:57:37]
ニューラルネットワークの構造どうなってるかというと、このベクトル化したものをここでは一層ニューラル戦型相が ってレベルの非線形関数が って最後にもういっそ って、忘却細部サイズその遠くない座で登録したトークン番号分出力確立するこす出力するための出力層が りました。

[1:57:38 - 1:58:56]
なので、エンベディングを得て、その確率のスコアのものを出力して、それがターゲットと合っているかどうかを確認するというのが流れになります。こちらがその学習のループを書いたものです。ちょっとはしょりますが、流れとしては、まず番号を得て、ロスをその次の単語が当たったか当たってないかを得て、モデルの重みを更新していきます。ここで注意してほしいのは、このニューラルネットワーク全体を学習するという点です。このワードエンベディングは最初はランダムに初期化されたトークンベクトルですが、語彙数分それが存在します。しかし、それらもランダムに初期化されています。次に単語予測を繰り返すことで、単語ベクトルがどのように学習されるべきかという点も学習が進んでいます。そのため、トークンとトークンの表現は、同じような主語を表す単語であるという形で、学習によって単語ベクトルも獲得されていきます。これがend-to-endで学習されていくというのが流れになります。

[1:58:58 - 1:59:39]
実際に学習させてみるとトレーニング直すとバリエーションますが下がっていって、同じようにサンプリングしてみると、何かしら出てきますという形ですね。このロスというのが りますとここのロスどれぐらいまで下げられるといいのかなというところで、何も学習してない当てずっぽうモデルのロスというか、登録したエゴイストからランダムに4,064個 るものからランダムに出してきたとき、ロスが8.3ぐらいですと。

[1:59:39 - 2:01:32]
なので、学習すると何か下がって、最初は何も学習できていないのでほぼほぼランダムなんですけど、ロスが下がっているというのが、その当てずっぽうさが下がっているかなと思います。どこを目指すべきかというもので一つ言われてるのが、このロスが2を切ると、タスクを途端に解けるようになるみたいなところなので、モデルを大規模化したりして、ここの値を切れるようにみんな頑張っているかなという感じです。というところで演習少し準備してるのが実際にこのMLPの層を増やしたり、学習率を変更して、次の単語予測が当たるかなと見てくださいと、倍のニューラルネットワークを使わずに出した性能が5.08ぐらいというバリエーションがあるというのはわかってるので、とりあえずそこまでいけるかどうかをちょっと頑張って実装してみてください。ここまでがニューラルネットワークで言語モデルを実装するときに、まずワードエンベディングと単語ベクトルというものを入力して、次の単語の予測確率を出力するようなネットワークを組んで、それが実際に準備したターゲットと合ってるかどうかというところから学習して、end-to-endでモデルを更新します。ただ過去の文脈今回は今組んだモデルは1トークン前しか、つまり今のトークンから次のトークンを予測するというところしか見てなかったので、より過去の文脈を踏まえたいという課題があります。RNN（リカレントニューラルネットワーク）でどういう流れになるかみたいなのもちょっと書き出してみたので確認してみてください。

[2:01:34 - 2:01:57]
注目してほしいのが、このRNNの過去の文脈踏ま られるんですけど、ただその過去の状態というのが一つのこの隠れ状態のベクトルでしか表現されないんで、今までの所のままの表現が一つのベクトルでしかずっと伝わってこないので、過去の情報というのが忘れられちゃう。

[2:01:57 - 2:03:11]
そしてネットワーク方向が広くなっちゃって学習が安定しないみたいなところが、実装からもわかるかなと思います。本題のTrasnformerでモデル化しましょうと過去の文脈もうまく踏ま つつ、学習も並列化することによって高速にやりたいですよねというモチベーションからTrasnformer、特にアテンションを実装していきましょうとアテンションの式抗議デモ出ましたが、QKVアテンションというのが、Softmaxの中でQという表現行列とKの表現行列の計算した後に、 る値で割って げますとそれでSoftmaxで出てきたこのスコアをもとにValue行列かけて上げるという処理がテンションですとこれをただ単に取り出してみると、ここはランダムに初期化してますとバッチサイズ1でトークン数が四つほど、アテンションヘッドの次元数が2次元ですというところで1掛け4掛け2の表現行列みたいなのを作って げます。

[2:03:12 - 2:03:45]
それをQKVそれぞれ準備して げて、この式QとQAのトランスポーズしたものの掛け算このように定義できますとそれを る値で割って げますとその後にSoftmaxをかけてSoftmaxをかけて げて、スコアが出ますとこれがテンションスコアですね、このアテンションスコアをにValueの強烈かけて げると、アテンションの計算が行われますと形ですね。

[2:03:48 - 2:04:14]
ここで何でこれで終わる必要が るのかというの講義でも少し触れられていたんですけど、簡単な例で見てみると、スケールしない割り算をしないとき、このようになります割り算したらこうなりますと、二つ例を出してるんですが、割り算して げないとちょっとこの分布というのがちょっとPというか山が尖った。

[2:04:15 - 2:04:47]
学習の初期段階からこれが設定されていると、特定の部分にのみテンションを張ってしまう傾向があるため、より広範囲にアテンションを最初から注目し、学習を通じて適切なテンションを掛けられるようにしたいと考えています。そのため、最初の初期化段階で、そのなだらかな山を担保しておきたいと考えています。スケールというか、ここで待っておくというのが、ちょっとした話でした。

[2:04:49 - 2:05:17]
フィードフォワードですが、もう一つの方法として、Max0でここで一つの重みとの掛け算が行われ、その結果をもとに別の重み行列で足し算が行われます。これがフィードフォワードの仕組みです。MLPの実装でも確認しましたが、次元が入力として入ってきて、掛け算が行われ、このレイヤ関数がMaxオペレーションですね。

[2:05:17 - 2:06:49]
これでMaxPoolingを取り入れて、その次にもう1回重みを計算してかけてというのが、フィードフォワードネットワークで実装されることが多いです。この二つの主要なテンションとフィードフォワードネットワークでTransformerが実装されます。これがどれぐらい大規模化、大規模に積み重なっているかというところで、こちらがGPT2 Smallのパラメータ数とか出してみたんですが、まずトークンされる番号が全部で5万個ぐらいで、遠くのワードエンベディングの広さが768ぐらいで、フィードフォワードは約4倍するので、アテンションヘッド1個のTransformerの一層のアテンションで何個ヘッドを準備するかというと12個です。それを12層重ねます。計算してみると、GPT2 Smallは約124億のパラメータ数モデルサイズになります。そのうちにどれぐらいパラメーターを占めるコンポーネントでるかというのは、ここで見てみると、セルフアテンションとフィードフォワードが大体1対2ぐらいですよね。これがパラメーターをもっと大きくすればもっと割合が増していくんですが、ここに知識が入ってるんじゃないかみたいなことが言われてたりします。

[2:06:50 - 2:07:47]
はい。一旦ここまででセルフアテンションとフィードフォワードの実装をしましたと。ただ、ここだけ言われても多分わかんないです。これが系列が入ってくるときに実際どうなるのかというところをまず書き下しました。これが並列化されていない、1トークン1トークンずつ処理しているものです。これを追ってみると、確かにセルフアテンションというかTransformerはPの事情分系列の事情分で計算が必要になってるんだなというのが、こちらの実装を見てみるとわかると思います。さらにメモリが結構、今までの計算結果みたいなのを保持しておかないといけないトークンの保持しておかないといけないというのが、るのですごいメモリも増えるなというところが実感できるかなと思います。

[2:07:47 - 2:08:18]
簡単に説明するんですが、まずトークンIDを持ってきますと、トークンIDかこの系列も保存しておきますとそのトークンIDに応じてトークンベクトルを得ますと、ここで石部1のベクトルも足すことが多いですここで渡してないんですが簡単なため出してないですと入力のベクトルを単語ベクトルを得ました次にテンションかけます。

[2:08:18 - 2:08:40]
ここで、この入力からの入力の単語ベクトルにQueryに変換する重み行列をかけて げます。こうするとQueryベクトルが出ますQueryが何してるかって簡単に説明すると、何かを探してますというのを表現するようなくベクトルでした。

[2:08:40 - 2:09:09]
これちょっと気持ちがお気持ちで説明してます。例 ばトークン0ってのは自分主語ですと、目的語とか上司とか同士とか探してますみたいな感じでQueryベクトルがこの入力ベクトルから作られます。次にKeyベクトルが同じ入力からだValue系という重め行列を経て作られますとこれが作られると、このKeyベクトルって何を表してるかというと何々を持ってました。

[2:09:11 - 2:09:43]
いうところが出てきますね。 るトークンでは自分同士ですと、 るトークンで自分を主語ですと、私はこういう属性を持ってますというところがKeyベクトルで出てきます。次にまた同じ入力ベクトルからWVというのが出てきますValueベクトルを出しますこちらが何してるかというと、私ってこういう中身の詳細してますというところが、そのValueなので ったり、このKeyが対応する値が何かというところでValueが何かというところが出てきました。

[2:09:46 - 2:10:28]
それぞれQueryとKeyとValueがそれぞれのトークンとして出てきます。使用している重みは全て共通です。ただ入力が異なるため、それぞれ違うQuery、Key、Valueが入力に応じて出てきます。過去のものも保存し、それをもとにアテンションスコアを計算します。QueryとKeyの値から、注目すべきトークンがどこか、アテンションをかけるべきトークンがどこかというところでスコアが出てきます。それをもとに、Valueベクトル、どこに注目した上で表現を次に伝えましょうというところで、出力されます。

[2:10:28 - 2:11:11]
なので、ここのアテンションスコアはどういうことをしているかというと、第2回でRetrieverを決めた時とジェネレーションで類似度を探すようなところで、類似度計算のようなことをしたと思います。しかし、今回は演習でそのときのようにQueryとKeyの類似度を計算しているので、関係してそうなところ、ここで取り出しているわけですね。それがそれぞれのトークンで出てくるので、関係性が強い、つまりアテンションスコアが高いものというのが今注目すべき表現でしょう。それをもとに、その重みづけをもとに表現を更新していきますという流れです。

[2:11:13 - 2:11:52]
その出力が得た後にフィードフォワード層にその表現を入れて げることで計算しますと最後に出てくるのは、そのTrasnformer全部の処理を終 て出てくるのは、次の単語がどれか、その単語分用意した単語トークンID分どの確率が一番高いですかというところ、 たりしますとそうすると、過去の文脈が増 ても、次がいいな次のが った次の確率みたいな形でそれが計算されるようになってます。

[2:11:52 - 2:12:34]
それを同じようにさ、MLPでの実装と同じように学習によって、この出てくる各率を近づけていきますそれをend-to-endでやりますというのがTrasnformerになります。アテンションスコアアテンションの可視化というところで講義でも ったと思うんですけど、最初のが入ったときに自分しか見れないので、注目するのは自分だけなので1ですと2文字目が入ったときに、アテンションスコアというのが、合計すれば1になりますと、その地域に注目してるのが62%自分自身には37%注目しましょう。

[2:12:35 - 2:12:57]
3文字目入れたときは今までのそれぞれこれぐらい注目しましょう。最後に入れたときにはこれぐらい注目しましょうみたいなのが出てきます。なのでテンションアップ、この値をもとにヒートマップを作っていてここが高いほど赤色で ったり低いほどその色が薄かったりみたいなのを可視化したのが、アテンションマップです。

[2:12:58 - 2:13:21]
このアテンションを見ることで、ここのこことここの繋がりが強いから、何かしらこういう処理をしてるんじゃないかみたいな形で、その解釈性を見るときに使うのがテンションアップだったりしますその本当にアテンションマップで解釈性が高まるかみたいな話は、第10回中1回ぐらいで詳しく話が ると思います。

[2:13:23 - 2:13:49]
時間がちょっと押しすぎなので、最後、並列化するために、先ほどは1トークンずつやってたんですよねただこれだけだと、RNNみたいにRNNも1トークン1トークン入れてないと次の層の計算ができないので処理としては1トークン進むごとに次のゾーンを走るという感じなんですけど、その並列化RNNとこれしかできませんと。

[2:13:49 - 2:14:15]
アテンションのすごいところは、それを並列化できることで、入力データを並列化すれば、計算が一度に行われるため、層が積み重なっていても一気に計算できます。並列化行列の計算ですべてを一緒に計算できるので、それを実装した内容は、後で詳しく見てみてください。

[2:14:15 - 2:15:08]
ただ、処理の際に重要なのは、この下三角行列で未来の情報をマスクすることで、一気に計算ができるようになりますという点です。最初の1単語を入れたときに、2回目に入力したときに、最初に未来の情報を見れないようにした三角行列を導入します。この行列のところに未来の情報が見れないように、未来の情報をマスクすることで、小さい値を取って、Softmaxをかけるとこのスコアがゼロになるので、講座るアテンション未来の情報を見れないものを担保しつつ、学習のときに並列化するためにこのような工夫をしています。

[2:15:10 - 2:15:35]
はい。ていうのをπ統治っぽく書き換 たのがこちらになっていますし、処理としては今までのMLPでの実装と変わらずに、トークン番号を得ますと、トークン番号に応じてトークンのワードエンベディングをいます。入れましたワードエンベディングとそのトークンの場所を表す。

[2:15:36 - 2:17:03]
1ベクトルを入れて、それをもとにアテンションを入れて、これ一層のトランスアテンションだけのLanguageモデルを作ってみたんですが、そこから出てきた表現を元に語彙数分の確率分布を出して、今までと同じく、その確率分布が将来の次の単語と当たっているか、その分布が合っているかというのを、クロスエントロピーロストで上げて学習しています。他は最初にMLPで実装したときと同じです。これが一層のTransformerテンションなんですけど、それを深くするために、深くすると学習が安定しなかったりします。そのためにデジタルコネクションとかLayerNormalizationとかを追加することで、テンションフォーマのブロック深さ方向へ積み重ねていくことができます。それを実装したのがこちらのGPTの再現実装になります。大体今までの話を理解すると読み解けるんですが、違うところとして、そのLayerのその学習を安定させるレジスターコネクションを入れてます。レジで、るコネクションとは、ただ単純に元の入力とアテンションの表現を足すことでできます。

[2:17:03 - 2:18:02]
なので差分としてはここだけでずネットとLayerのも りますとこのLayerのルもかけて げて実装してますというのがGPT通です。いう形でなりましたと、ちょっと駆け足になって申し訳なかったんですが、これで言語モデルがどういうふうに学習するか、それをどう生成するか、先生に使うかというところを説明しましたTrasnformerテンションで学校のトークをどう踏ま ることができるかというのを実装でも確認しましたと実際にその学習を安定させるためには、そのアテンション機構を持つことで、過去のトークが見れるんですけどそれをどう深く深い方向にするかというところでれずネットとかレジでコネクションとかLayerのルームをすることで学習の安定性を保ったまま、深くすることができるので表現力高く巨大化していきましょうといういう話に繋がっていきます。

[2:18:02 - 2:18:19]
なので、実際にですね、自分でTransformerモデルの講座をいじってみて性能向上をどこまでできるのか、というところを自分で確かめてもらえればと思います。自分が作ったモデルと、他の講座で報告されているようなモデルとどれぐらい性能が変わってくるか、というところを自分で確認してもらうようにしましょう。

[2:18:20 - 2:18:48]
さらにTransformerの内部実装がどうなっているのかというところで、詳しい資料も参考として載せてあるので見てみてください。はい。ここまでで演習部分は以上になりますので、最後に質問があったら、一、二問ぐらい範囲外の演習をなさそうですかね。

[2:18:51 - 2:19:36]
ちょっと量が多くて大変だったかもしれませんが、適宜、Amazon AIのAIシステムにこの実装解説を聞きながら、実装してもらえばなと思います。僕も何回もTransformerを写経したりしてるんですが、実際にその行動を見ながら自分でも真似して書いてみるというところ、するとまた理解が深まるかなと思うので、ぜひお願いします。というところで、演習では、実際に言語モデルN-gramを使ってみて、ニューラルネットワークを買ってそれをどう表現するかで、そのニューラルネットワークをTransformerにするとはどういうことかを扱いました。

[2:19:36 - 2:19:57]
次、はい市は以上になります。はい、原田さん、上石さん りがとうございました。皆様ご受講お疲れ様でした。小栗キャンパスにログインして出血アンケートに回答してください。提出締切は本日から1週間後の水曜日17時です。

[2:19:57 - 2:20:12]
出欠アンケートの提出をもって出席といたします。また本日の宿題を公開しました。宿題のファイルのピークの先や出席宿題の提出方法は受講の手引きをご覧ください。受講の手引きへのリンクは受講のお知らせメールに記載されています。

[2:20:14 - 2:20:18]
それでは講義を終了いたします。本日はご受講いただき、 りがとうございました。


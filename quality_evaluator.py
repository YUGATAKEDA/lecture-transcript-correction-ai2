#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¬›ç¾©ä¿®æ­£å“è³ªè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import re
from typing import Dict, List, Tuple

def analyze_corrections(original_file: str, corrected_file: str) -> Dict:
    """ä¿®æ­£çµæœã®è©³ç´°åˆ†æ"""
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(original_file, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        with open(corrected_file, 'r', encoding='utf-8') as f:
            corrected_content = f.read()
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²
        original_segments = extract_segments(original_content)
        corrected_segments = extract_segments(corrected_content)
        
        # åˆ†æå®Ÿè¡Œ
        analysis = {
            'file_info': {
                'original_file': original_file,
                'corrected_file': corrected_file,
                'original_length': len(original_content),
                'corrected_length': len(corrected_content),
                'segments_count': len(original_segments)
            },
            'corrections': [],
            'statistics': {
                'total_changes': 0,
                'technical_terms': 0,
                'fillers_removed': 0,
                'punctuation_added': 0,
                'naturalness_improved': 0
            }
        }
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åˆ†æ
        for i, (orig, corr) in enumerate(zip(original_segments, corrected_segments)):
            if orig['content'] != corr['content']:
                changes = identify_changes(orig['content'], corr['content'])
                analysis['corrections'].append({
                    'segment_id': i + 1,
                    'timestamp': orig.get('timestamp', 'N/A'),
                    'original': orig['content'][:100] + ('...' if len(orig['content']) > 100 else ''),
                    'corrected': corr['content'][:100] + ('...' if len(corr['content']) > 100 else ''),
                    'changes': changes,
                    'improvement_score': calculate_improvement_score(orig['content'], corr['content'])
                })
                
                # çµ±è¨ˆæ›´æ–°
                analysis['statistics']['total_changes'] += len(changes)
                for change in changes:
                    if 'technical' in change['type']:
                        analysis['statistics']['technical_terms'] += 1
                    elif 'filler' in change['type']:
                        analysis['statistics']['fillers_removed'] += 1
                    elif 'punctuation' in change['type']:
                        analysis['statistics']['punctuation_added'] += 1
                    elif 'natural' in change['type']:
                        analysis['statistics']['naturalness_improved'] += 1
        
        return analysis
        
    except Exception as e:
        return {'error': f'åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}'}

def extract_segments(content: str) -> List[Dict]:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæŠ½å‡º"""
    segments = []
    lines = content.split('\n')
    current_segment = {'timestamp': '', 'content': ''}
    
    for line in lines:
        line = line.strip()
        if re.match(r'\[\d+:\d+:\d+ - \d+:\d+:\d+\]', line):
            if current_segment['content']:
                segments.append(current_segment)
            current_segment = {'timestamp': line, 'content': ''}
        elif line:
            current_segment['content'] += line + ' '
    
    if current_segment['content']:
        segments.append(current_segment)
    
    return segments

def identify_changes(original: str, corrected: str) -> List[Dict]:
    """å¤‰æ›´ç‚¹ã®ç‰¹å®š"""
    changes = []
    
    # å°‚é–€ç”¨èªä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³
    tech_patterns = [
        (r'ãƒ™ãƒ«ãƒˆ|ãƒ™ãƒ«\s*ãƒˆ', 'BERT', 'technical_term'),
        (r'ã‚¸ãƒ¼ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼', 'GPT', 'technical_term'),
        (r'ãƒ©ãƒ¼ãƒ ', 'Llama', 'technical_term'),
        (r'ã‚¨ãƒ«ã‚¨ãƒ ', 'LLM', 'technical_term')
    ]
    
    for pattern, replacement, change_type in tech_patterns:
        if re.search(pattern, original) and replacement in corrected:
            changes.append({
                'type': change_type,
                'description': f'{pattern} â†’ {replacement}',
                'impact': 'high'
            })
    
    # ãƒ•ã‚£ãƒ©ãƒ¼é™¤å»
    filler_patterns = [r'ãˆãƒ¼+', r'ã‚ã®ãƒ¼+', r'ãªã‚“ã‹\s+']
    for pattern in filler_patterns:
        if re.search(pattern, original) and not re.search(pattern, corrected):
            changes.append({
                'type': 'filler_removal',
                'description': f'ãƒ•ã‚£ãƒ©ãƒ¼ "{pattern}" é™¤å»',
                'impact': 'medium'
            })
    
    # å¥èª­ç‚¹è¿½åŠ 
    original_punct = len(re.findall(r'[ã€‚ã€ï¼ï¼Ÿ]', original))
    corrected_punct = len(re.findall(r'[ã€‚ã€ï¼ï¼Ÿ]', corrected))
    if corrected_punct > original_punct:
        changes.append({
            'type': 'punctuation_added',
            'description': f'å¥èª­ç‚¹ {corrected_punct - original_punct} å€‹è¿½åŠ ',
            'impact': 'medium'
        })
    
    return changes

def calculate_improvement_score(original: str, corrected: str) -> float:
    """æ”¹å–„ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    score = 0.0
    
    # é•·ã•ã®é©æ­£åŒ–ï¼ˆå¤§å¹…ãªå‰Šæ¸›/è¿½åŠ ã¯æ¸›ç‚¹ï¼‰
    length_ratio = len(corrected) / max(len(original), 1)
    if 0.8 <= length_ratio <= 1.2:
        score += 0.2
    
    # å°‚é–€ç”¨èªã®æ­£ç¢ºæ€§
    tech_terms = ['BERT', 'GPT', 'Llama', 'LLM', 'Transformer']
    for term in tech_terms:
        if term in corrected:
            score += 0.1
    
    # ãƒ•ã‚£ãƒ©ãƒ¼ã®é™¤å»
    fillers = ['ãˆãƒ¼', 'ã‚ã®ãƒ¼', 'ãªã‚“ã‹']
    filler_count_orig = sum(original.count(f) for f in fillers)
    filler_count_corr = sum(corrected.count(f) for f in fillers)
    if filler_count_corr < filler_count_orig:
        score += 0.2
    
    # å¥èª­ç‚¹ã®é©æ­£ã•
    punct_density = len(re.findall(r'[ã€‚ã€]', corrected)) / max(len(corrected.split()), 1)
    if 0.1 <= punct_density <= 0.3:
        score += 0.2
    
    # è‡ªç„¶ã•ï¼ˆèªå°¾ã®é©æ­£åŒ–ï¼‰
    proper_endings = ['ã¾ã™', 'ã§ã™', 'ã§ã‚ã‚‹', 'ã—ãŸ']
    for ending in proper_endings:
        if ending in corrected:
            score += 0.05
    
    return min(score, 1.0)

def generate_report(analysis: Dict) -> str:
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    if 'error' in analysis:
        return f"âŒ {analysis['error']}"
    
    report = f"""
ğŸ“ è¬›ç¾©ä¿®æ­£å“è³ªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ
{'=' * 50}

ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:
  â€¢ å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {analysis['file_info']['original_file']}
  â€¢ ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«: {analysis['file_info']['corrected_file']}
  â€¢ æ–‡å­—æ•°å¤‰åŒ–: {analysis['file_info']['original_length']:,} â†’ {analysis['file_info']['corrected_length']:,}
  â€¢ å‡¦ç†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {analysis['file_info']['segments_count']}

ğŸ“Š ä¿®æ­£çµ±è¨ˆ:
  â€¢ ç·ä¿®æ­£å›æ•°: {analysis['statistics']['total_changes']}
  â€¢ å°‚é–€ç”¨èªä¿®æ­£: {analysis['statistics']['technical_terms']}
  â€¢ ãƒ•ã‚£ãƒ©ãƒ¼é™¤å»: {analysis['statistics']['fillers_removed']}
  â€¢ å¥èª­ç‚¹è¿½åŠ : {analysis['statistics']['punctuation_added']}
  â€¢ è‡ªç„¶åŒ–ä¿®æ­£: {analysis['statistics']['naturalness_improved']}

ğŸ” ä¿®æ­£è©³ç´°:
"""
    
    for correction in analysis['corrections'][:5]:  # æœ€åˆã®5ä»¶è¡¨ç¤º
        report += f"""
  ğŸ“ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {correction['segment_id']} {correction['timestamp']}
     æ”¹å–„ã‚¹ã‚³ã‚¢: {correction['improvement_score']:.3f}
     ä¿®æ­£å‰: {correction['original']}
     ä¿®æ­£å¾Œ: {correction['corrected']}
     å¤‰æ›´å†…å®¹: {len(correction['changes'])}ä»¶ã®ä¿®æ­£
"""
    
    if len(analysis['corrections']) > 5:
        report += f"\n  ... ä»– {len(analysis['corrections']) - 5} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚‚ä¿®æ­£æ¸ˆã¿\n"
    
    # ç·åˆè©•ä¾¡
    avg_score = sum(c['improvement_score'] for c in analysis['corrections']) / max(len(analysis['corrections']), 1)
    report += f"""
ğŸ† ç·åˆè©•ä¾¡:
  â€¢ å¹³å‡æ”¹å–„ã‚¹ã‚³ã‚¢: {avg_score:.3f} / 1.000
  â€¢ ä¿®æ­£åŠ¹æœ: {'å„ªç§€' if avg_score > 0.7 else 'è‰¯å¥½' if avg_score > 0.5 else 'è¦æ”¹å–„'}
  â€¢ æ¨å¥¨: {'å®Ÿç”¨ãƒ¬ãƒ™ãƒ«' if avg_score > 0.6 else 'èª¿æ•´æ¨å¥¨'}
"""
    
    return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import sys
    
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python3 quality_evaluator.py original_file corrected_file")
        return
    
    original_file = sys.argv[1]
    corrected_file = sys.argv[2]
    
    print("ğŸ” ä¿®æ­£å“è³ªåˆ†æã‚’é–‹å§‹...")
    analysis = analyze_corrections(original_file, corrected_file)
    
    report = generate_report(analysis)
    print(report)
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚ä¿å­˜
    with open('correction_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    print("\nğŸ’¾ è©³ç´°åˆ†æçµæœã‚’ 'correction_analysis.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()

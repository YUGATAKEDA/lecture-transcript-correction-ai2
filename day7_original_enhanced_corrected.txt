[0:00:01 - 0:00:18]
はい。画面見 てますかね。大丈夫そうですかね。はい、撮ります。はい りがとうございます毎回僕なんか最初喋ってたんですけど、何か始めてくれたんでそのまま行っちゃいましょうって感じで。はい、わかりました。はい、 の講師お願いします皆さんよろしくお願いします。

[0:00:18 - 0:00:41]
はい、どうぞよろしくお願いします。では、第7回のRRHFとAlignmentの講義を始めたいと思います。講師の高城です。よろしくお願いします。まず簡単に自己紹介ですが、現在松尾・岩澤研の博士1年の学生です。

[0:00:41 - 0:01:19]
元々高専にいて、そこから阪大の木曽幸生と石黒賢の研究室に所属していました。そこを卒業して、現在は松江にいます。Internとして、光線というプロジェクトのエンジニアのInternや、負けん気のInternを経験しました。専門家として、大規模言語モデルや強化学習に携わっていました。また、ロボティクスにも関わっていました。講義では、DeNA基礎講座や、SIMPOQ学習、ワンスレッド1などの講師を担当しています。さらに、LLM講義や世界モデル内の講義も担当しています。

[0:01:20 - 0:01:56]
右の写真は、岸田首相が来たときに、そのp θという部分でちょっと話題になりましたが、一応、キャッチーな内容を載せています。はい。arXivinterpreterというツールを使っています。今のLLMコミュニティでは、どのチャンネルに入れてあるのか、というところなんですが、このarXivのピンクを投げると、こういうふうに答えてくるようなBotを作り、図表も含めてそういったBotを作って入れています。

[0:01:57 - 0:02:26]
なんか、いろいろまだベータ版ってのもあって、不安定なところもありますので、何か意見や改善案があれば、コミットも募集しているので、よろしくお願いします。はい。今回は目標としてRRHFとDPOスマートランニング、そしてフィードバックとは何か、その仕組みや必要性について講義していきたいなと思います。

[0:02:26 - 0:02:45]
目標としては、そもそもAlignmentとは何なのか、そしてRHFとは何なのか、というところから、PyTorchでもRFを実装するところまで進めたいと思います。内容の前提知識としては、これまでの内容があれば十分理解できる内容だと思います。

[0:02:46 - 0:03:48]
はい。今回はボリュームが多いので、時間内に収まるかどうか若干不安ですが、頑張っていきます。まずは一時間半ぐらいで講義を進め、その後30分ほど演習にしようかと考えています。はい、始めたいと思います。まず、少しおさらいなんですが、Mの訓練法におけるファインチューニングとは何かというと、まず最初にステップ1としてPretrainがあります。その辺りはファインチューニングが関係していると思います。前回、前々回の講義で、ファインチューニングの概念について説明しましたが、それがステップ2として、今回説明するSFTとRRHFが関係しています。一応講義の定義では、このSFTとRRHFを合わせてファインチューニングと呼ばれることもあります。

[0:03:48 - 0:04:10]
また、またPost-Trainingとかそういうふうに言われたりします。今回はこのπチームの中でもステージStepⅢのこのRReplayというところを解説していきます。はい。では一応これでφ部との切り分けですが、強化学習だったり、 とは人間のモデルFeedbackところを次でできる外出していきます。

[0:04:12 - 0:04:33]
はい。ではまず、その基本的なところから説明します。これは大まかに説明しますが、ChatGPTやその前身のInstructGPTなど、最近では基本的にAlignmentの目的で、様々なモデルでRチーフが使われています。

[0:04:33 - 0:04:49]
Alignmentの詳細についてちょっと後ほどまた説明します。簡単に言うと、LLMで同じ問題に対して複数の答 を出力させて、その後に人間がそのいろんな出力ん中でどれがいいのかというのをラベリングして げます。

[0:04:50 - 0:05:10]
そのラベリングの結果、Preferenceと呼ばれることもありますが、その結果をもとに強化学習を行うという流れになります。ちょっとポイントを挙げます。RRHFの応用例ですが、代表的なものにChatGPTがあります。

[0:05:11 - 0:05:37]
これ元々は、GPT-3というものをベースとしていて、このRHFを追加したことで、行動の回答が昨日のように、意味理解ができるようになりました。主に2022年11月に公開されたHPですが、元々のGPT-3との大きな違いは、チャット用に主に設計されていることです。

[0:05:40 - 0:06:11]
さらに応用例としてLLaMA-2だったりLLaMA-2でももちろん使われていて、最近は3.1とか3.2とか出ていると思うんですけど、こういったモデルというのはRRHFを何回も行っています。これ右の図がRチーフした結果どうどのぐらい精度が上がってるのかってのを示してるんですがSFTはここら辺でSFT2回目でRGFを1に三、四個というこれLLaMA-2の例なんですけど、LLaMA-2だと5回行ってます。

[0:06:11 - 0:06:49]
こういうふうにIterativeにRRHFをしていて、どんどん性能が上がっていくというそういったような形になっています。はい。最初になんでRRHFが必要なのかというところを説明していきます。これまでの言語モデル問題点、これはChatGPTとかそ大規模言語モデルより前の古典的なモデルの例になるんですけど、これまでの言語モデルだと人間によっとっては好ましくない発言、設計者が意図しない発言というのを行いって炎上した例というのが度々 りました。

[0:06:49 - 0:07:08]
Microsoftてとかだったら、この人新しかった人大嫌いみたいな言葉を発してしまって、すぐに炎上してしまっただったり韓国のeaseだというサービスだと実施だったり性的少数者に関する差別発言を連発して、サービスを提供停止してますし提出してしまったという例が ります。

[0:07:12 - 0:07:31]
こういった問題ってインストラクションチューニングで解決できないのかというと、非常に難しい問題で、なぜかというと、自然言語のデータを集めるのがコストがかかるというのも理由の一つであり、そもそも何かを言わないようにするという正解データを集めるのが非常に難しいという点にあります。

[0:07:32 - 0:07:56]
なんで直接的に人間の意図をマークさせるってのが結構難しいです。これはテキストダ・ヴィンチずにというChatGPTの前身のインストラクションチューニングをした後、RGFする前のモデルなんですけど、このモデルで設定を行う方法を教 てくださいというふうにプロンプトを与 て げると、待ち答 てしまってる。

[0:07:56 - 0:08:35]
こういった問題を何とかして解決したいというところが理解できます。RRHFによって、そういった意図を学習したいんですけれども、それをどうやるのかというところですね。最初のちらっと説明したんですが、例えば設定を行う方法を教えてほしいというプロンプトが来たときに、いくつかのプロンプト例が返ってくる場合があります。その中には、良くない設定を行う例や、これは設定を行うにはというふうに説明してしまっている例があったり、とはセット犯罪なので、それは強くおすすめしませんみたいな出力例が返ってくるときがあります。このどれがいいのかというランキングをつける形ですね。

[0:08:36 - 0:09:04]
これよりはこれがいいだろうと、この真ん中の釣りはもうこっちの一番綺麗なものがいいだろうみたいなのをPreferenceとしてつけて げるこれをモデルにFeedbackして げるという形です。実際にこのRRHFした後のモデルtextダメージ03だと、窃盗を行う方法を教 てくださいというプロンプトを与 ても、こういうふうに窃盗犯罪ですのでというふうに教 てくれないならこういったことを学習したいという形になります。

[0:09:06 - 0:09:32]
ヒューマングループ型のアプローチモデルに対してフィードバックを行うことで、そのアウトプットに基づいて再度フィードバックを繰り返すようなアプローチになります。これまで説明したように、こういった人間の意図に沿ってモデルを学習させることをAlignmentと呼んでおり、このAlignmentを行うためにもRRHFを使用しているという形です。

[0:09:34 - 0:09:57]
元々OpenAIが提唱したものです。Alignmentチームというものが存在します。そもそも人間の意図について、これまでの議論では、そのような意図は何かという分類があるのですが、他にも明示的な人と暗黙的な人の二つが存在しています。例えば、明示的な人は、言語化して伝えるような人ですね。

[0:09:58 - 0:10:36]
こういった指示に従ってくださいとか、こういうアシスタントとして振舞ってくださいというプロンプトとして伝わらないというのは、SFTでも学習しやすいんですが、こういった暗黙的なものは言語化していません。対話において、いわゆる当たり前とされているような、例えば捏造するしないとか、有害なこと言わないというBias（バイアス）が働いているようなことを言わないというのは、いちいちプロンプトには書かないですが、そういったものは標準的に即して欲しい、つまり組み込みたいというのがこのAlignmentの人になります。

[0:10:38 - 0:11:01]
その意図ってのはどういう基準が るかってところなんですが代表的なところにこのHelpfulとHonestとHarmlessというのが りまして。Helpfulというのはできるだけ簡潔で効率的な情報量が多い回答を、まずは質問に対して行うさらに不足情報が ったら、適正な質問を投げかけるみたいなところをHelpfulという指標で測っています。

[0:11:01 - 0:11:30]
【修正後】（修正されたテキストのみを出力してください）

ChatGPTやその他の大規模言語モデルでも、プロンプトの情報が不足していると、こういったことが起こり、より詳細に解決できるかもしれませんみたいな返ってくると思うんですけど、そういったことですね。とHonestですね。これはHallucinationしない、つまり嘘をついて捏造したりしないというところです。Harmlessというのは、差別的な発言をしない、といった三つの軸でAlignmentを図れられることが多いです。

[0:11:33 - 0:11:54]
この上にある三つの要素を組み合わせてH1と呼ばれるものがあり、これらの要素を満たすAIを定義しているものもあるんですが、一応Alignmentの基準なので、複数の基準が考案されています。例えば、日羽やマイナースペックなど、さまざまな基準が考えられています。

[0:11:58 - 0:12:16]
今言った三つの基準ですね、Helpful、Honest、Harmlessでそれぞれどのようなものがあるのかについて具体的に説明が行われていますが、Honestに該当するものとしてTruthfulQA旧というデータセットを用いたり、はるばるというものを使用したりしています。

[0:12:16 - 0:12:40]
クエスチョンに対して正しい答えと、Hallucinationのアンサーが存在するような、そのようなデータだったり、Harmlessという、これはプロクラウスポリスというデータセットになりますけど、こういったアフリカンだったり、白人の文章が存在するときに、こういったものにBiasが存在するかどうかを判定するようなデータになっています。

[0:12:42 - 0:13:16]
その他の基準についてなんですが、NLIが持っている死亡リスクについて、3タイプの合計60のリスクタイプに分割して評価を行うという研究が行われました。これが答えというものなんですが、システムも大変ですが、悔いの残る仕様だったり、誤った情報による外のようなリスクというものを細かく分析して、それぞれデータセットを作っていって、そういったものも含まれています。

[0:13:18 - 0:13:50]
こういったふうに人間にフィードバックをしていくと、どんどん賢くなるのかというところなんですが、タスクは複雑になっていくため、そもそもその文章が良いかどうかというのが、人が評価できなくなるという点をOpenAIは考慮しています。例えば、論文の評価の場合、その分野の専門の研究者なら理解できるかもしれませんが、一般人が二つの論文を見て、どっちが良いのかを判断するのはなかなか難しいでしょう。

[0:13:50 - 0:14:19]
その点が重要だと思います。なぜなら、そういったものだと逃げた体では評価できなくなるんですが、ここですね、この緑の線が人間が評価できる限界だとすると、AIアシスタントの力を借りると、どんどん越えて評価できるものが、より難しいタスクを評価できるようになって、さらに進化していくんじゃないかってところが、ロープウェイからこういった概念が出されてます。

[0:14:21 - 0:14:52]
これはスーパーインテリジェンスのSuperalignmentですね。Superalignmentというものは、将来どんどん賢いAGIのようなものが作られたとき、そのAIが暴走しないように、人間の意思で制御できるのかという点で、はるかにこの人よりも賢くなってしまったAIがその人間の意図通りに動作するようにするにはどうすればいいのかという問題について、OpenAIを研究していました。そして今、このSuperalignmentチームは解散していて、なくなっちゃっていますが、一応こういった概念というのも提案されていました。

[0:14:54 - 0:15:27]
アナロジーですねこれは、これエクストラ無事アルデシンというんですか、これは何かというと今までのAIだと、人間が教師として家を学習していくという流れなんですがSuperalignmentとこのスチューデントの方が、AIの方がどんどん賢くなってしまっているのでこれをどうAlignmentしていくかなんですがそれを行うために、同じアナロジーとして小さいモデル小さいLanguageモデルを使ってキーランRaceモデルをAlignmentさせる。

[0:15:28 - 0:15:46]
テキストを修正して、自然で正確な日本語に直します。

【修正後】
テキストを進めると、ストロングジェネレーションというもので、ロームとして公開されています。これが本当にできるかどうかは、まだ少し怪しいところもありますが、このようなものを研究しているところがあります。

[0:15:47 - 0:16:25]
でしょうか？ とは、AGIとAlignmentの関係性についてで、AGIの実現が大規模言語モデルの登場によって極めて現実的になってきていて、Alignmentの重要性が推進されているってのは、言われていて、ここちょっと怪しいというか、本当にそうなのかって疑問に思う人もいるかもしれませんが、OpenAIとかはAGIがAlignmentされていないと、今後人類重大リスクをもたらす可能性があるというところで、こういったようなリスクの種類をいくつか分割して、こういったリスクが起こるんじゃないかというのを真剣に考 えている。

[0:16:26 - 0:16:48]
ていう形になります。ここら辺はちょっと意見がわかれと思うんでこういったふうな主張も るんだなぐらいで聞いてください。はい。この辺でもR1Fのマイクというか、なんでAlignmentしなきゃいけないのというところを説明したんですが、この後にも具体的に るチーフの基礎について入っていきます。

[0:16:51 - 0:17:19]
RRHFの学習は何回も行っているんですが、この三つのステップで構成されています。少し①の会なので、少しくどいになりますが、1個ずつ説明していきます。まずこのRGF全体像を理解するために、最初の最後のStep1強化学習について、Q学習ってそもそも何なのかってところから少しだけ話しましょう。

[0:17:21 - 0:17:44]
強化学習というのは何かというと、こういう環境と何かAgentが ったときに、このAgentというのが何か環境の状態に基づいて行動します。この行動の方策のことをPolicyとか言ったりするんですけど、このAgentか何かしら行動すると、環境に何かしらの影響を与 て、環境の状態が変わります。

[0:17:45 - 0:18:12]
それプラス、環境の環境からRewardというのを経て、どういう行動をすれば、こういう報酬が得られるんだなというところを理解した上で、次の行動を決定していくそういった流れになります。強化学習の目的としては、どういう行動をすれば一番環境からの報酬を最大化できるのかという、この行動方策を学習するってのが強化学習の目的になります。

[0:18:13 - 0:18:31]
例 ば囲碁の場合だったら、状態というのがこの盤面の5-1ですね。行動というのは自分がどこに石を置くのか報酬というのは、その囲碁に勝つか負けるか。というような形でAgent学習することができるわけです。

[0:18:32 - 0:18:45]
これどうやって学習するのかというところなんですが、その前にですね、強化学習の応用例として、いろいろ自動運転やロボット制御、囲碁など、ChatGPTでもRL（強化学習）を使っていますよというものですね。

[0:18:48 - 0:19:16]
使われているQ学習の手法なんですが、そもそも強化学習の手法はたくさんあります。ここに載っているだけでもわかると思うんですが、たくさんあります。今回使うのがこのPPOというものですね。PPOはOpenAIが開発したアルゴリズムで、デファクトスタンダードとしてよく使われています。

[0:19:19 - 0:19:37]
はい。強化学習のPPOってどういったグッズアルゴリズムなのかというのを簡単にこれ開始するんですが、RRHFでもよく使われてる強化学習のアルゴリズムでActor-Criticって呼ばれるアルゴリズムになっています。

[0:19:37 - 0:20:12]
Actor-Criticとは、このActorという部分が、環境からの情報を基に行動を決定するエージェントの一部ですが、その意味する方策というものがあります。一方、Criticは、このActorが取った行動と環境からの情報を基に、その行動が本当に良いものだったかどうかを評価する役割を担っています。このActorとCriticが相互に連携して学習を進めることで、最終的に環境からの報酬を最大化することができるという仕組みになっています。

[0:20:12 - 0:20:38]
ちょっと概念的な説明で申し訳ないんですけど、こういったようなアルゴリズムになっています。もう少し詳細に説明すると、時間が厳しいかもしれませんので、ざっくりと説明しますが、このPPOでは方策勾配法を用いて学習しています。

[0:20:38 - 0:20:52]
これは何かというと、方策、さっき言ったActorだったり、Criticというのは、パラメータを持ったネットワークで表現できるので、それの勾配を直接計算して、最適化を行うというのがこの手法になってます。

[0:20:52 - 0:21:33]
目的関数はこんな感じになってるんですが、詳しく知りたい人は、この最後の方にサンコー向けのしてるんで、そこら辺を参照していただけたらなと思います。これもPPOの説明になるんですが、元々PRPPOって言われる方策勾配法の中でも、更新幅をKL距離をというそれを掛けて更新するような、モデルが ってそれを簡略したものがPPOになるんですが、ちょっと多分何言ってるかわかんないと思うんで大体こんな感じのアミューズを使ってるんだなというぐらいの理解で今一旦大丈夫です。

[0:21:35 - 0:22:15]
はい。Q学習を学ぶための資料として、CourseraやAdobeなど、PolicyBasedの授業などがあります。これらは無料で見ることができ、参考になると思います。また、強化学習のサマースクールや強化学習の講義を県でも開催しているので、そういったものに応募して、1から勉強するのもいいと思います。Q学習の第2波ですね、これは松尾・岩澤研で翻訳したものですが、非常にわかりやすいので、こういったものを参考にしてみてください。

[0:22:16 - 0:22:31]
この後の説明では、PPOの具体的な詳細がわからなくても、一応理解できるような構成になっていますので、一旦こういったアドレスが使われる仕組みを理解していただければ大丈夫です。はい。

[0:22:35 - 0:22:58]
次に、人間のフィードバックを用いて同居学習を行うのかという点になりますが、これはOpenAIが発表したRanking from Human Preferenceシリーズという論文が起源となっており、これは言語モデルではなく、ロボットの学習に人間のフィードバックを用いるものです。

[0:22:59 - 0:23:14]
AtariというシミュレーターからAtariというゲームのですねシミュレーターと、 と0Bot種メーターを使って人間のFeedbackを用いることによってサンプル効率よく学習することができたというのがこの部位になってます。

[0:23:14 - 0:23:37]
これもStep1から3までわかれていて、最初に方策が環境で報酬祭だけするように、丸の内の学習するんですが、その出力した行動の中から二つを選択して、その二つの中でどっちがいい行動だったのかというの人が目で見て評価する形になります。

[0:23:37 - 0:24:12]
その評価の結果をもとにRewardフィルターを使って学習する。プロジェクターを学習するというのが重要です。つまり、これが報酬モデルの学習に当たるものですね。実際にアンケートする内容としては、例えば、この棒がバク転するのを学習させたい場合、左のものと右のもので、どっちがうまくバク転できているのかというのを、このようなふうにPolicyを設定して、Preferenceをつけるというものになっています。

[0:24:13 - 0:24:36]
AIは、人間の選択を最もよく説明するような報酬関数を見つけて、その報酬関数を使って強化学習を行うことで、より人間が見て「ちゃんとバク転している」ものというものを学習していきます。

[0:24:39 - 0:25:11]
このロボットタスクでうまくいったRHFのようなものなんですけど、これを言語モデルに応用したのがこの論文になっています。これは最初の初期のピーマンフィードバックを使った言語モデルの強化学習で、初期のものなんですが、これはGPT-2を学習させるのに火はフィードバックを使っています。方法としては、今RRHFにかなり似てきていますが、言語モデルは四つの出力をします。

[0:25:12 - 0:25:29]
四つ出力された中で、最も良いものを選択して、Rewardモデルを学習していく流れになります。PolicyとRewardモデルのトレーニングを交互に行っていくようなものです。これだと感情分析タスクですね。

[0:25:30 - 0:25:49]
の文章がポジティブなのかネガティブなのかを判断するタスクで性能が良くなったというところの論文についてです。さらにそれが発展されたものがこの論文になっていて、これは人気のフィードバックをようやくタスクに適用したものになります。

[0:25:49 - 0:26:17]
これももうほぼ、かなり現在のRRHFに近いというか、ほぼ同じ枠組みなんですが、予約モデルの予約タスクについてのみ学習を行っていて、ファインチューニングの結果よりも紐づけを使った結果が大きく上回っています。このリファレンススコアというのが人間が作成した要約のスコアになるんですが、それよりも良くなっているというところが挙げられます。

[0:26:19 - 0:26:39]
この成功を見て、最終的にOpenAIが作ったのがこのインパクトGPTというもので、これがいわゆるRRHFと呼ばれるものです。これはChatGPTの前身であり、インストGPTで用いられていて、タスクではなく、既存のこのGPT-3をアラインメントすることが目的となっています。

[0:26:41 - 0:27:06]
先ほどお話ししたようなH1ですね。ハーフとかHarmlessとかHelpfulの基準プラス、チャット用にAlignmentするというところをやっています。段階としては、前半でお話しした通り、まずSFTを行い、次に報酬モデルを学習し、最後に協調させるという流れになっています。

[0:27:07 - 0:27:26]
一応ちょっとくどいようですが紹介詳細を解説します。おっしゃった。まずStep1では、プロンプトモデルセット最初に用意しますそのプロンプトに対する人間のLabelerの回答をもとに教師 り学習というのを行います。

[0:27:26 - 0:27:52]
ここはSFTのものですね。これは普通のFeedbackというのはデータセットを使ってただ強者で学習してるだけという形になります。その後にSFTしたモデルに対して、これと四つですね。ABCDって四つを出力して げて、その出力の結果というのを人間がどれがいいのかなというところをランク付けする流れになります。

[0:27:53 - 0:28:26]
その後に報酬モデルを学習します。このランク付けと同じになるように、報酬モデルを学習したいというのが目的になります。その後に特定のプロンプトに対して、GPTのモデルの出力をするわけですが、その出力したものに対して、Rewardモデルを介してRewardが出るわけですが、それをもとにPPO強化学習をするという流れになります。

[0:28:26 - 0:28:52]
これをどんどんGoogleGoogle繰り返すというのが、のInstructGPTの流れになります。報酬モデルの学習になるんですが一応直感的な理解として、一応図解して るんですが、元のInstructGPTのモデルだとKが4から9の出力の力、二つの組について全てランク付けを行うというのがやってます。

[0:28:53 - 0:29:23]
この下の例だと、傾向4つまり四つの出力に対して、二つの組をそれぞれランク付けして げる形になっていて、例 ばこのお金持ちにはどうすればいいでしょうみたいなアプリとか ったときに、SFTモデルが四つ回答して げて、この四つの中の二つのペア、つまり4Cの6通りですね、これ6通り書いてないんですが、6東陵全部ペア作って げて、それを人間がどっちがいいのかというのを る程度して げる、そういった流れになります。

[0:29:26 - 0:29:52]
よいしょ実際に報酬モデルの各州どうやってるのかというところの具体のところなんですが、報酬モデルで何したいかというと、この確率分布のリードされたい顔したいこれ何かというと、さっきの人間のアノテーションによってプロンプトXと とそれに対する回答こっちですね。

[0:29:52 - 0:30:31]
YBこれWってのがYBYLってのがワイルズを指してるんですが、いい回答と悪い回答のこの三つのものが得られます。やりたいのは、Yウィンというのが、ワイルズよりもわかっている確率というものを求めたいというものです。この確率の誘導を最大化することで、これがこの確率を求めるように最適化を行うんですが、この確率はどういうふうに表せるかというと、RRHFではブラッドReplayモデルというのにこのデータが従うと仮定します。

[0:30:32 - 0:30:55]
これ何かというと、このワイルズ弱みが勝ってる確率というのをこういうふうにしかし上げます。これ何してるかというと、このRというのが報酬換水なってるんですが、つまりXとYが与 るときに、何か絶対値のスカラーが取得されるようなそういった関数になってます。

[0:30:56 - 0:31:19]
これ、この報酬関数にYBワイルズの結果を与 て げて、ワインの確率を分子に取ってるんでソフトマックスをかけてるような形になります確実にしているそういったモデルになってます。これ書き換 て げるとφGoogleカードを使って、こういうふうに書くことができます。

[0:31:19 - 0:31:41]
なんで、要するにこれを代入して げると、このログp θのところに、この仕組みの数を使ったものを代入して げるとこのような形にかけます。つまりはやってることは一緒です。ブラッドも減りモデルに従うこういった確率分布に対して、家を最大化して、そういった流れになります。

[0:31:42 - 0:32:04]
つまり、やっていることは日群のようなもので、良い回答ではr(x,y)ウィンのペアの報酬を、悪い回答ではr(x,y)ルースのペアの報酬よりも高くなるように、高くなる確率を学習していくと、そういったものになっていきます。よいしょはい。

[0:32:06 - 0:32:35]
報酬関数はさっきの目的関数で求まったとします。その次にその報酬関数を用いて強化学習をするという流れになります。単純に考 ると、r(x,y)のピアノピアノ文章が って、それに対する報酬というのを最大化するように学習すればいいんじゃないかってこういった目的関数が想像できると思います。

[0:32:35 - 0:32:57]
これは、牧田優が設定されていない報酬なので、通常の教育では、各ステップにおいて累積した報酬を採用していると思いますが、この言語モデルの強化学習では、このような目的関数になっていて、これは文脈付きバンディットとも呼ばれます。

[0:32:59 - 0:33:19]
これを学習する必要があると思いますが、単純にこのままではうまく学習できないので、工夫が必要です。先ほどのobjectiveだと何が問題かというと、この二つの問題が起こる可能性があるんです。一つはRewardHackingと呼ばれるもの、二つはAlignmentXと呼ばれるものです。

[0:33:21 - 0:33:53]
一つ目の問題としては、RewardHackingというものが起きるということです。これは報酬を最大化することを目的に、どんどん方策を学習していくモデルが、その際にRewardモデルの欠点を突いて、本当は良くない文章なのに、高い報酬が出るような入力を上手く見つけてそれを最大化してしまうような現象が起きてしまいます。これはなぜ起きるかというと、Rewardモデルが完全に心の報酬関数を近似できないからです。

[0:33:53 - 0:34:18]
ゆ に起きることなんですがそういったような、 るいはハックのことをしてしまうのが問題で ります。こういう発行してしまうと何か長い文章をRewardモデルが豊か報酬つけたがるみたいなBiasがちょっと ったりするんですが、そういったBiasを工夫して、どんどん長い文章を、ばっかりを作ってしまうとかそういった問題が起こってします。

[0:34:19 - 0:34:40]
その対策としてKLペナルティというものがあります。これは、生成する文章が元のSFTモデルから大きく変わりすぎないように制御するものです。具体的には、元のRに対して、この項を追加することで、生成される文章を制御しています。

[0:34:41 - 0:35:07]
ベータというのは、このKL項というのをどのぐらい考慮するかというハイパーパラメータになっていて、これは何してるかというと、これ普通に分解して げると-βのこのログの期待値取ってるんで、ここがこのπφのRLとφSFTのKL距離になります。

[0:35:07 - 0:35:26]
つまり、この分布がどれぐらい違うのかという距離を測っているものです。これが遠ければ遠いほどペナルティが大きくなるので、どのように大きく変わってしまうのか、それをできるだけ最小化しつつ、報酬も最大化するという目的関数になります。

[0:35:29 - 0:36:01]
先ほどのKL距離について見てもらえば、ちょっと気づく方もいるかもしれませんが、KL距離というのは、こういうふうに表されて箱優先ですね。若干違うんですが、こういうふうに合わせて、これを先ほどの説明に書き下すと、このログのπRL分子がπRで、分母がいっぱいSFTなんですが、これがリバースケールでもなります。

[0:36:01 - 0:36:33]
つまり今学習してる学習したいモデルが、この後ろの方に るのが余りフォワードKLで、この前の方に るガリバースケールになってます。これは、RGAEの場合だとリバースケールを使うんですが、これがなんでかというと、フォワードKLの場合は、この右の図のようにターゲットが ったときに、このターゲット全体をカバーするように学習してしまいます。

[0:36:34 - 0:36:59]
一方で、ガリバースケールだとこの特定のモードを学習カバーするように学習するここはだから無視しても全然ペナルティにならないという形になります。今やりたいのは、元のターゲットから大きく変 るような学習はして欲しくないんで、このリバースケールのものを使います。

[0:36:59 - 0:37:24]
つまり今、最初の一番最初の段階では、元々のモデルと今学習したモデルってのは完全に一致していて、そこからRewardを使って文法を変 ていくという流れなんで最初は一致してるんすね。この一度できるだけ維持したまま詳細玲香させたいので、このリバースケールの方を使うという流れになっています。

[0:37:26 - 0:37:58]
二つ目の問題として、AlignmentTaxというものが存在します。これは、人間の意図通りにこのモデルを学習しようとすると、パラダイムシステムが劣化してしまうと、事前知識の忘却のようなことが起きるというものです。これを「アラインメント税」と呼んでいますが、こういった問題が起こります。こういった問題を解決するために、リプレイという手法が使われます。これは単純な手法で、事前学習データの一部を用いて、その誘導最大化のようなことを行います。

[0:37:59 - 0:38:34]
つまり、通常のPretrainのデータを使用して、Pretrainの目的関数に先ほどのRRHFのf-divergenceを追加するような流れになります。要するに、こういった方法で、主人のデータDPretrainを使用して、多様性の維持を行っています。具体的には、PretrainのデータからXサンプルを抽出し、そのXについて、この言語モデルの方策は現在の言語モデルなので、言語モデルの確率の尤度最大化を行っていく流れになります。

[0:38:35 - 0:38:58]
この努力が、そのReplayがどれだけ考慮するかという点で重要であり、博士の維持が可能になりますが、今週は考慮しすぎてしまうと機能しなくなるので、あまり適切ではありません。小さいと報酬よりもジュースの方が優先されるので、何か制度が劣化しやすくなる傾向があります。このパラメータハイパーポールのようなものを追加することで、顧客の離脱を防ぐという流れになっています。

[0:39:00 - 0:39:22]
はい。PPO-ptxというものは、先ほどのKLペナルティとReplayの二つをクリアしたもので、Instruct GPTで使われている目的達成になります。その仕組みは、先ほどの二つのものを追加しただけの単純な式になっています。

[0:39:24 - 0:40:08]
この目的関数で学習すると、元々のSFTと比べてもさらにPPOのモデルと比べても性能改善が見られたという結果が出ています。米Stepの最終的な評価なんですけれども、GPT-3と比較してより正しい指示に従って、Hallucination（妄想）だったりも抑えられている結果になっていて、さらによく、日本語で返し、日本語で売っているので英語で返してしまうみたいな問題も、これを使うことで抑えられているという結果になっています。つまり、英語で打つと英語で返してくれるみたいです。

[0:40:08 - 0:40:28]
そういったことですね。公開データセットでの評価というところで、Alignmentの本質的な意味は何かというと、真実性や無害性といった点に焦点を当てているということになります。具体的には、Harmlessness村一成のような言語モデルの実装を紹介しています。

[0:40:29 - 0:40:49]
これは何か悪意の る程度セットってか海の るデータだけはかないかというところで、これは小さい方がいいんですけど、MSXGPTが一番使 ばいいTruthfulQAは真実性ですね、より正しいこと言ってるかという判断ですが、これはSFTと比べてもだいぶ良くなってるって形になります。

[0:40:49 - 0:41:15]
実際、性的な評価も明らかに良くなっているというところです。ただ、PPO-ptxを使えば、十分に学習できるかという点で実はそんなことはありません。RLをやっている人はわかるかと思いますが、基本的に学習が不安定で、さらにこのハイパーパラメータが非対称に多いんですね。

[0:41:15 - 0:41:28]
PPOの中でも結構多くて、しかも何か細かい実装のテクニックが必要だったりして、すごい調整しないとうまく性能が出ないというのが ります。なので基本的にこのRというのはみんなやりたくないというところです。

[0:41:30 - 0:41:58]
そこら辺を解決するためにこのPPO-maxというものがあります。従来の強化学習の教育では、学習安定化のためのさまざまなテクニックが使われてきました。例えば、報酬Clippingや、ペナルティの追加などがあります。これらのテクニックを駆使して、学習を何とか可能にしているのがPPO-maxというものです。

[0:42:00 - 0:42:23]
詳細は割愛するんですがこんぐらい、 の工夫しないとなかなかうまくいかないんだよってところを知ってほしいというところです。 れこれはPPO-maxの結果ですね、一応このPPO-maxを使うことによって、長期的にも安定した結果を実現してSFTモデルよりも、精度が良くなったよってのが右の図になってます。

[0:42:24 - 0:43:31]
はい。そ、そんな感じで都丸チーフってのは学習されていきます。でも一応これ、全体像、復習として一応出してるんですが今話しながらですね強化学習して報酬モデル学習して教育するという流れになっています。どういった学習データ使うかというところについてちょっと話していきたいと思うんですがFeedbackは先ほどの説明だと、二つのペアが って、どっちがいいのかというのをアノテーションしていくというふうに説明したんですが、Feedbackタイプはその他にもいろいろ って例 ば絶対値の数値すから値で与 て げたり、例 ば何か10段階評価で5とか、そういうことですねとか とはRankingってのはペアを評価さんと一緒なんですが、自然言語というのは るプロに対してどこが悪かったのかというのを自然言語の形式でFeedback与 るみたいなそういったいろんなFeedbackKCが一応考 られてはいます。

[0:43:35 - 0:44:06]
ただ学習は難しくなるので、どのように学習するかという点が一応問題になりました。学習意欲と、使用されるデータセットとしては、HH-RLHFと呼ばれるものがあります。このH1というのは、先ほども説明しましたが、テープRaceとHelpfulの二つを組み合わせたもので、HH-RLHFと呼ばれています。これはHPというスタンフォードNPがリリースしているものです。

[0:44:07 - 0:44:41]
物がたりたりとは、Feedbackとか、HGRチェフでは、こういうのプロンプトに対して、シューズってこれが出力ですね。出力とリジェクトサンプル、悪い出力の二つが与られているって形になります。その他の出せたとしても、いろいろってhealthcareとか輸入RGFとかUltraFeedbackみたいな詳細はちょっと説明しないんですけど、そういったセットもいろいろ提案されています。

[0:44:45 - 0:45:07]
データの集め方ですが、こういったデータセットをどのように集めればいいのかという点についてです。元々のインストラクションソフトである GPT の論文では、さまざまな工夫がされていて、そもそもそれが意味をなさないので、ラベラーの選択という部分からどのように進めればいいのか、論文に書かれています。

[0:45:09 - 0:45:34]
まず少数のデータにラベル付けを行いますこのWづけこの人本当に間違いが絶対ないような、限られた人で、ラベル付けを行いますそのラベル付けを行った結果をもとに、他の人をスクリーニングして、そのベルトの位置度合いが高いLabelerってのが信頼できるLabelerだとして、スクリーニングを行います。

[0:45:36 - 0:45:57]
そういったLabelerに対しLabelerを選択して、その人に対してクラウドソーシングか何かでどっちのPROのかというのを選択してもらってデータを集めるってのがわかります。 とはLabelerの特性が偏らないように、統計データというのはアンケートを用いて収集してまで契約Biasが載ったような状態ならないようにするというところの工夫も行われてます。

[0:45:59 - 0:46:21]
Labelerのマニュアルみたいなのを作って、WebGUIでこういった形でポチやっていくみたいなそういった集め方にInstructGPTではなっています。RRHFを実装するためのライバルとして、いろいろ って、TRLだったり日RXRLforMLとかReplayチャットみたいなチャット形式のモデル学習できるようなモデルも ったりします。

[0:46:22 - 0:46:43]
それぞれのモデルで何が違うのかというと、PROとPPOだけ実装してるんですが、TRXだとKTOも別のメソッドを使 たり、 とはこのRLforMLだと、他の様々な強化学習を使ってたり と報酬関数だったりいろんなトリックが使 るってところでライブラリの差別化がされています。

[0:46:45 - 0:47:11]
RRHFの評価なんですけど、一番発信とRealignmentの基準で、HonestHelpfulnessとHarmlessnessの三つの基準が一般的に使われています。HonestSだとTruthfulQA Atariは、HelpfulnessだとRHFとか、HarmlessHarmlessnessだとクラスプレスとか、ジェンダーとか、そういったものが使われています。

[0:47:13 - 0:47:35]
評価に関するデータセットの一応説明なんですがloss9Aは真実を評価するようなベンチマークになっていていろんなカテゴリー38のカテゴリーに分け、または810名の質問と回答を用意して るって形でこの評価ってのはどうするかというと、もうφに記されたBPSDを使って評価を自動化するような、そういった流れになっています。

[0:47:35 - 0:47:53]
はるばるというのは、イエスノーで単語の出力になっている自動評価とかは、GPT-3とか別のモデルを使う必要はありません。完全解凍の一律を見て、どれだけHallucinationを起こしていないのかという評価を行っています。

[0:47:54 - 0:48:38]
はい。1R1フェア前半でも何度も説明したので、ちょっと飛ばしますが皮膚RaceとHarmlessに関する一つも評価にも使ったりします。 とはクラウスパリスというAgentですね。lossPressは肌の色とか政治とか宗教とか年齢とか、そういった様々なBiasに関する評価でベストになっていて、例 ばこの右の図がCrows-Parisす左ですね、例がクロスプラスなんですが、前半の方で説明したようにフリー感だったり、割と白人だったり とMLメールに関するデータとか、 とオールドヤングという時に関するBiasとかが含まれてますね。

[0:48:38 - 0:49:13]
Atariの人という偏りがある技術的な問題について、そのハードルが必須なのか、それとも税なのかという評価セットになっています。 とはFLASKというものがあり、これはOpen-set問題のセットで、答えがないOpen-setのベンチマークになっています。ロジカルシンキングやバックトラッキングなど、合計12個のスキルを評価するものです。

[0:49:14 - 0:49:41]
GPT-4を用いてそれぞれの評価を5段階評価している。論文では、人間ベースの評価とGPT-4ベースの結果が同様の傾向を示しているという主張がなされている。左が人間の評価で、右がGPT-4の評価だが、同じというのかは若干微妙なところだ。しかし、少なくとも人所関係については、そう考えるというふうに元論文では示されている。

[0:49:42 - 0:50:03]
とは前半でも少し説明しましたが、うどんと関連するゲームのリスク評価のための包括的なデータセットも評価や学習に使用されます。これもGPT-4によって各カテゴリーに該当するかを0か1で判定しているようなものになっています。

[0:50:05 - 0:50:37]
とは報酬モデル評価ですね。報酬モデルがちゃんと心の報酬生地的できているのかってところを評価してるのがこのRewardベンチになっていて、いろんな観点のデータセットを、が包括されていて、しかもこのリーダーボードも公開されていて今が今どれが一番いいRewardモデルなのかというのが出ていますこれ一応昨日の結果なんですけどLLaMA-2り3.1mというのが、70ビジョンが一番今評価としては高いものになってます。

[0:50:39 - 0:51:15]
はい次に、1R1Fの発展的内容に行くんですが、これが40分ぐらいなので、一旦この発展的内容をやってから質問か。一旦ちょっとバーっと説明してしまいますね。はい。今では基礎やってきたんですが、いろんな発展手法が って先ほどのRL使いたくないという話もそうなんですが、いろいろ苦労してるところを改善するような手法というのがいろいろ出てきてます。

[0:51:16 - 0:51:40]
その代表的な指標としてDPOというものがあります。これは、RLフリーのアルゴリズムになりますが、これが最もデファクトスタンダードでよく使われているアルゴリズムです。例えば、ミストラルでも使われており、他の多くのオープンなモデルでも活用されています。今回は、このDPOについて重点的に見ていきたいと思います。

[0:51:40 - 0:52:00]
その後に長谷部長の派生仕様もたくさん出ているので、その説明と、その他のAlignment賞についても説明していきたいと思います。DPOとは、報酬モデルを使用せずに直接Preference Rankingを学習するものです。

[0:52:02 - 0:52:19]
今までのRRHFでは、最初に報酬モデルを学習して、その後に強化学習という流れでしたが、その強化学習はできればやりたくないよねというのは前回話しましたが、そのステップを飛ばしたのが、DPODirectPreferenceOptimizationというものです。

[0:52:20 - 0:52:40]
Rewardモデルは暗黙的に定義されていて、こういった式で定義されますこれはちょっと後ほどまた詳しく説明します。こういうことをすることによって、実質的に強者学習のみをすることによって、RRHFと同等な効果というのがこのDPOになります。

[0:52:40 - 0:53:07]
このDPOの勾配はこういう式で表されますが、ちょっとここ、ぱっと見よくわかんないと思うんですが、一応概念だけ説明すると、ここの部分がワイルズの方が先に来て、そこから報酬を引いていますが、推定報酬、これは今Rθを学習しているものなので、推定報酬が間違っている分だけを修正しています。

[0:53:10 - 0:53:34]
間違ったものに高い重みをつけて、間違いのないものには小さい重みをつけるというのがこの後のプロセスです。YBが生成される確率を最大化し、その後は悪い方の回答をより生成されないように、茉夕度最小化のようなことをしている、そういった購買になっているのです。

[0:53:34 - 0:53:58]
ちょっと具体的にここを説明していきたいと思います。DPOとRチーフというのは、数式的に等価になります。それが何でかというところを説明していると思います。つまり、RRHFのRLをしなくても、何でこの強化学習だけでRチーフと等価の式なのかってところですね。

[0:54:00 - 0:54:20]
RRHFの目的関数というのは、こういったふうに表されました。これ先ほど説明したものと全く同じになってます。心の報酬を気にするために、 のブラッドリーモデルを使用して報酬モデルを学習していくというのが最初の流れでその後にその報酬を使ってRしていくというのが主でした。

[0:54:21 - 0:54:37]
目的はこれになるんですが、これというのは、この最適解はクローズドフォームにとっことができて、つまりこれを満たすようなこのπという最適なπというのが、これを式変形することによって得ることができます。

[0:54:38 - 0:54:58]
その結果ってのがこのような結果になっていて、これが元々のSFTのもとのモデルを、ベータ分のRで、このEXPOのイニシャルをかけた分、ものになっていて、つまりこの元の分布をこのXポテンシャルで分布変 てるみたいなそういったイメージですね。

[0:54:58 - 0:55:25]
なってます。Zというのは、全体を確立するために使われる分配関数と呼ばれるものです。Zはこのような形で表されます。これは少し難しく聞こえるかもしれませんが、よく見ると簡単です。この上のπSFT×EXPO電車の部分で、Yに対して相関を取っている形になります。

[0:55:25 - 0:55:45]
なんでこれをそのままYで全部そう取ってるんです。が、このZってのは直接計算は深野やってます。なんでかというと、RXに対して、全ての考 うる具合を計算して足して げるみたいな、こうやってるんで直接これは負荷計算不可能です。

[0:55:49 - 0:56:11]
先ほどのクルーズホームに最適化計算できるって言ったんですが、それはどのように行っているのかを一応軽く説明しますね。式変形自体は結構簡単な式変形の導出でできており、これは元の論文からそのまま引用しています。πFと書いてある部分は、φSFTだと思って読み替えてください。

[0:56:11 - 0:56:30]
元々の目的ますこれなんですが、これをそのまま試験して げます。この下の式では、これ単純に れですね、 のKLの子を中に入れるだけですね、中に入れてまとめただけです。これをどう変形していくかってことなんですが、最初にこのマイナスデータで終わっております。

[0:56:30 - 0:56:51]
ML全てで割って げると、この部分ところとRが-データ分の1πされますねマイナスでは待ってるので最大化問題じゃなくて最初かもしれないます。このその次にこのマイナスべた分の1というのを、このログの中に無理いただけます真ん中にいるんで、これはXPolicyがかかるわけですね。

[0:56:53 - 0:57:16]
この中に入ると、これ確実にメールしたいんで、ずっとで終わります。Zで終わった分だけこっち外に出てくるという流れです。これをそのまま地形変形して げると、ここはKLのになるんで、このπとこのφアスタリスクってのはこうですね。

[0:57:17 - 0:57:50]
この分子の前πSFTます。この項のKLと-6Zってのが出てくるわけですけど、この-6ZってのはこのY2φの出力に依存しないので、無視することができて、結果的にこの二つのKL距離の最初かになります。このKLこのKLの最初かって、この二つが一致してるときが一番最適なので、つまりKLが0になるんで、最適方策はこのπアスタリスクが最適方策になります。

[0:57:50 - 0:58:15]
つまり、この部分ですね、の部分が最適な方策なるってことが証明できます。はい。これ見てもら ばわかるんですが、この最適方策のπアスタリスクってのはこういうふうに表せるという説明したんですが、これよく見ると、これRの形にも関わることができて、つまりこのRを取り出して書いてみるとこのような形にかけます。

[0:58:16 - 0:58:51]
これは対の関係になっていて、つまり報酬関数が決まれば、最適な方策πが決まります。逆にこのπが決まれば、その裏に隠れている報酬関数も自動的に求めることができます。これがこのDPOのタイトルになっていて、Language Models CoPAtari Rewardモデル、つまりこの言語モデルは、目的にRewardモデルも含まれているというのが、一応このタイトルになっています。

[0:58:51 - 0:59:10]
これが結構すごいことです。これ発見したのがそういうことですこれがわかってしま ば、このRというの学習をするだけで、方策も一緒に学習できてしまう。それを学習すれば最適なπが決まるんで、つまりRだけ学習すればいいという流れになります。

[0:59:12 - 0:59:43]
よいしょ。ここちょっと変なサイトが混じってます。一応これ復習のスライドなんですけど、報酬モデルの福祉学習どうやったかというと、こういったような形で学習してましたこれ復習ですね、前のスライドで、この報酬関数の学習のところに、さっき求めたRというのをそのまま代入して げるような形なります。

[0:59:44 - 1:00:08]
つまり、さっきのこのlossのところ、これですね、これ元の式です。これは一応説明しておくと、このワインルールは便が勝つ確率への誘導を最大化しようとして、ぷらっとテリーモデルか仮定するとこのようなlossが出てくるこのlossのこのRのところに、この最適なRというのを大にして げます。

[1:00:08 - 1:00:37]
するとこのような試験出てくるんですが、このデータ6Zというところは、よいしょデータ6Zというところは、これ差分を見てるので、消 消 ます。本来ならこのZというのは計算できないんですが、この目的完遂いて げると綺麗にこのベトナムセットというのは聞いてくれるので、ピースしなくて良くなるというところです。

[1:00:38 - 1:01:23]
そのまま来院してきてこのような式変形ができて、これ何をしているかというと、報酬関数がこれとみなしているのと同じです。だから最初に述べたRっていう述べたログをπSFT分のp θとみなして、報酬モデルを学習しているのと同じです。これが暗黙的に報酬モデルを仮定しているというものです。この目的関数に従って学習すると、自動的に最適なφθが得られるという流れになっていて、非常にすごい発見みたいな形で、2023年のBest Paperにもこの準備を行っています。

[1:01:24 - 1:02:07]
つまり、今まで説明した通り、近似や過程を追加せずに、このRチーフとDPOが等価であることが示されました。つまり、lossRewardとlossRという二つのステップを使って学習しなければならなかったものが、一つの目的関数を強化学習の形で学習することで、RLAIFと全く同じことをすることができるというのが示されていて、非常にRLの細かい不安定さを考慮せずによくなっているので、非常によく使われています。

[1:02:08 - 1:02:26]
はい、PPOの接種とは何かというところなんですが、いろいろなDPOの接種について考察されており、このPPOというものは、DPOをさらに一般化しようという動機で提案されたアルゴリズムになっています。

[1:02:28 - 1:03:04]
これもいきなりこのφというΨというこの数が出てきて、分かりにくいかもしれませんが、このΨというもので、元々の目的関数はほとんど変わりません。しかし、このYルールがYBに勝つ確率というものをこのΨで囲むことで、目的関数がPPOと同じものになっています。つまり、この手法はDPO包含するような支援を行っています。

[1:03:05 - 1:03:56]
さらにこのΨを9つの行動関数にしたものは、家にフィア・オブ・プレファレンス・リプレイ（φPreference Replay）と呼ばれることもあります。KPOというものも存在します。これは、これまでX、Y、W、X、Y、Vという3つのピアノデータが必要だった方法ですが、これは単一のXYピアノから学習できる手法で、プロスペクト理論と呼ばれる人間の声モデル戦略に組み込まれたものです。プロスペクト理論とは何かというと、この左側の効用関数で、横軸は委託を受けたときに、縦軸は紅葉の価値を表しています。

[1:03:56 - 1:04:24]
新しい超るかというもので、つまり5万円を得るとときの高揚感、喜びですね。しかし、喜びという意味は少ないんだけど、5万円を失ったときの喜びや悲しみってのは圧倒的に大きくなっていくというのが、この価値関数という関数をやっていて、だからここが逆向きの椅子みたいなってる感じですね。

[1:04:25 - 1:05:08]
なんでつまりこの失ったファイルがどんどん失った方が、悲しみが大きくなっていく喜びも大きいような感じなっててこれを組み込んだものがこのKPOになってます。詳しくは解決しないんですがlossはこんな感じになっていて、単一のペアから学習するんで るPreferenceというのが三つのペアじゃないんで、どうやってこの報酬の竿というのを学習するかというと る基準値からの差分を使って、どれだけそのだから失うのと、いるのみたいな。

[1:05:08 - 1:05:25]
なんていうんでしょう。説明になると思うんですがどれだけ報酬がこの失ったか出たかというところで、このKPOのプロスペクト理論を使って、うまいこと向けBotモデルを学習していくという、そういった流れになります。

[1:05:27 - 1:05:59]
DPOの発生資料結構いろいろ出てるんですが結局どれがいいのかというところで、一応おさらいすると、マルチFDPOIPOIPOKPOというのをLIMAで説明したんですが、それはどういった分類になってるかというと、データセットと、 と報酬関数に買って関する過程で一応開けることができて、RRHFからIPOまではPreferenceデータ使ってるんですが、KPOってのは、このアンペアと毎日データを使ってます。

[1:06:00 - 1:06:25]
報酬関数に関する家庭だと、RJSDPPOってのは元々のブラッドリーモデルを使っていて、IPOってのはそういうの一般化でIPOってのはIPOの特殊なケースですね。使っていて、KPOってのが、これはプロスペクト理論を使ったモデルなんですが、このモデルを使ってる、そういった報酬関数以下する過程がこの各章で違うというのが担ってます。

[1:06:28 - 1:07:16]
結局どれを使えばいいのかってところなんですが、これはDPOIPOKPOCPOについてちょっと今説明したいんですが、これはネガティブデータだけを使って学習するような手法になるんですが、これが結果になっていて、SFTをした後にこのDPOとか各種を適用していく手法だと、一番DPOが高いんですが、最初のこのSFTという段階を除いてそのまま適用させるってこともできます。そうした場合は、DPOというのは当たり前しかもよくなくて、ただこのKPOとかCPOと呼ばれる方法は、SFTなしでも一定程度の性能を出すことができるので、学習コストを抑えたい場合はこのKPOとかCPOを使うのが良いでしょう。

[1:07:16 - 1:07:53]
ただ一番汗の出るのは、SFTを行った後に、さらにDPO数を増やすことが精度の向上につながるとの結果になっています。気になるのは、DPOがSony DPOの接触の中でも、学習コストをかければ最も良くなる可能性があるということですが、DPOとQ学習SPPOのどちらが性能的に優れているのかという点です。数式的には先ほど説明しましたが、実際に性能的に見ると、DPOはPPOよりも悪い結果が出ています。

[1:07:58 - 1:08:23]
この表がその結果になってるんですが、ベージュを見ればよくて、PPOとDPOだとPPOは様々なデザートで精度が高くなっていることがわかります。結構いい勝負はするんですが、なかなかこの最後のひと押しがPPOには勝てないというのが、いろんな論文で報告されています。

[1:08:24 - 1:08:53]
順番としては、SFTが一番悪くて、その次にDPOしたモデルその次にTDPOって言われるDPOが何回か複数回提供させる手法も って損傷が本当のDPOが良くなる とフィルターのDPOも って、これ何かというと、データのフィルタリングして げる質の良いデータを使ってDPOして げると結構性能が良くなるんですが、PPOにはやっぱり勝てなくて、こういった順番になってます。

[1:08:54 - 1:09:28]
この右の図がそんなような手法になってますが、最初のSFTモデルがで、AtariDPO使って、さらにデータでDPOも返して げるとまたよくなって、DPOPPOに変 て げると、またさらに良くなる。PPOの中でももっと強いRewardモデルを使って げるとまたさらに良くなるみたいな感じになっていてよく るのはDPO何回かやって最後にPPOみたいな流れが、精度を出すためにはよくやられてるような手法になっています。

[1:09:29 - 1:10:00]
DPOは学習コストが低いんですけれども、それに比べて、トップの性能を出すには、PPOを使う必要があるのが現状の結果です。なぜかというと、PPOを使うことで、まずRewardモデルを学習します。Rewardモデルは、様々な入力データを学習データで与えられなかったような分布も、一応Rewardで推定できるわけですね。

[1:10:00 - 1:10:19]
外装のデータがうまく検知できれば、その外挿データのRewardを使ってさらに学習ができるので、学習を増やすことができることで良くなっているのではないか、という意見もあります。個人的な主観としてもそう思っています。

[1:10:19 - 1:11:08]
これは実際に本当に何で良くなっているのか、数値的に等価なのに、なぜPPOが良いのかはまだわかっていません。先ほどRRHFリバースケールを使用すると言いましたが、リバースケールのときの問題点も、最適報酬が、先ほどDPOの説明でこういうふうに書けると言いましたが、これは結構すごいことをしているんです。なぜかというと、表のモデルをこのEXPO電車に変えているというか、モデルを結構分布を尖らせるような形で分布を変えているんです。こうすることで、多様性が結構損なわれてしまうというのが、いろんな報告でも見られます。

[1:11:09 - 1:11:31]
この右の所というのはRSFTで、各Z(x)というのは、多様性が高ければ高いほど多様性が るというふうなメトリクスになってるんですが、これを見ると、明らかにこのRRHFが多様性が減ってもこの最後はちょっと同じぐらいですけど、明らかに減ってるってのが見られます。

[1:11:33 - 1:12:30]
では、この左もそうですね。DiversityのやつもBardKLとRewardKPORKLでReward傾斜ですね、リバース系ですね、リバースケールで多様性が下がってしまう現象が見られます。これを解決するために、リバース系ではなくフォワード系ファンドの経路を使うなど、Xポテンシャルが悪いため、このXプレーンシャルというのが、どこから来たかというと、KL-divergenceのところで、無理矢理ログの中にこの日Rというのを入れて言いたいことを言ったと思うんですが、そのときにXPolicyが出てきてしまい、ここをなくすためにKL-divergenceではなくFダイバージェンスというスペシャルログじゃないやつを使うんです。

[1:12:30 - 1:13:01]
6じゃない不具合ダイバージェンスを使ってる研究も って、こういった研究だと、多様性というのは確保されるんですがただ、Alignmentの性能は落ちてしまいますこの表でも る通り、リバースKLってのが一番Alignmentの明義は高いんですが、多様性が下がってしまっているという、他のφRKLとAlignmentはかなり握手下がってしまうんですが多様さが るってここがなんかTrade-offになっていて、ここがうまく両立できる手法というのはおそらくまだ考 ていない段階だと思います。

[1:13:02 - 1:14:15]
何でもいろいろ研究がやられているという形です。その他の手法について、DPOの派生手法についていろいろとあります。例えば、気晴らしに対IterativeなDPOだと、Self-Rewardingという所が、このRewardを一番最後に与えるということをやっていたと思いますが、それを選定することではなく、トークレベルに与えるような手法もあります。とは、これはもうSFTの段階でPreferenceを考慮しながらSFTをしてしまうという方法です。ORPO出力忘れちゃいましたけど、打つ日を使って、それを使ってSFTをする方法というORPOって症状があります。とは、BPOするときにも、ReferenceモデルとBotのモデル、二つのモデルを読み込まないといけないんですが、そのリファレンスモデルというのをSFTもですね、消してしまおうというのがこのPreferenceFreeモデルで、SimPLePreferenceOptimizationって言われるんですが、こういった手法も結構精度が出て、メモリ効率も良くなるので、かなり良いショーですね。

[1:14:15 - 1:14:34]
とはネガティブPreferenceだけを使うPrevalenceOptimizationAtariAtariCPOってましょう とはLUSHラーニングというのが ってこれは普通のピアノ専攻だったら、SIMPOだけを考 たらいいんですけど三つの専攻と考 るとそれが何か3足みたいになってしまう。

[1:14:34 - 1:14:48]
ようなラベルも存在していて、それのナッシュ均衡を取るような形で学習していくというのが、このSPPOReplayPPOPreferenceOptimizationだったり、ダイレクトな種Optimizationで呼ばれる手法になります。

[1:14:49 - 1:15:10]
いろいろな立派な手法があるんですが、ちょっと全部説明しきれないんですが、大体の仕様は今、ざっくり示したかなと思います。この赤い部分について説明してください。はい。その他のAlignment手法はどのようなものがあるのかということです。

[1:15:12 - 1:15:28]
最初にRAFTすいませんごめんなさいちょっと一旦休憩を挟みつつ、質疑対応していただくこと可能ですかちょっとわかりました。一旦休憩挟みますか。そうですね5分ぐらい取っていただけると良いかなと思います。わかりました。

[1:15:28 - 1:15:49]
一旦5分休憩をとります。その間ちょっと質問とか行っていきますね。はい、よいしょ。はい一旦5分休憩で20分から始めさせていただきたいと思います。はい。皆さんちょっと各自はい休憩とっていただきつつ、もしはい聞ける方は質疑応答の方を聞いていただければと思います。

[1:15:49 - 1:16:33]
ちょっと高城さん画面切り替 ていただくことが可能ですか。はいわかりました。はい。上は見 てますかね。そうそうそうここのFeedbackのところをこれは選択してうん。その状態で大丈夫です。範囲内でごめんなさいFeedbackはごめんなさい選択してくださいとうんわかりました優先度を入れる。

[1:16:36 - 1:16:50]
はい。はい、 りがとうございます。はい。いくつか出てきてるのでここからいくつかはいピックアップして皆さんで共有すると良さそうなやつを回答いただけるといいかと思います。わかりました。これはばっと入って解決した方がいいんですかね。

[1:16:51 - 1:17:19]
そうですね一旦そこを気にせずで大丈夫かなと思います。わかりました。とか、そうですね。そこら辺からやっていけますか。これに無限大の人がFeedbackられ来庁な学習管理をさせます。これは無限大のですが駆使された言語モデルとなること るんですか。

[1:17:20 - 1:17:50]
これは無限大のZがちょっと何かわかってないんですが、WebのデータだったらWebの全てのデータを使っているとLengthのと、 とはFeedbackも何か らゆるFeedbackを与 てRKLするというのと何が違うのかというところというふうに認識したんですが一応無限Rセット何かやるんすかそうい ばWebの全体の技術を使ってPre-Trainingをしたとします。

[1:17:51 - 1:18:18]
その分布というのは、あらゆる価値観が入った平均的な多分布になるはずなんですよね。つまり、悪い時も止まるというそういったもので、PPOにすると何だろう、平均的な分布になっていて、その分布のどこを押さえ、どこを尖らせるかというのが、その意味RRHFの役割になっていると思います。

[1:18:18 - 1:18:48]
つまり、そのPretrainとモデルは、暴力的なことも要因として含まれており、そのバイアスのようなことも知ってはいるけれども、知っているけれどもそれを抑えていく、出力する。確率を抑えていくという形ですね。そのようなことを言う確率をどんどん増やして、この確率分布を曲げていく、つまり尖らしていく形がRHFの役割になっているのです。

[1:18:48 - 1:19:25]
ちょっともう書いてって、わかんないんすか、そういった形になって、とはこれとかですかね？弱いやつを監督するということでしょうか？靴の良い絵が強い監督するということでしょうか？解釈としては合ってますね。ウィークとストロングジェネレーションということで、例えば7Billionのモデルとcolorとかが70ビデオのモデルを、うまい具合にAlignmentして、げる。

[1:19:25 - 1:19:53]
例えば、二つの出力のうち、どちらがlossが良いのかということは、小さなモデルでは判断できないかもしれないけど、何となく、どっちが良いのかという日分類の問題に落とし込むと、小さなモデルでも程度の良い悪いというのは判断できるじゃないかというのが一応仮定になっていて、そういったモデルを使うと、大きなモデルの分布を上手く変更できる。

[1:19:54 - 1:20:14]
さっき言ったように、またバラして出すことが可能かもしれないという仮説がOpenAIのものとして存在しますね。ただ、これが絶対にできるかどうかはまだわかっていません。概念として存在しているものの、本当に実現できるのかという疑問を抱いている人もたくさんいます。

[1:20:18 - 1:20:39]
と1個ぐらい行きましょうか？Post-Trainingでも人間にとってより良い回答が学習として与 ると思います。が、それだけではLLMが人間にとって良くない回答をしてしまう理由は何でしょうか？Post-Trainingって言ってるのはSFTという話ですかね。

[1:20:40 - 1:22:03]
一応R時半にPost-Trainingに入るんですが、ていうかToxicityのSFTだけというふうに解釈をするんですが、SFPだけだと確率分布を変えるというよりは、何て言うんでしょうね。その文章がよく生成できるように、その部分だけを尖らしているようなイメージ、何ていうんですかね。そのデータだけを尖らしているみたいな形で。例えば、Alignmentってふわっとしたもので、Biasを言わないとか、Biasなることを例えば窃盗のことだけ言わないとかだったらSFTでも十分できると思うんですけど、そのBiasの中でもこの抽象的ないろんなBiasが存在する中で、全てのBiasをSFTで隠すのは難しいと思うんです。このRFPという曖昧なものを使って、その中からこのBiasというのは大体こういうものだよみたいなところを、分布を飛ばせることで、それを言わないように学習していくみたいな、そういったイメージですかね。

[1:22:03 - 1:22:34]
つつ ってかわかんないですか。なんでSFTだけだと完全にこのAlignmentするのは難しいってのはそういうことです。はい。ちょっと時間が過ぎてしまったんで、一旦説明に戻りたいと思います。尚、はい。続きしたいと思います。

[1:22:36 - 1:23:02]
その他のAlignment手法について、DPO以外の他のRLFree以外のAlignmentについても触れていこうと思います。はい最初にこのRAFTというもので、これについて説明します。これは先ほど言いましたフィルターのDPOに若干近いんですけど、データをフィルタリングすることによるAlignmentを行うものです。

[1:23:03 - 1:23:23]
これは報酬モデル学習した後に、報酬モデルのトップ上位K%というのをKLの100%ですね。なんでKが後だったら20%ですねファインチューニングすることによって、Alignmentしていくという流れになってます。

[1:23:25 - 1:23:48]
これはPPOを使用することなく、いつも同等の性能になっていて、フィルタリングするだけで、 る一定程度の効果は出るという形ですかね。でもPPOとかには完全に勝ててるわけじゃないのでそこが難しいところですが る程度の当てるという形です。

[1:23:51 - 1:24:26]
RLCDと呼ばれるAIフィードバックを使った仕組みになりますが、これは、特定のプロンプトに対して、Harmlessとハム振るという二つのプロンプトを用意します。つまり、Harmlessに行ってくださいという指示を与える一方で、ハム振るというRAFTに対応するものに対しては0のラベルを付けます。これを使ってPreferenceモデルを学習し、PPOを行うという流れになります。

[1:24:27 - 1:24:49]
元々のRLAIFだと二つの回答が ったときにどっちがいいのかというのをAI例 ばGPT-4とかによって、Preferenceをラベル付けて げるみたいなそういった流れなんですが、これはそういったような外部の家を使わずにπRLAIFAIFができるような、そういった手法になってます。

[1:24:50 - 1:25:21]
複数のモデルを使ってランク付けする方法についてですが、これはRQueryが使われる際に、エキスパートのレスポンス（人間のレスポンスなど）とChatGPTのレスポンス、あるいは他のオープンソースのモデルのレスポンスを作成し、それらを何らかのスコアでラベリングします。そのスコアと言語モデルのスコアが一致するように学習していくことが、このRRHFの仕組みです。

[1:25:22 - 1:25:37]
とは知恵部品のサイトというもので、GAEを使ってフィードバックを行うというので、Rankingを付けた後に、なぜこのRankingになったかというのを後付けでフィードバックします。

[1:25:37 - 1:25:54]
つまり、AはBより悪い、こういう観点で悪いBRewardCはこういう観点で、イコールで一番いいのはDですよみたいなのを文章によってFeedbackを与 て げるそれをπにして げるってのがこのショーになっています。

[1:25:55 - 1:26:22]
とは人間社会を使命とするスティーブAlignmentってのが ってこれは何かというと、LLMのこのAgentというのを多数用意して げて一度同士が るブログで ったときに、それに対してどういう回答するのかというのを対話のような形でFeedbackを与 て げるそのFeedback不足されたFeedbackを使って、ファイリングするってのが、このStepAlignmentという手法になったりします。

[1:26:22 - 1:26:39]
ちょっと時間がないので、説明を続けていきますが、これはAlpacaFarmという手法で、評価の仕方についてです。人間の評価と同じような評価をできるだけ行うようにメディックスを作りたいので、その方法がこのAlpacaFarmと呼ばれる手法です。

[1:26:41 - 1:27:05]
これは、様々なDPOのモデルやオープンソースのモデルを複数使用し、どの出力が良いのかを評価する方法で、人間の数評価と高い相関で一致しているという評価方法を用いて、Alpacaevalと呼ばれるものなどが評価として使われたりします。

[1:27:08 - 1:27:26]
こういった発展的に本体別としては、このような形でいろんな軸で分けられて、隙はFeedbackを使う方だったりAIFeedbackを使う方法、マジックだったり とはRLを使う方かRLフリーの手法なのかDPOとか るいは使わない所ですね。

[1:27:27 - 1:27:48]
とは、ランキングでフィードバックするのか、言語でフィードバックするのかという観点から考えると、ランキングでフィードバックする方法は、R1F（R1FはPreferenceを使って魅力的なものにする手法）のようなものになりますが、言語ベースのフィードバックというのは、先ほど説明した、手ぶらで目を引くようなもの、つまりチームヒントサイトのようなものになります。

[1:27:50 - 1:28:22]
RLの課題や対策についてなんですが、ちょっと時間がないのでちょっと説明していくんですが、全体像としてはこのような形で、日々のFeedbackにおける課題とRewardモデルにおける課題、 とはPolicyにおける課題というの三つが存在していて、全体像としてはこのような形になっていて、ちょっといくつか説明していくと、実スライド呼ばれたやつってところでRHFによって訓練されたモデルってのは、誰の意見に反映、誰の意見を特に反映しているのかってところですね。

[1:28:22 - 1:28:55]
先ほどからRRHFによってもトランプを取らせるというのを質問したと思うんですが、このRHFに使うデータってのが結構バイアスがのったデータ、一部の偏ったラベラーによってラベリングされていると、バイアスが尊重されるような結果になって、例えばこれだと低額所得や低学歴に位置するような意見だったんですが、RGFRRHFはその黄色だったり毛質を維持するような意見に変わってしまったという、そのような結果になっています。

[1:28:56 - 1:29:18]
とは監督するのが難しいところで、そもそもクラウドワーカーに対して、ラベリングをお願いしますというふうに説明すると思うんですけど、クラウドワーカーがそのLを使って作業をしないで、鍋を付けてしまうような方がもらう報酬よりもAPIの方が安くなっちゃうんです。

[1:29:18 - 1:29:32]
そうすることに合理性が るんで、そういったことが起こってましたよってのがこの日になっていて、大体クラウドワーカーの33パーから46%がLMを使ったと推定された。ような結果になっていたというところが報告されています。

[1:29:32 - 1:29:47]
とはクオリティですね。このLIMAというのは評価Alignment仮説ってのが りまして、それ何かというとモデル知識と能力ってのはほとんどこのPretrain段階に学習されてるという過程が ります。

[1:29:47 - 1:30:20]
なんで、少数の例でも、高い品質のデータを使って げることで、より良いAlignmentができるというのが一応ここで示されてることになって とはHumanFeedbackにおける課題ですね、FeedbackいろいろRankingとかFeedbackにおけるLanguageFeedbackにおける自然言語でFeedbackする方法だったり、 とは絶対値でFeedbackする方だったり るんですけど、どういったFeedback州で使 るのかというのと、効率さってのはTrade-offになってます。

[1:30:21 - 1:30:38]
Rankingするのは、どっちがいいのかって選ぶだけなので簡単なんですが、効率は非常に悪くなっています。つまり、いっぱいデータを集められないと現行のFeedbackでは質の担保が大変というのが、そもそも人間の認知の限界としてRankingが一番いいんじゃないかという考え方で、一番よく使うのがRankingになっています。

[1:30:39 - 1:31:08]
とは、Rewardモデルにおける課題としては、黒の方が挙げているように、まず一つ目としては、複数の意見がある問題に対して、そもそもRewardModerateSCARの一つの値をつけるわけですが、そのいろんな意見が絶対的にいいみたいなところってのは難しいものです。そういうときにどうスコアをつけるのかってのが非常に難しいというのがここで挙げられている課題になっています。

[1:31:09 - 1:31:26]
とは、 のRewardモデル一番最初に言ったように夜Hackingみたいなのがやっぱり起きてしまうことが多いんで、どういったパラメーターのRewardモデルを使って、どういうPolicyのパラメータとどういうリアルを使 ばいいのかというところが非常に難しい。

[1:31:26 - 1:31:44]
ていうことになっています。過剰適応を起こすHackingってのが起きやすいので、はいこれだと、どこまでスケールさせればより良いRewardで買 るのかというのを実験したものになっています。 とPolicyにおける課題ですね。

[1:31:46 - 1:32:21]
Policyを使用する際に、そのPolicyにも敵対的プロンプトのようなものを用意して、Jailbreakというものが可能な可能性があるということがここで挙げられています。つまり、学習前はなかったからというのが、その主要な原則によって生まれてしまうその穴を突くようにプロンプトを与えるというと、本当はバイアスのようなことを言わないように学習したはずなのに、言ってしまうような、あるいは安全規則や制限を無視するように設定してしまうといった課題が生じる可能性があります。

[1:32:22 - 1:32:45]
とは多様性が失われるという点ですね。確実に特定の分布を学習しているため、元々1RRHFをしていたような位置の部分でも、学習後には特定の結果が強調される分布になってしまうという点が挙げられます。

[1:32:46 - 1:33:28]
例 ばGPT-4だと、①不可と自信を持ってこれをこうだというふうに間違 る場合が多くなるってところが報告されていたりします。 とはRewardモデルとかPolicyに共通する課題ですね。この辺は対策になっていて、ヒューマンFeedbackによる対策ってところで、Feedback形式どうするかというところで単一の一つのスコアだけだと情報量が少ないんで、各センテンスごとにいろんな観点のRewardモデルを作って、Feedbackを与 て げるというのが、このファイングレインでFeedbackという論文になっています。

[1:33:29 - 1:34:03]
とは多様性を確保するにはどうすればいいのかってところで、Rewardモデルをたくさん作って げて、それをくっつけて げる、くっつけて げるというのは思いを足して げるということをして、各観点でAlignmentされたモデルをいい感じにこのパレット待機的なモデル海鳥が一番この全ての観点を考慮して、Alignmentされてるようなモデルになるんですが、こういったモデルがRewardモデルのStepによって、つまり平均がして げることによって確保できるよねというのが、この日になっています。

[1:34:05 - 1:34:28]
とは、RLの不安定さを解消するためにRRHFをマックスまで使うという手法ですね。これは先ほど説明したものになっています。いろいろ発展的な手法を挙げていますが、この辺は個人的な主観なので事実ではありません。なるほど、と感じていただければいいんですが、そもそもなぜRLで性能が上がるのかというところですね。

[1:34:29 - 1:35:25]
そもそもイベントをやりたくてRしているのが前提なんですが、性能が上がっているわけではないということですね。だからこれまでずっと言っているように、人分布データ分布というものを出力に変化させるだけで、新しい知識を得ているわけではありません。元のモデルのこの分布をいい感じに請求しているというのが主なところです。RLが必要なのかという点ですが、基本的に最初に説明しましたが、DPOではほとんど足りることが多く、トップの性能を使うトップの性能のモデルを作るには、PPOを用いないと今の現状ではできないところです。おそらく元々の目的関数自体は、DPOでも歩いても全く同じなので、データも同じ場合は、同じ結果でないと本当はおかしいのです。

[1:35:26 - 1:35:44]
OKです。なんでRではなく、本当はAlignmentできるはずなんですが、今なぜかPPOの方が優位になってるというところで、おそらくここら辺は研究が進んできて、RLが必要なく、PPOと同等、もしくはそれを超すようなモデルが出てくるというふうに個人的には思っています。

[1:35:45 - 1:36:03]
とはSFTとRFですね。さっきの説明でも、質問でも聞いたと思うんですが、SFTは人間からのLanguage Feedbackって解釈することもできて、そうなったらSFTだけで十分じゃないかという疑問が湧きます。先ほどの質問でもちょっと関連するかもしれませんね。

[1:36:04 - 1:36:21]
結論として、僕の考えではSFTで十分だと考えていますが、残りの1%をカバーするには必ず必要になると思っています。つまり、モデル出力制御にはフィードバックが今後も絶対必要になると考えています。

[1:36:21 - 1:36:42]
人間のフィードバックの限界としても、Language Slackはかなり難しいものです。つまり、分散が結構大きくなってしまう。人によってだいぶ差が出てしまいます。ランキングに関しては、大体どっちがいいって言われれば、大体こっちがいいという結果が得られるので、分散が少なくなる。そのため、そっちの方が最適なんじゃないかというところですね。

[1:36:42 - 1:37:26]
と①太RLAIFという未熟なアルゴリズムどっちがいいのかってところで、これも最近いろいろ研究されてますが、人間が介在しないようなフィードバックだと、フィードバック元のモデルの性能を超えることは多分ないんじゃないかなと思っていて、最近だと合成データを使って、もっとモデルをどんどん上げていくみたいなのもあるんですが、アレフの文脈に関しては、もっとモデルの性能情報的に上がっていないんで、こういうことないんじゃないかなって個人的には思ってます。ただ、人間のフィードバックの性能を家で引き上げて、より何でしょうフィードバックのラベルの質を上げていくという手法でのNFというのはついていくんじゃないかと思います。

[1:37:26 - 1:38:01]
プラス、 とは の外物ですね例 ば数学とか、リングの問題とかだと、明確な答 が計算できるので自動でそういった らゆる外部ツールを用いてFeedbackを与 ることによって勝手に行動なんでしょう、自己進化じゃないですけど、自分自身でどんどんどんどん性能が上がってくみたいなことは り得るんじゃないかなと思います とは何かWebデータに自動的にアクセスしてそこのデータをもとに学習していくみたいなそういった景色だとせ自己進化を送るんじゃないかなと思ってます。

[1:38:01 - 1:38:25]
このようにRDCFリーフの炭谷はコンピュータからのフィードバックという名前も付けられており、つまり、様々なソフトウェア上の、あるいはソフトウェアの世界のあらゆる情報を外部データにアクセスできれば、現実世界でもそのような情報を活用してフィードバックを行い、自己進化していくという概念も存在します。

[1:38:26 - 1:38:46]
はい。ちょっと若干長くなってしまったんですが、一応まとめとして、マルエフってのは、Alignmentを適用する手法の一つで、マイニングのフィールド使うものでしたよとAlignmentの基準としては、一時HelpfulとHonestとHarmlessという三つのものが代表的な基準として りますよというところです。

[1:38:47 - 1:39:07]
とChatGPTの詳細とInstructGPTは、このSFTとRewardモデル学習とRLの三つのステップで書きされていて、RRHFには様々な問題が今 るんですがRL必要としないDPOみたいな学習法も次々と提案されてますよってのが全体としてのまとめになります。

[1:39:07 - 1:39:29]
はい。ちょっと長くなってしまって、しかも若干過ぎてしまったんですが以上になります。ここからちょっと編集に入っていきたいんですが、休憩したらいいですかね。そうですねちょっと時間ないので、はいちょっと皆さん演習環境準備がてらに細部5分ぐらい休憩いただければと思います。

[1:39:29 - 1:39:44]
と高橋さん りがとうございました。 と れですよねコミュニティのイベントでまた10月14日14日発表いただく来週月曜か発表いただくのかなと思ってるんですけどそこで何か今日の補足的な部話とかって る る感じなんですかね。

[1:39:45 - 1:40:08]
そうですね。もし求められバスとRHF関係の労務についての発表をしようと思いますので、質問があれば、この講義の質問も受け付けて大丈夫です。わかりました。はいということなので、ぜひご参加いただければと思います。参加リンクはチャットの方に参加リンクを貼っておきますね。

[1:40:10 - 1:40:32]
はい。今、負けましたので、ぜひはいこちらご参加いただければと思います。ございます。はい、 りがとうございましたお疲れ様ですお疲れ様です編集42分ぐらいですねちょっと短いんですが、2分間だけ休憩をさししていただいて、その間に演習環境準備とかしていきます。

[1:40:32 - 1:41:08]
はい、お願いします。そっか演習も高城さんがいるんでしたっけそうです。ごめんなさいちょっと勘違いしました。はいちょっと休憩はい、皆さん休憩取っていただければと思います。自分の方はちょっと演習用意してるんでちなみに れですね17時半前後ぐらいにダウンロードした方、ご自身の環境に移した方、ちょっとそれ以降にちょっとまた更新しているので、お手数なんですが の際いたダウンロードいただければと思います。

[1:41:08 - 1:41:24]
17市何分ぐらいで45分ぐらいでしたっけ、それ以降になります。ベンチマークですね。ですね。それ以降にダウンロードされた方は問題ないです。はい。てことで、お手数ですがよろしくお願いします。 りがとうございます。

[1:41:24 - 1:41:48]
データとしては、inputデータ.Jさんと、 のレクチャーセブンのエクササイズの二つのデータを今回用いる予定でノートブック開けていただくと、このような画面が出ると思うんですが、ここのアップロードで、inputデータ.ジェイソンというのを らかじめ今度行ってもら ると助かります。

[1:41:52 - 1:42:13]
ごめんなさい時間的にちょっと惜しいそうですよね。想定の4時間10分ぐらい押してると思うんでそうですね。一応短い加湿もできる。うん。できるので、はい多分arXivも残すので、もし問題なければちょっと延長する前提で話していただければと思います。

[1:42:13 - 1:42:30]
わかりました。はい、受講者の皆様への連絡として、もし次の予定がある方はすみません。毎度のことながら、本日は欠席していただいて、後でarXivでキャッチアップいただければ助かります。よろしくお願いいたします。

[1:42:33 - 1:43:14]
ありがとうございます。ちょっと急ぎ足で申し訳ありません。ではでは、時間ですね。はい。よろしくお願いします。はい、これからも引き続き演習の方をやっていきたいと思います。ちょっと皆さん、お疲れで申し訳ないんですけれども、今回の演習では今まで説明してきたRRHFのStep1からStepⅢの中で、最初のファインチューニングは省いていて、このStep2の報酬モデル学習とStepⅢのPPOで強化学習を行うというところの実装を行っていきたいと思います。

[1:43:16 - 1:43:51]
ライブラリとしては、①から実装するという方法もありますが、PPOの実装は1からやるとかなり大変で、本質的な部分が大きく異なるため、このTRXというライブラリを使用します。このprxトラブルではPPOとこのエルキュールという二つのアルゴリズムが使用されています。今回、後者の方は無視していただいて構いませんが、ImitationRankingなどで使われるような思考になっています。

[1:43:51 - 1:44:36]
今回はIPOの方を使うので特に気にしなくても大丈夫です。最初にPRLXとライブラリというものをインストールしていきます。はい、これでもう待つだけなので、次の説明に移りますが、今回はさっきインプットデータとシチュエーションというものを読み込んでもらったんですが、まだ読み込んでいない方は心配いりません。このデータというのはどのようなデータなのかというと、プロンプトとアンサー1という二つのデータが構成されています。Answer1がデータですね。

[1:44:36 - 1:45:07]
ワイン札ってかわいいです悪いデータ法になっています。このデータというのを使って報酬モデルを最初に学習していくという流れになります。実際にここはサンプルを表示してるようなんですが、プロンプトに対して朝晩と暗殺って二つが るようなデータになって既存のデータセットを使用する場合は、ここですね。

[1:45:09 - 1:45:37]
今回はサプライズのセットでタスクについてのセットを使用しています。これでもインプットデータは使わない設定になっています。ちょっと勘違いしていましたが、多分使わない設定になっているので、違うかもしれません。

[1:45:37 - 1:46:22]
最初の報酬モデルはIMPALAデータを使用して、その後の強化学習では翻訳タスクを用いているんです。はい。実装の内容になっていくんですが、まず報酬モデルを定義して報酬モデルを学習するという流れを実装しています。このGPTRewardモデルというのが、基本的に報酬モデルを計算するものです。関数としてユニットとフォワードの二つが実装されていて、何をしているのかというと、いろいろやっているのでごちゃごちゃしていて、わかりにくいかもしれませんが、本質的にはRewardlossを計算しています。

[1:46:22 - 1:46:47]
つまり、中寸という選ばれた方のデータのRewardとリジェクトの方のRewardを使って、その差のマッチングモードを取ってるって形ですね。今回の講義でもいろいろ数式説明したと思うんですが、ここでこの損失関数を用いて、Rewardモデルを学習しているとそういった流れになります。

[1:46:47 - 1:47:08]
ちょっとこの詳しい流れはpaddingが存在するかどうかだったり細かい種類なってるんで後からじっくり見ていただきたいなというふうに思います。とり ずこれで報酬モデルというのを定義しています。これ一応上から実行すれば動くやってるので、皆さんそうしていただけたらというふうに思います。

[1:47:09 - 1:47:28]
次にデータセットを定義している段階なんですが、今回POSのデータセットつまり、Xというプロンプトを出すときにワインとワイルズという二つが出てくるようなセットになってるんで、この会津セットを使って学習をしていきます。

[1:47:28 - 1:47:58]
今度セットの定義なんですが、それぞれの今回はchooseリジェクトって二つのデータですね、データが ったときにそれを特例として挙げる。IDに変 て げるってところがここでやってるところです。それに対して、インプットとシューズのインプットとチーズのアテンションリジェクトのインプットリストのアテンションを返すようにデータセットをこれ作っています。

[1:47:58 - 1:48:34]
コンセプトのインテック戦略すると、各それぞれの中でリステッドのIDとテンションます勝手に入るというそういったデータセットの構造になっています。この実行してもらって、はいここは のDセットの間瀬正解率を算出する関数と、 とはこのバッチ化の処理のところのデータになっていて、中ずっとリジェクトのデータを結合しているようなそういったデータになっています。

[1:48:37 - 1:49:04]
ここでモデルセットと報酬モデルとさ確認をすることができて、フジッコ作ります。この制度を実行してもらうと、chooseプロジェクトとかどういうデータが るのかというのが、これ見れます。これでマーティセットのデータセットを用意することができているところになりますね。

[1:49:09 - 1:49:32]
中でアテンションのこのマスクってのはどういうふうに定義されてるかというと、地図のこのインデックスはこんな形で表されていて、この一番最後の50256というのがpadding表していて、データ自体はもうここまでここまでが文章で、とか全部paddingになってるって形です。

[1:49:33 - 1:49:51]
そのpaddingの部分と、0テンションゼロにしていってそういったマスクやってます。リジェクトっての方も一緒ですね。はい。このようなデータを作るためにこの上でいろいろごちゃごちゃやってたという形です。

[1:49:55 - 1:50:35]
でしょうか？ とはバッチ化する処理ですね。ちょっと若干入れてますが、もしちょはい。これでネクタイバッチかされたセットインプットとアテンションマスクと、 とはこのラベルですね。ゼロのエトワール悪い方のすデータなのか、いい方でいたらかという01のラベルが作られたデータがここで出力することが確認できていると思います。

[1:50:35 - 1:50:50]
とはこのモデルRewardモデルというのをこれ定義して げて、実際に出力としてどうなってるのかというのを見ることができます。これ元々のGPT-2のPretrainのモデルを使って学習をしています。

[1:50:52 - 1:51:20]
ここはモデルのダウンロード段階です。このGPTRewardモデルが何してるかというと、魚はちょっと戻るんですが、ここはGPTRモデルの提言ところなんですが、本当のGPT-2のモデルをロードして、ここで最後の日に出力だけまりnewsを押すcolorのAtariに変更するようにBiasをこの変更してます。

[1:51:20 - 1:51:51]
これをVヘッドと呼ぶことで、一番最後の出力だけをスクラッチにして、Rewardを出力するようなモデルに変えています。とはものすごくさせるというワードが追加されているだけです。このモデルを使うことで、出力と損失が入るといったものになります。

[1:51:54 - 1:52:11]
ここで見てもらうとわかりますが、データを入力すると、lossとchooseの方のスコアとリジェクトの方のスコアが、こういった形で出る形になっています。これのバッチ化をすると、今回の位置に三、四、5個を出力しているというようなものになっています。

[1:52:15 - 1:52:39]
実際にこのRewardモデルというのを学習していくんですが、ひょここでとり ずセットとバリエーションとセットというのを定義して、と、Rewardモデルの最初の70パーの層を凍結して、最後の30パーだけを学習するようにしてます。

[1:52:42 - 1:53:03]
僕は の学習効率化のためですね。実際にこのトレーニング面と、というのを使って、プラスFormatのトレーナーってのが るんですけど、それを使うことによってコンフィグを設定するだけで簡単にこのRewardモデルというのを学習することができます。

[1:53:04 - 1:53:30]
今これを実行してもらうと、実際に50エポックの学習が始まります。ここではRewardモデルを学習しているものになります。実際にここから出てくるステップが増えることにより、高くなっていくのがわかると思います。

[1:53:30 - 1:53:56]
これはちょっとTforなので、若干学習が遅いんですが、もしこのProとか課金されている方は、これをシェアしていただけると、まっすぐ学習が進むと思います。今日は時間がないので、最後まで待つ暇がないかもしれませんが、一旦この秋で仕上がっていくところを確認していただけたらなと思います。

[1:54:05 - 1:54:30]
こんな感じですね、若干曲がっていないですが、lossを下げ、lossも上がっていますね。もう少し多めに増やしてみるのと、データセットが今少ないため、うまくいっていないかもしれませんが、量を増やして学習していくと、Rewardモデルもどんどん良いものが学習されていくという流れです。

[1:54:30 - 1:55:08]
流れです。今回は、ちょっとDPOの関係で、この流れだけを体感していく行動の流れだけを体感していただいて、そもそもRL（リインフォースメントラーニング）は何をしているのかというのをざっくり理解してもらうのが目的になっています。ちょっとこれ時間がかかるので、委託を飛ばしますが、この上の段階で報酬モデルというのが実際に学習できたらそれをフリーズさせて、TRXというライブラリで実際にPPO（プロポーショナル・デトル・オプティマイゼーション）を行うところが下の行動になっています。

[1:55:09 - 1:56:10]
ここの行動で、先ほど学習したRewardモデルというものを読み込んでいます。実行していくか。これも必要な数を定義しているので、大体は関数のようです。詳細は中身を見ていただければと思いますが、このスコアだとサンプルをRewardモデルに入れて、そのRewardモデルの結果のスコアを根拠として出すような関数になっています。今回ようやくタスクでやっているので、TDRという文言を追加して、独立してデータ設定を行っているものをやっています。

[1:56:10 - 1:56:47]
とはRewardFunctionはサンプルを入力したときに、Rewardの前でしまして、成果したスコアってのが取得するように必要な関数になっています。よいしょその後にいろいろはいぱらが るわけですが、いろんなハイパワーを設定して報酬モデルも学習するという流れになってます。

[1:56:50 - 1:57:09]
報酬もじゃないっすねこれ方策のモデルですね。方策のモデルを学習する流れになっています。OpenAIのサプライズDeNARという予約タスクのデータセットを使って、今回は報酬モデルじゃなくて方策もですね、方策モデルを学習するという流れになっています。

[1:57:14 - 1:57:57]
まだ終わらないですね。ここをよく見ていただければわかる通り、PPOのコンフィグをたくさん設定しているんですが、スケジュールのコンパクト化を進めているので、この日も進んでいますし、お祭りのような感じです。元々今週もそうなんですが、PPOだと、ここら辺のコンフィグ1つが初見だと全くわからないかもしれませんが、このクリップする練習やそういった行為がたくさんあるので、どのコンフィグが最適なハイパーパラメータなのかを探索するのが、非常に重要だと考えます。

[1:57:59 - 1:58:53]
実際にここのコンフィグを設定して、TRXのTRAINという感想を読んで、実際に報酬モデルを使って方策の学習というのをPPOで行うことができるんです。時間がないので、この学習した後の成績結果だけを確認してみるんですが、元々のノートブックは学習後のチェックした結果を記録しているので、このような形で1dBで結果を見るような形にしています。実際にここにもプロンプトとアウトプットの結果、そしてRewardのスコアが見られるような形になっています。これ全部上から実行していくと、これが出力される形になっているんです。

[1:58:54 - 1:59:27]
実際にこの日のプロンプトの最後にDeNARをつけて要約するというものをやっているんですが、一番報酬が低いのが、何か高いのか高いのがこれになってますね。こういうこれはメールですかね。メールの文に対して、これがようやく結果になっていても、これが本当に若干正しいのかというのは、まだ疑問かもしれませんが、これの出力に対して高い報酬が付いているってのが、実際見てわかると思います。

[1:59:27 - 1:59:52]
今回は の学習リソースの関係も って、Epoch数が少なかったりlossが少ないので500円しか使ってないので、 んまりちゃんと学習はできないんですがこういったイメージとしては、この書くことに対してアウトプットができて、それに対するRewardを、高い音がいるように学習していくというそういった流れになっています。

[1:59:56 - 2:00:26]
はい。元の方も元の方はやっとRewardモデルの学習が終わりそうな段階ですね。Rewardモデル学習が ると とは、この辺はすぐ多分実行できて、最後のRLのところまでスムーズに実行されていくと思います。

[2:00:31 - 2:01:03]
はい。一応どういうふうな結果だったのかと、 とはDBの結果とかも見ていただいて、やっぱりRL難しいなというところを体感していただけたらなというふうに思います。はい。若干 の行動の解説は、流しで細かいところは外出できなかったんですが、一旦9時になったので、ここで退出される方は、一旦講義を終了したいと思います。

[2:01:08 - 2:01:26]
はい。ということで演習は一旦このような形です。 りがとうございました。はい りがとうございます。これ延長しなくて大丈夫です。一旦これでちょっと、はいとどうします。一旦ここの最終的な結果出るところまで一旦やってみますね。

[2:01:26 - 2:02:03]
うんはいわかりましたはい、お付き合いできる方はそこまでいきたいというふうに思います。ちょっと質問を答 ましょうか？では、薬処はいっぱい りますね。こっちの実行がちょっと終わらないんで若干引き続き残っていただいてる方向けに、質問の回答をしていきたいなというふうに思います。

[2:02:05 - 2:02:40]
はい。これですかね。なぜPPOを用いて、ランク付けした文書の良し悪しから次の登録を予測する確率分布を変更することができるんですか。これはいい質問ですね。なんでかというと、言語モデルにおける強化学習というのは何してるかというと、 るプロンプトが得られたときに、次のトークンを出力するってのが一つのActionに当たるわけですね。

[2:02:41 - 2:03:10]
つまりトークンを複数取得していくのは、Actionを何回も探索していく。流れになっていて、一番この文章を生成しきった段階で、Actionの決議まで積み重なってると思うんですけど、それが る意味文章を採用していてその文書に対して、 の出力だったのか悪い出力出力だったのかという01がRewardとして与 るわけです。

[2:03:10 - 2:03:50]
なので、最終的な出力としては、Rewardモデルは使うと、その文章の絶対的なスコアというのがRewardモデル変 られるので、それを用いて各ActionActionというのはトークンですね。次のトークン不足する、こういう方向にトークンの系列を生成していけば、最終的に高いスコアを得るってのがPPOの強化学習によって学習できるという、そういったわけですそれが間接的にさ 元のモデルの分布というのをとがらせるような役割をしている。

[2:03:51 - 2:04:17]
先ほどの最適方策のXポテンシャルのところの子からわかるように、元の分布をエクスプレーンシャルの方で強化してるようなそういった学習になっている。わけです。はい。。 とは、これとかですかね。これさっき答 ましたね。

[2:04:21 - 2:05:18]
結構解決も多いです。これはわかりましたRLのプロセスで各モデルの回答の振る舞いに差が出ると考 ているでしょうか？学者の価値観やIMPALAとかHarmlessというかそうですね。これは正しいくて基本的にAlignmentの基準はさっき示した通り三つの位置でクリアしてるってのは前提の上でそれぞれモデルクローズがジムニーがどういったデータを用いて主しているか、もしくはDPOしてるかってのは公開されてないんでそのどういったLabeler、もしくはそうですねどういったLabelerによって作られたデータかによってどういう価値観を表現されてるのかというのはそれぞれ別ですね。

[2:05:18 - 2:05:33]
ただ一般的にこれはないといけない価値というのは、どのモデルでも多分きちっとされていると思うんで、そこの細かい違いってのはそのデータによって変わってくる、もしくはLabelerって変わってくるってそういった名が形だと思います。

[2:05:35 - 2:05:56]
はい。これ れですね。1DのPPO入れてないとエラーが出るんで、多分この1DのAPI機、皆さん持ってる方は のご入力画面が出てくると思うんで、それを入力していただけると学習が進むかと思に思います。

[2:05:56 - 2:07:14]
もしくは、ここですね。ドラッカーイコールの設定を変更すると、一応、1Dアカウントがなくても学習できるようになっています。しかし、ちょっと重すぎて、ランタイムがリセットされてしまいました。皆さんの方ではもう実行できたかもしれませんが、やっぱりPPOの実行が重くて、メモリが落ちてしまうので、今ちょっとRankingしちゃいました。一応、皆さんの方では上から順次実行していくように設定されているので、結果として見られるのかと思いますが、自分の方ではまた1からRewardまでの学習からやり直さなきゃいけないんです。ちょっと今回の時間内に間に合うかわかんないんですが、10分以上超えるとちょっとまずい感じですよね、おそらく。

[2:07:16 - 2:07:44]
すいません。今んところ特に10分以上とか指定はしてなくてですね。はい本当ですか。はい可能な限り質問答 てという感じで大丈夫ですかねそれではい、そんな感じでも大丈夫そうで れば、はい、お願いします皆さん の適宜用が る方抜けていただいて構わないんですが、自分の方は一旦この最後の出力が出るまで、 の質問答 ていきたいなというふうに思います。

[2:07:44 - 2:08:20]
なぜ今質問してくれたら、自分でも答えるので録っていただいて大丈夫です。ビジネスコードの生成を行った場合に、そのコードの評価方法として具体的な方法を教えていただけますか。ですね。コード生成の評価のバリエーションとして、よく使われているのは大きく二つあります。一つはマヒワevalで、これはOpenAIが公開しています。

[2:08:21 - 2:08:53]
確かオープンだと思います。ML（機械学習）と、MBP（行動の生成タスク）に関してですが、そのタスクではテストコードが用意されており、テストケースのようなものを書くと思います。ACERプログラムを実施する際に、テストコードがそのテスト行動の正解に合うようにフィードバックを提供するという形です。

[2:08:53 - 2:09:23]
つまり、そのテストケースを通したら、1つ通らなかったらゼロのような形ですね。力を入れて、collegeするみたいなそういったものもありますね。行動の場合、絶対評価が行われるのでしょうか。そもそもそのプローブが実行できたかどうかという評価も、実行できた上で、ちゃんとテストケースが通ったかどうかという二つの軸が評価として重要だと思われます。

[2:09:24 - 2:10:36]
はい。 とは、ここら辺を解決したらいいんですかね。 とはAlignment税が発生する原因について教 てくださいってところですね。ちょっとこれ資料を見ながらいいか待ってください。AlignmentTaxというのは、評価1個になるんですが、Pretrainのデータから、そのアップデートしてるわけなんで、完全に分布だけを変更してるかって言われると、その元のPretrain殿実を更新してるわけなんで、もちろん何かしらの知識がそこに る可能性というのは全然 るわけですね。

[2:10:37 - 2:11:10]
このAlignmentXを得ない場合というのは、その事前知識のTRAINの分布というのを、 る程度維持するということをしたくて、それをしないと、一見別な方向の分布に偏ってしまう。というのが起こり たり、 とはパラメータプレートによって知識が回せてしまうというのが るんで普通にパラメータ更新してる限りこのAlignmentでというのは起きる可能性が る。

[2:11:11 - 2:11:37]
ここでのポイントです。なぜこのReplayという部分を使って、Alignment勢をできるだけ少なくするように学習しているのかという、そのような流れになります。つまり、パラメータを最小限に抑える限り、忘却が起こってしまうというのが答えになります。

[2:11:43 - 2:12:36]
ここまできてます。はい。 とは、質問 りますかね。ちょっと検索でしょうか？ミドルとかもできますか。以上です。言いますね。はい失礼します。RRHFやDPOはモデルとの総合学習更新することが多いですか。そうですね。

[2:12:36 - 2:13:17]
GIFの場合は報酬モデルと方策モデルの学習がわかれているので、それぞれ違うんですが方策モデルは、報酬モデルは先ほどの行動の例でも、最初の70パーとか80%は固定して学習する。って言った例が多いですね。つまり最終的な出力値color値の図値になるので、最終層付近と最後のヘッドだけを更新するという流れで、普通の方策のモデルは全部学習することが多いですねDPOも一緒だと思います。

[2:13:17 - 2:13:36]
ただDPOの場合、多分ローラーを使って学習するというのも、DPOのラップやHackingSのモデルでも見かけます。学習効率を高めるためにDPOローラーを使って学習することも多いです。

[2:13:37 - 2:13:59]
PROってのは要約するということです。グローブとかでもarXivとかもEADRH5の薬とか、予約分というものを想定しています。結局このRチーフの流れる部分は、人間に対する回答文をうまく変換するような出力ができる海斗の確立できればいいのではないかと思います。

[2:13:59 - 2:14:29]
大体合ってます。てかほとんどその通りだと思います。RJSDPPOChartsACERRチーフはPPOを使って学習してるんでRジェフとDPOって比較なると思うんですが、DPOのが圧倒的に少ないし、メモリ容量も少なくて済みますとかRHFって れ使ってるんで、Actor-Criticって所を最初に説明したと思うんですけど、ActorとCriticでもさらにこのモデルを分けないといけない。

[2:14:29 - 2:14:54]
ですねそうするとすごい のメモリは苦しい、 の学習も重いし、時間もかかるというので んまりというか絶対やりたくない。地方ですね。しかも、実際これも時間がかかってるので、 とはSFTR違うしたときにlossのスパイクを発することは りますね。

[2:14:54 - 2:15:11]
Pretrainでもlossのスパイクが発生して、なんていう学習としてやり直したり、 のlossのスパイクを抑 るためにいろんな工夫したりとかすると思うんですけどレジでもしますね。なんで工夫は必要だと思います。

[2:15:12 - 2:15:40]
DPODPOの方が優れているかどうかはわかりません。各ニュースサイトではPPOの方が精度が高い理由は、ゲームなどを見せている認識でよろしいですか。そうですね。DPOの方がすごいという理由に関しては、先ほども高下の中でも一つ説明しましたが、DPOだと強者学習なので、ストロー教師データ以外の情報を使いません。

[2:15:40 - 2:15:59]
それはPPOも同じように思うんですけど、PPOはRL段階において探索が含まれているんですね。つまり、元の学習では、入力Xのプロンプトも探索され、それに対応するRewardも出ているんですね。

[2:16:00 - 2:16:33]
なんで、そこのデータの違いというか、その道のプロ人に対してRewardモデルは元の学生体内出力に対してReward計算しないといけないんですが、そこのデータがいい感じにそのRewardモデルがシノブ近似していると、その外装のデータに外装の出力をにアクセスできるので、そのデータを使って、方策モデルを学習することで、PPOよりもDPOよりも良くなるんじゃないか。

[2:16:33 - 2:16:59]
ていうふうに自分は考 てますね。そうやって主張してる人もいます。ただ、元々考 てみると、そのデータセットは一緒なので、ぺPPOもDPOも同じ目的関数で数学的に本岡なわけなので、本来的には一瞬ないとおかしいはずなんですね。

[2:17:00 - 2:17:14]
ただそのPPOが良くなるってのは先ほど説明した理由も りますし、他にも多分おそらくいろいろ るかもしれないんですが、確定的にこれだから悪くなるという理由は解明されてないという認識で合ってると思います。

[2:17:18 - 2:18:29]
はい。もうちょっとできそうですね動き始めた と一、二個ぐらい外出して、質問に回答して終わろうかと思に思います。画像生成モデル言語モデルRRHF使ってるんですがこれは結構いい質問ですね。これは結論としてはイエスで、最近Preferenceを使ったreligionとかも って、何やってるかというと、religionのロジックプロセスで自ら画像生成してるわけですけど、その生成した最後の画像を複数用意して げて、最初のノイズのランダム性によっていろんな多様性が る画像って生成できるので、それに対して10とかRankingとかをつけて げて、それを使って、 の言語モデルと同じようにcollegeするってのは実際に研究でも ります言語モデルの場合はActionが次のトークンだったんですけど画像の場合は次のステップの画像、つまりでのミーティングの1Stepですね、ノイズを取り除くて1Stepを一つのActionとしてやってます。

[2:18:30 - 2:18:57]
結論としては、使われていることが多く、性能も優れているという点で、画像だとその傾向と言わないとか、具体的な例を挙げることは難しいですが、スタイルの生成性能、つまりよりプロンプトに忠実な画像生成の結果に、そういった面で性能向上しているという研究はありますね。

[2:19:00 - 2:19:20]
できたかな。若干もうちょっと多分ここら辺はこの時間ちょっと難しいかなと思うんで、一旦ここまで出力できたら皆さん、いいかと思に思います。RAFTとアウトプットと、 とRewardの結果が見れたらいいかなと思います。

[2:19:21 - 2:19:46]
これと一番高いのこれですね。ここのRewardが一番高いんじゃないかなと思いますが、良くなっているかどうかはわかりません。BPOは難しいので、50エポック近くまでやってみようかと思います。

[2:19:49 - 2:20:27]
はい。という感じですかね。最後の1つだけの回答をして、今回の講義時間終わりたいと思います。それでは、これに答えますか。例えば、ドラマなどのSSモデルを特定のビジネスやタスクで使う場合、継続的な事前学習を配置することが有効ですが、その際にRLのDPOも一緒に実施した方が良いんでしょうか？それともモデルが硬直して、ときにDPOなどが実施されている場合不要だと考えるものですか。

[2:20:27 - 2:20:55]
これは結構、良いといいですね。元モデルが既にDPOされてる場合、RRHFもさらにすればいいのかという質問と捉 たんですが、結論としてはやった方がいいと思ってAlignment特にDPOだと多段階の最適化でもできるわけですね。

[2:20:55 - 2:21:52]
さっきの最適方策がXポテンシャルで分布を変化させてる。というふうに言ったと思うんですけど、 れって、また例 ば何か る観点でAlignmentして学習しましたその次にまた別の観点でAlignmentして学習しました2段階学習してるわけですけど、これってEXPOね者の方がまた掛け算で掛け合わさってくだけなので、結局スペースを掛け算すると中身が私を去っていって、一番元のモデルにその全てのこのだったでしょ、それぞれAlignment全部混ぜこぜにして げて、学習するのと同じ子を変 るってのは薄々基準でも示されて、つまり、最初にもう出すとごっちゃにしてPPOして げるのと、多段階でそれぞれの観点でそれぞれ別々に学習するのって、数式的には10日になるんですね。

[2:21:52 - 2:22:29]
なんで、既にBPOされたモデルでも、さらに毎日とかDPOしても、全然精度良くなると思います。なんで制度た局面を高めたい場合は、実施した方がいいというふうな回答になります。はい。という感じで、ちょっとだいぶ長くなりましたが、ここまで実行していただけたら皆さん大丈夫なので、今回の講義はこれで終了させていただきたいと思います。かなりちょっと長い長丁場やってしまって申し訳ないんですが皆さん りがとうございました。

[2:22:30 - 2:22:49]
はい、それでは終了したいと思います。はい高階さん りがとうございましたお疲れ様です。お疲れ様です。はい、ではちょっと最後運営の方からいくつかご連絡させてくださいちょっと時間もしてるので手短にご説明できればと思います。

[2:22:50 - 2:23:19]
まずですね、イベントのご案内です。時間が限られているため、詳細は割愛しますが、コミュニティの「もくもく会」についてお知らせします。同コミュニティのメンバー主導による勉強会をそれぞれ開催します。もくもく会は、各自がオンラインで講座の内容を繋ぎながら勉強する、フリーな勉強会です。

[2:23:20 - 2:23:52]
こちらのリンク りましたのでぜひこの講座の振り返りや、次の予習でお使いいただければと思います。もう一つですね、今貼ったのが去年のLLM講座の受講生で って今年人役のプロジェクトで実際LLMを開発した人たちによるLLMの作り方の勉強会というところが、 の企画企画してます今週日曜日なのでぜひ皆さん参加いただければと思います既に450名ぐらいの方々事前応募いただいてるというような状況になります。

[2:23:53 - 2:24:23]
はい。というのがイベントのご案内でございました。次貼ってるのが、コミュニティのですね、 のなんだっけコミュニティLLMコミュニティのポータルページというのを作りましたリンクのところから入っていただいて、ですね入っていただくと様々な情報をこちら集約しておりますのでぜひご活用いただけると りがたいですと思っております。

[2:24:23 - 2:24:39]
はい。最後に、もう一度補足としてSlack上でご連絡させていただければと思っているんですが、チャットボットが少しバージョンアップしましたので、最後に少しだけお話させてください。

[2:24:42 - 2:25:29]
この質問方法についてですね。最近、管理職チャットボットでいつも質問いただいているのは基本的には同じ方法なんですが、少しBotで質問する方法をバージョンアップしました。後期範囲内のところを選んでいただいて質問することで、これまで講義内でしか集め扱ってなかった専門知識以外のものについても取り扱うことができるようになりました。

[2:25:30 - 2:26:01]
言い直します講義内で講師の人が教 たりとかして、講義で扱った内容に対して質問をいただきたいという他いただく場合はこの講義範囲内ですね質問いただければと思います。講義範囲外で質問いただくと例 ば今日の講座とかもまだ情報が入ってない状況なんですが、そういったときとかに通常のちょっとGPTないしはLLMを使った回答というのをできるようにしてますので併せてこちらご活用いただければというふうに思います。

[2:26:02 - 2:26:34]
はい。ここでは、参考文献というものが出てくると思うんですけど、そちらをより深く知りたいという方のために、先ほど高城講師が紹介されたarXiv Interpreterというものを実装しております。クリックしていただくと、Slackのページ、チャンネルに飛ぶと思います。そこで気になる論文のリンクをペタペタ貼っていただくと、ちょっとお見せした方がいいです。

[2:26:37 - 2:27:01]
ようやく説明をしてくれるという感じですね。例えば、ここにこのチャンネルですが、様々なarXivインタプリタがあるので、皆さんこちらに入っていただくと、気になる論文のリンクを貼っていただけると、このスレッドで要約してくれたものを見ることができます。

[2:27:01 - 2:27:33]
これは高城さんが作ってくれた の素晴らしいアプリケーションなのでぜひ皆さんご活用いただければと思います。ので、はい。そちらのご案内になりました。はいということですいませんちょっと漢字も弱くなってしまったんですが以上とさせていただければと思います。ご参加いただき りがとうございます。また次回、来週の19時ですかねまたご参加いただければと思ってますので、よろしくお願いします通常通り出欠アンケートにご回答いただくというのと、宿題も来週の水曜日の17時までですかね。

[2:27:33 - 2:27:40]
にご提出いただくようお願いいたしますでは本日以上とさせていただきますお疲れ様でした りがとうございます。


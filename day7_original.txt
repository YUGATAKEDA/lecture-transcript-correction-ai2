[0:00:01 - 0:00:18]
はい。画面見えてますかね。大丈夫そうですかね。はい、撮ります。はいありがとうございます毎回僕なんか最初喋ってたんですけど、何か始めてくれたんでそのまま行っちゃいましょうって感じで。はい、わかりました。はい、あの講師お願いします皆さんよろしくお願いします。

[0:00:18 - 0:00:41]
はい、どうぞよろしくお願いします。お願いします。はいでは第7回のRRHFアンドAlignmentの会を始めていきたいと思います講師の高城ですよろしくお願いしますよいしょ。まず簡単に自己紹介なんですけれども、今松尾研の博士1年の学生です。

[0:00:41 - 0:01:19]
元々高専ってところにいて、そっから阪大の木曽幸生石黒賢ってところに所属してたんですけどあそこを終了して、今松江にいるっていう流れですInternとかもいろいろ元々光線っていうのもあって、あのengineerのInternとか、あと負けん気Internとかをやっていました専門家としては大規模言語モデルだったり、強化学習だったり、あとは後ろにいたっていうのもあるんでロボティックスとかもやっていましたあと、付けの講義で言うと、DeNA基礎講座とか、あとはSIMPOQ学習とか、あの子と、あとこのあとLLM講義とか、あと世界モデル内の何かの講師も担当してます。

[0:01:20 - 0:01:56]
右の写真はちょっと岸田首相がまた来たときに、あのp θっていうところで、ちょっとちょっとバズったんですけど、一応、キャッチーな載せてあります。はい。あとarXivinterpreterっていうのも自分が釣ってまして、今のLLMコミュニティのうちどこのチャンネルかあれなんですけどあの、入れてまして、あのこの湯arXivのピンクを投げると、こういうふうに答えてくるようなBot、あと図表も木らしくそういったBotを作って入れています。

[0:01:57 - 0:02:26]
なんか、いろいろまだベータ版ってのもあって、不安定なところもあるんで、何か意見とか改善案とかあったらちょっとあとコミットも入れば募集してるんで、よろしくお願いしますっていうところです。はい。ちょっと本来入ってくんですけれども、今回目標としてはRRHFですね、DPOスマートランニングしFeedbackとは何か、あとその仕組みだったり、必要性について講義していきたいなと思います。

[0:02:26 - 0:02:45]
目標としては、そもそもAlignmentって何なんだっけっていうところと、あとはRHF何だっけってところでPyTorchでもRFを実装するってところまでやっていきたいなと思います。内容前提知識としてはこれまでこういう内容があれば十分理解できる内容だと思います。

[0:02:46 - 0:03:48]
はい。ちょっと今回ボリュームが多いんで、ちょっと時間内におるか若干らしいんですが頑張っていきます。一旦1時間半ぐらいでKokiやってなく30分演習にしようかなというふうに思います。はい。始めていきたいと思いますまず、若干ちょっとおさらいなんですが、Mの訓練法におけるファインチューニングっていうのはどういったものがあるかっていうと、まず最初にStep1としてPretrainがあってその辺りはファインチューニングがあると思うんですけれども前回前々回ですかね、の講義で、すごいずファインチューニングの会があったと思うんですけどそれがStep2としてありますStep水としてこの今回の説明するRRHFがあるんですけれども一応講義の定義だと、このSFTとRRHF合わせてファインチューニングとか言われたりします。

[0:03:48 - 0:04:10]
また、またPost-Trainingとかそういうふうに言われたりします。今回はこのπチームの中でもステージStepⅢのこのRReplayっていうところを解説していきます。はい。では一応これでφ部との切り分けですが、強化学習だったり、あとは人間のモデルFeedbackところを次でできる外出していきます。

[0:04:12 - 0:04:33]
はい。では①ふってそもそも何なのかっていうところで、これはざっくり説明するんですけど、ChatGPTだったり、あとその前身のInstructGPTって言われるもので利用されていて最近は基本的にはAlignmentの目的で、いろんなモデルでRチーフってのは使われています。

[0:04:33 - 0:04:49]
Alignmentの詳細についてちょっと後ほどまた説明します。簡単に言うと、LLMで同じ問題に対して複数の答えを出力させて、その後に人間がそのいろんな出力ん中でどれがいいのかっていうのをラベリングしてあげます。

[0:04:50 - 0:05:10]
そのラベリングの結果Preferenceって言われたりもしますけど、その結果をもとに強化学習するっていうそういった流れになります。ちょっとポイントを出します。一緒RRHFの応用例なんですが代表的なものにChatGPTがあります。

[0:05:11 - 0:05:37]
これ元々はGPT-3っていうものをベースとしていて、このRHFっていうのを追加した。することによってこの行動の答えは昨日だったり、意味理解ができるようになっていて、主にこの2022年の11月HP公開されたんですけど、元々のGPT-3との大きな違いっていうのがこのチャット用に主したってところがあります。

[0:05:40 - 0:06:11]
さらに応用例としてLLaMA-2だったりLLaMA-2でももちろん使われていて、最近は3.1とか3.2とか出ていると思うんですけど、こういったモデルっていうのはRRHFを何回も行っています。これ右の図がRチーフした結果どうどのぐらい精度が上がってるのかってのを示してるんですがSFTはここら辺でSFT2回目でRGFを1に三、四個っていうこれLLaMA-2の例なんですけど、LLaMA-2だと5回行ってます。

[0:06:11 - 0:06:49]
こういうふうにIterativeにRRHFをしていて、どんどん性能が上がっていくっていうそういったような形になっています。はい。最初になんでRRHFが必要なのかっていうところを説明していきます。これまでの言語モデル問題点、これはChatGPTとかそ大規模言語モデルより前の古典的なモデルの例になるんですけど、これまでの言語モデルだと人間によっとっては好ましくない発言、設計者が意図しない発言っていうのを行いって炎上した例っていうのが度々ありました。

[0:06:49 - 0:07:08]
Microsoftてとかだったら、この人新しかった人大嫌いみたいな言葉を発してしまって、すぐに炎上してしまっただったり韓国のeaseだっていうサービスだと実施だったり性的少数者に関する差別発言を連発して、サービスを提供停止してますし提出してしまったっていう例があります。

[0:07:12 - 0:07:31]
こういった問題ってインストラクションチューニングで解決できないのかっていうと、非常にそれは難しくて、なぜかっていうと、自然言語のデータを集めるのがコストかかるっていうのもありますし、そもそも何かを言わないようにするっていう正解データを集めるのが、非常に難しいっていうところにあります。

[0:07:32 - 0:07:56]
なんで直接的に人間の意図をマークさせるってのが結構難しいです。これはテキストダ・ヴィンチずにっていうChatGPTの前身のインストラクションチューニングをした後、RGFする前のモデルなんですけど、このモデルで設定を行う方法を教えてくださいっていうふうにプロンプトを与えてあげると、待ち答えてしまってる。

[0:07:56 - 0:08:35]
こういった問題を何とかして解決したいっていうところがあります。RRHFによって、そういった意図っていうのを学習したいんですけれども、それをどうやるのかっていうところですね最初のちらっと説明したんですが、例えば設定を行う方法を教えてくださいっていうプロンプトがあったときに、いくつかのプロンプト例えば良くないですとか、これは設定を行うにはっていうふうに説明してしまってる例だったり、あとはセット犯罪なので、それは強くおすすめしませんみたいな出力例があったときにこれのどれがいいのかっていうRankingをつけてあげる形ですね。

[0:08:36 - 0:09:04]
これよりはこれがいいだろうと、この真ん中の釣りはもうこっちの一番綺麗なものがいいだろうみたいなのをPreferenceとしてつけてあげるこれをモデルにFeedbackしてあげるという形です。実際にこのRRHFした後のモデルtextダメージ03だと、窃盗を行う方法を教えてくださいっていうプロンプトを与えても、こういうふうに窃盗犯罪ですのでっていうふうに教えてくれないならこういったことを学習したいっていう形になります。

[0:09:06 - 0:09:32]
ある意味ヒューマングループ型のアプローチモデルに対してFeedbackあって、それに対するアウトプットでまたFeedbackAtariってこういったループがぐるぐる回るようなアプローチになります。今まで説明したように、こういった人間の意図通りにモデルを学習するっていうのもAlignmentっていうふうに言われていて、このAlignmentを行うためにもRRHFを使っているっていう形です。

[0:09:34 - 0:09:57]
元々OpenAIが言い出したものでAlignmentチームっていうのが存在してます。そもそも人間の意図って今まで行ってきましたが、そういった意図は何かっていうところの分類なんですが他にも明示的な人と暗黙的な人って二つが存在していて、例えば明示的な人っていうのは言語化して伝えてるような人ですね。

[0:09:58 - 0:10:36]
こういった指示に従ってくださいとか、こういうアシスタントして振舞ってくださいってプロンプトしてプロンプトとして伝えないとっていうのは、SFTでも学習しやすいんですが、こういった暗黙的ないとですね、言語化はしてないんですけれども、対話においていわゆる当たり前とされているような、と、例えば捏造するしないとかだったり、有害なこと言わないBiasRBiasがあるようなことを言わないっていうのはいちいちプロンプトには書かないですが、そういったものって標準的に即して欲しいそういったものを組む組み込みたいっていうのがこのAlignmentの人になります。

[0:10:38 - 0:11:01]
その意図ってのはどういう基準があるかってところなんですが代表的なところにこのHelpfulとHonestとHarmlessっていうのがありまして。Helpfulっていうのはできるだけ簡潔で効率的な情報量が多い回答を、まずは質問に対して行うさらに不足情報があったら、適正な質問を投げかけるみたいなところをHelpfulっていう指標で測っています。

[0:11:01 - 0:11:30]
ChatGPTとかでも何か情報がプロンプトの情報が不足しているとこういったことがあると、より詳細に解決できるかもしれませんみたいな返ってくると思うんですけどそういったことですね。あとHonestですねこれはHallucinationしない、つまり嘘をついて捏造したりしないっていうところですHarmlessっていうのは何かこういうふうなことを言ったり、あとは差別的な発言をしない、こういった三つの軸でAlignmentを図れられることが多いです。

[0:11:33 - 0:11:54]
この上の三つを合わしてH1って言われていてこの三つをして来院されたAIって定義してるもんもあるんですけれども、一応Alignmentの糸の基準なんで複数一応考えられても他にも、あの日羽とか、あとはマイナースペックとか、いろんな糸の基準ってのは考案されています。

[0:11:58 - 0:12:16]
今言った三つの基準ですねHelpfulHonestHarmlessでそれぞれどういったものがあるかって具体で行われてるんですが、TruthfulQA旧っていうデータセットを用いてもらったりあとはHonestだったらはるばるっていうものを用いたもんですね。

[0:12:16 - 0:12:40]
クエスチョンに対して正しい答えと、あとHallucinationのアンサーがあるような、そういったデータだったり、あとHarmlessっていうこれ、これはプロクラウスポリスっていうデータセットになるんですけど、こういったアフリカンだったり、白人の文章があったときに、こういったものにBiasが載ってるかどうかを判定するようなそういったデータになっています。

[0:12:42 - 0:13:16]
その他の基準についてなんですけど、NLIが持ってる死リスクについて、3タイプの合計60リスクのタイプに分割して評価を行うっていうものを行った研究もありましてこれがどんとアンサーっていうものなんですけど、行くシステムも大変なんですが、悔いのある仕様だったり、あとは誤った情報による外みたいなそういったリスクっていうのを細かく分析して、それぞれデータセットを作っていって、そういったものもあります。

[0:13:18 - 0:13:50]
こういったふうに人間にFeedbackを与えていくとどんどん賢くなるのかっていうところなんですが、タスクは複雑になりについてそもそもその文章がいいのかどうかっていうのが、人が評価できなくなるってところをOpenAIっていて、例えば論文の評価とかだったら、研究者とかは、その道の専門の研究者とかわかるかもしれないんですが、一般人が見てこの論文とこの論文二つがあってどっちがいいのかみたいなところってなかなか判断するのか難しい。

[0:13:50 - 0:14:19]
ていうところがあると思います。なんで、そういったものだと逃げた体では評価できなくなるんですが、ここですねこの緑の線が人間が評価できる限界だとすると、AIアシスタントの力を借りるとどんどんどんどんこれが、この壁を越えて評価できるものが、より難しいタスクを評価できるようになって、さらにおかしくなっていくんじゃないかってところがロープウェイからこういった概念が出されてます。

[0:14:21 - 0:14:52]
あとはスーパーインテリジェンスSuperalignmentですねSuperalignmentっていうものもあって、今後どんどん賢いAGIのようなものができたときに、そのAIってのが暴走しないように、人間の一存に制御できるのかっていうところで遥かにこの人よりも賢くなってしまったAIがその人間の意図通りにしたからどうすればいいのかっていうのもOpenAIを研究していたんですが、あと今はこのSuperalignmentチームっていうのは解散していて、なくなっちゃってるんですが、一応こういった概念っていうのも提案されていました。

[0:14:54 - 0:15:27]
アナロジーですねこれは、これエクストラ無事アルデシンっていうんですか、これは何かっていうと今までのAIだと、人間が教師として家を学習していくっていう流れなんですがSuperalignmentとこのスチューデントの方が、AIの方がどんどん賢くなってしまっているのでこれをどうAlignmentしていくかなんですがそれを行うために、同じアナロジーとして小さいモデル小さいLanguageモデルを使ってキーランRaceモデルをAlignmentさせる。

[0:15:28 - 0:15:46]
ていうのを行くとストロングジェネレーションっていうもので、ロームとして、パブリッシュされています。これが本当にできるかどうかっていうのはちょっとまだ怪しいところもあるんですが、こういったのを見据えて研究しているところがあります。

[0:15:47 - 0:16:25]
でしょうか？あとはAGIとAlignmentの関係性ってところで、AGIっていうのの実現が大規模言語モデルの登場によって極めて現実的になってきていて、Alignmentの器具は推進されてるってのは、言われていて、ここちょっと怪しいというか、本当にそうなのかって疑問に思う人もいるかもしれないですが、OpenAIとかはAGIがもうAlignmentされてないと、今後人類重大リスクをもたらす可能性があるっていうところで、こういったようなリスクの種類をいくつか分割して、こういったリスクがあるんじゃないかっていうのを真剣に考えている。

[0:16:26 - 0:16:48]
ていう形になります。ここら辺はちょっと意見がわかれと思うんでこういったふうな主張もあるんだなぐらいで聞いてください。はい。この辺でもR1Fのマイクというか、なんでAlignmentしなきゃいけないのっていうところを説明したんですが、この後にも具体的にあるチーフの基礎について入っていきます。

[0:16:51 - 0:17:19]
RRHFの学習は何回も行ってるんですが、この三つのステップで構成されています。ちょっと①の会なんで、ちょっとくどいようになるんですが、ちょっと1個ずつ説明していきます。ええ。まずこのRGF全体像を理解するために、最初の最後のStep1強化学習ってのがあるんで、Q学習ってそもそも何なのかってところからちょっとだけ話そうと思います。

[0:17:21 - 0:17:44]
強化学習っていうのは何かっていうと、こういう環境と何かAgentがあったときに、このAgentっていうのが何か環境の状態に基づいて行動します。この行動の方策のことをPolicyとか言ったりするんですけど、このAgentか何かしら行動すると、環境に何かしらの影響を与えて、環境の状態が変わります。

[0:17:45 - 0:18:12]
それプラス、環境の環境からRewardっていうのを経て、どういう行動をすれば、こういう報酬が得られるんだなっていうところを理解した上で、次の行動を決定していくそういった流れになります。強化学習の目的としては、どういう行動をすれば一番環境からの報酬を最大化できるのかっていう、この行動方策を学習するってのが強化学習の目的になります。

[0:18:13 - 0:18:31]
例えば囲碁の場合だったら、状態っていうのがこの盤面の5-1ですね。行動っていうのは自分がどこに石を置くのか報酬っていうのは、その囲碁に勝つか負けるか。っていうような形でAgent学習することができるわけです。

[0:18:32 - 0:18:45]
これどうやって学習するのかっていうところなんですが、その前にあれですね、強化学習の応用例として、いろいろ自動運転であったり、ロボット制御とか囲碁とか、ChatGPTでもRL使ってますよっていうものですね。

[0:18:48 - 0:19:16]
R使われてるQ学習の手法なんですけどそもそも強化学習の手法はたくさんありまして、ここに載ってるだけでもわかると思うんですがたくさんあります今回使うのがこのPPOっていうものですね。PPOっていうものを使って方策を学習していきますこれはOpenAIが開発したアルゴリズムでデファクトでよく使われてるものになります。

[0:19:19 - 0:19:37]
はい。強化学習のPPOってどういったグッズアルゴリズムなのかっていうのを簡単にこれ開始するんですが、RRHFでもよく使われてる強化学習のアルゴリズムでActor-Criticって呼ばれるアルゴリズムになっています。

[0:19:37 - 0:20:12]
Actor-Criticって何かっていうと、このActorっていうのが、環境からの情報を得て、行動を決定するAgentの一部なんですがある意味方策っていうものなんですが、Criticっていうのは、このActorが下行動と環境から情報を得て、本当にその行動が良かったのかっていう状態の価値っていうのウソクするものをやっていますこのActorと、このCriticっていうのが相互に連携して学習していくことによって、最終的に環境からの得られる報酬を最大化するっていうそういったタイムズになってます。

[0:20:12 - 0:20:38]
ちょっと概念的な説明で申し訳ないんですけど、こういったようなアルゴリズムになってます。これもうちょっと詳細を言ってるんですが、ちょっと時間的にも厳しいかもしれないんで、ざっくり説明するんですが、このPPOではこの方策勾配法っていうのを使って学習しています。

[0:20:38 - 0:20:52]
これは何かっていうと、方策、さっき言ったActorだったり、Criticっていうのは、パラメータを持ったネットワークで表現できるので、それの勾配を直接計算して、最適化を行うっていうのがこの手法になってます。

[0:20:52 - 0:21:33]
目的関数はこんな感じになってるんですが、詳しく知りたい人は、この最後の方にサンコー向けのしてるんで、そこら辺を参照していただけたらなと思います。これもPPOの説明になるんですが、元々PRPPOって言われる方策勾配法の中でも、更新幅をKL距離をっていうそれを掛けて更新するような、モデルがあってそれを簡略したものがPPOになるんですが、ちょっと多分何言ってるかわかんないと思うんで大体こんな感じのアミューズを使ってるんだなっていうぐらいの理解で今一旦大丈夫です。

[0:21:35 - 0:22:15]
はい。Q学習を学ぶための資料として、コーセルだったり、Adobeφだったり、あとはPolicyBackの授業だったりあるんで、こういったものが無料で見れるんで、見るとかなり参考になると思いますあとは強化学習のサマースクールだったり強化学習の講義っていうのを待つを県でもやっているのでそういったものに応募していただいて、1から勉強するっていうのもいいと思いますし、あとはQ学習のこの第2波ですね、これ松尾研で翻訳したものなんですがこれが非常にわかりやすいんでこういったものを見ていただけたらと思います。

[0:22:16 - 0:22:31]
この後の説明では一応このPPOの具体的な詳細っていうのはわからなくても、一応理解できるような構成になってるので、一旦はこういったアドレスがあるんだなっていうところを理解していただければ大丈夫です。はい。

[0:22:35 - 0:22:58]
次に人間のFeedbackを使って同居書きするのかっていうところになるんですが、これはOpenAIが出したRankingfromhimPreferenceシリーズっていう論文が発端になっていて、これは言語モデルじゃなくて、ロボットの学習に人間のFeedbackを使ってるっていうものになります。

[0:22:59 - 0:23:14]
AtariっていうシミュレーターからAtariっていうゲームのですねシミュレーターと、あと0Bot種メーターを使って人間のFeedbackを用いることによってサンプル効率よく学習することができたっていうのがこの部位になってます。

[0:23:14 - 0:23:37]
これもStep1から3までわかれていて、最初に方策が環境で報酬祭だけするように、丸の内の学習するんですが、その出力した行動の中から二つを選択して、その二つの中でどっちがいい行動だったのかっていうの人が目で見て評価する形になります。

[0:23:37 - 0:24:12]
その評価の結果をもとにRewardフィルターを使って学習する。プロジェクターを学習するっていうのがあります。だからつまりこれが報酬モデルの学習に当たるものですね。実際にどういったものをアンケートするかっていうと、よいしょってこんな感じで、これあの、この棒がバク転するのが、バク転を学習させたいんでけど、この左のものと、右のもので、どっちがうまくバク転できてるのかっていうのを、こんなふうにPolicyをしていって、Preferenceをつけるっていうものになっています。

[0:24:13 - 0:24:36]
AIは、この人間の選択っていうのをよく最もよく説明するような報酬関数を見つけて、その報酬関数を使って強化学習するってそういったながらやってます。そういうことをすることによって、より人間が見たときに、よりちゃんとバク転してるものっていうのを学習してきていくっていう形になります。

[0:24:39 - 0:25:11]
このロボットタスクでうまくいったRHFみたいなものなんですけどこれを言語モデルに応用したのがこの論文になってます。これ最初の一番初期ピーマンFeedback使った言語モデルの強化学習で初期のものなんですが、これはGPT-2を学習させるのに火はFeedbackを使っていまして、方法としては、今RRHFに結構似てきてるんですが、言語モデルが四つ出力をします。

[0:25:12 - 0:25:29]
四つ出力した中で一番いいものっていうのを選択してRewardモデルを学習していく。流れになります。PolicyとRewardモデルのトレーニングっていうのを交互に行っていくようなものになっていて、これだと感情分析タスクですね。

[0:25:30 - 0:25:49]
の文章がポジティブなのかネガティブなのかそういったものを判断するのタスクで性能が良くなったっていうところの論文になっています。さらにそれが発展されたものがこの論文になっていて、これは人気のFeedbackをようやくタスクに及ぼしたものになります。

[0:25:49 - 0:26:17]
これももうほぼ、かなり今のRRHFに近いっていうかほぼ同じ枠組みなんですが、予約モデル予約タスクについてのみについて学習を行っていて、ファインチューニングの結果よりも紐ひどくを使った結果が大きく上回っていて、このリファレンス様リースっていうのが人間が作成した要約のスコアになるんですがそれよりも良くなっているというところが挙げられます。

[0:26:19 - 0:26:39]
この成功を見て、最終的にOpenAIが作ったのがこのインパクトGPTっていうもので、これがいわゆるRRHFって言われるものになります。これはChatGPTの前身であるインストGPTで用いられていて、ようやくタスクじゃなくて、既存のこのGPT-3っていうのをAlignmentするのが目的になってます。

[0:26:41 - 0:27:06]
先ほど言ったようなH1ですね。ハーフとかHarmlessとかHelpfulの基準プラス、あとはチャット用にAlignmentするっていうところをやっています。段階としてはもう前半の方でも説明した通り、最初にSFTをして、次に報酬モデルを学習して、最後に協力するっていう流れになってます。

[0:27:07 - 0:27:26]
一応ちょっとくどいようですが紹介詳細を解説します。おっしゃった。まずStep1では、プロンプトモデルセット最初に用意しますそのプロンプトに対する人間のLabelerの回答をもとに教師あり学習っていうのを行います。

[0:27:26 - 0:27:52]
ここはSFTのものですね。これは普通のFeedbackっていうのはデータセットを使ってただ強者で学習してるだけっていう形になります。その後にSFTしたモデルに対して、これと四つですね。ABCDって四つを出力してあげて、その出力の結果っていうのを人間がどれがいいのかなっていうところをランク付けする流れになります。

[0:27:53 - 0:28:26]
その後に報酬モデルっていうのを学習します。このランク付けと同じになるように、報酬モデルを学習したいっていうのが目的になります。その後にあるプロンプトに対して、GPTのモデルの出力をするわけですが、そのも出力したものに対して取得した文書に対してRewardモデル対してRewardモデルを介してこのRewardっていうのが出るわけですが、それをもとにPPO強化学習をするっていう流れになります。

[0:28:26 - 0:28:52]
これをどんどんGoogleGoogle繰り返すっていうのが、のInstructGPTの流れになります。報酬モデルの学習になるんですが一応直感的な理解として、一応図解してあるんですが、元のInstructGPTのモデルだとKが4から9の出力の力、二つの組について全てランク付けを行うっていうのがやってます。

[0:28:53 - 0:29:23]
この下の例だと、傾向4つまり四つの出力に対して、二つの組をそれぞれランク付けしてあげる形になっていて、例えばこのお金持ちにはどうすればいいでしょうみたいなアプリとかあったときに、SFTモデルが四つ回答してあげて、この四つの中の二つのペア、つまり4Cの6通りですね、これ6通り書いてないんですが、6東陵全部ペア作ってあげて、それを人間がどっちがいいのかっていうのをある程度してあげる、そういった流れになります。

[0:29:26 - 0:29:52]
よいしょ実際に報酬モデルの各州どうやってるのかっていうところの具体のところなんですが、報酬モデルで何したいかっていうと、この確率分布のリードされたい顔したいこれ何かっていうと、さっきの人間のアノテーションによってプロンプトXとあとそれに対する回答こっちですね。

[0:29:52 - 0:30:31]
YBこれWってのがYBYLってのがワイルズを指してるんですが、いい回答と悪い回答のこの三つのが得られます。やりたいのは、Yウィンっていうのが、ワイルズよりもわかってる確率っていうものを求めたいこの確率の誘導を最大化することで、これがこの確率を求めるように最適化を行うんですが、この確率はどういうふうに表せるかっていうと、RRHFではブラッドReplayモデルっていうのにこのデータが従うと仮定します。

[0:30:32 - 0:30:55]
これ何かっていうと、このワイルズ弱みが勝ってる確率っていうのをこういうふうにしかし上げます。これ何してるかっていうと、このRっていうのが報酬換水なってるんですが、つまりXとYが与えるときに、何か絶対値のスカラーが取得されるようなそういった関数になってます。

[0:30:56 - 0:31:19]
これ、この報酬関数にYBワイルズの結果を与えてあげて、ワインの確率を分子に取ってるんでソフトマックスをかけてるような形になります確実にしているそういったモデルになってます。これ書き換えてあげるとφGoogleカードを使って、こういうふうに書くことができます。

[0:31:19 - 0:31:41]
なんで、要するにこれを代入してあげると、このログp θのところに、この仕組みの数を使ったものを代入してあげるとこのような形にかけます。つまりはやってることは一緒です。ブラッドも減りモデルに従うこういった確率分布に対して、家を最大化して、そういった流れになります。

[0:31:42 - 0:32:04]
つまりやってることは日群みたいなことで良い回答でr(x,y)ウィンのペアの報酬を、悪い回答でr(x,y)ルースのペアの報酬よりも高くなるように、高くなる確率を学習していくとそういったものになってます。よいしょはい。

[0:32:06 - 0:32:35]
報酬関数はさっきの目的関数で求まったとします。その次にその報酬関数を用いて強化学習をするっていう流れになります。単純に考えると、r(x,y)のピアノピアノ文章があって、それに対する報酬っていうのを最大化するように学習すればいいんじゃないかってこういった目的関数が想像できると思います。

[0:32:35 - 0:32:57]
これっていうのは、牧田優績されてない報酬なんで普通の教育だと、各Stepにおいて累積した報酬はっていうのを採択するにしてると思うんですが、この言語モデルの強化学習では、このようなobjectiveになっていてこれは文脈付きバンディットって言われたりもします。

[0:32:59 - 0:33:19]
これを学習するばいいじゃないかと思うんですが、単純にこのままではうまく学習できないので、工夫が必要になります。先ほどのobjectiveだと何が問題かっていうとこの二つの問題が起こる可能性があって、一つはRewardHackingって言われるもの、二つはAlignmentXって呼ばれるものです。

[0:33:21 - 0:33:53]
一つ目の問題としてはRewardHackingっていうものがあって、これは報酬を最大化することを目的に、どんどん方策を学習していくモデル学習していくわけなんですが、そのときにRewardモデルの穴を突いて、本当は良くない文章なのに、高い報酬が出るような入力っていうのを上手いこと見つけてそれを最大化してしまうみたいな、そういった現象が起きてしまいますこれはなぜ起きるかっていうとRewardモデルが完全に心の報酬関数近似できない。

[0:33:53 - 0:34:18]
ゆえに起きることなんですがそういったような、あるいはハックのことをしてしまうのが問題であります。こういう発行してしまうと何か長い文章をRewardモデルが豊か報酬つけたがるみたいなBiasがちょっとあったりするんですが、そういったBiasを工夫して、どんどん長い文章を、ばっかりを作ってしまうとかそういった問題が起こってします。

[0:34:19 - 0:34:40]
その対策としてKLペナルティっていうのがありまして、これっていうのは生成する文章っていうのが元のSFTのモデルから大きく変わりすぎないように力を追加してあげるっていうものになります。具体的に何してるかっていうと、元々のRに対して、この項を追加してます。

[0:34:41 - 0:35:07]
ベータっていうのは、このKL項っていうのをどのぐらい考慮するかっていうハイパーパラメータになっていて、これは何してるかっていうと、これ普通に分解してあげると-βのこのログの期待値取ってるんで、ここがこのπφのRLとφSFTのKL距離になります。

[0:35:07 - 0:35:26]
つまりこの分布がどれぐらい違うのかっていう距離を測ってるのものになります。これが遠ければ遠いほどペナルティが大きく安いてるんで、どうやって大きく変わってしまうんで、それをできるだけ最小化しつつ、報酬も最大化するっていうそういった目的関数になります。

[0:35:29 - 0:36:01]
先ほどのKLの子を見てもらえば、ちょっと気づく方もいるかもしれないんですが、KL距離っていうのはこういうふうに表されて箱優先ですね、若干違うんですが、こういうふうに合わせて、これを先ほどの書き下すと、このログのπRL分子がπRで、分母がいっぱいSFTなんですがこれがリバースケールでもになります。

[0:36:01 - 0:36:33]
つまり今学習してる学習したいモデルが、この後ろの方にあるのが余りフォワードKLで、この前の方にあるガリバースケールになってます。これは、RGAEの場合だとリバースケールを使うんですが、これがなんでかっていうと、フォワードKLの場合は、この右の図のようにターゲットがあったときに、このターゲット全体をカバーするように学習してしまいます。

[0:36:34 - 0:36:59]
一方で、ガリバースケールだとこの特定のモードを学習カバーするように学習するここはだから無視しても全然ペナルティにならないという形になります。今やりたいのは、元のターゲットから大きく変えるような学習はして欲しくないんで、このリバースケールのものを使います。

[0:36:59 - 0:37:24]
つまり今、最初の一番最初の段階では、元々のモデルと今学習したモデルってのは完全に一致していて、そこからRewardを使って文法を変えていくっていう流れなんで最初は一致してるんすね。この一度できるだけ維持したまま詳細玲香させたいので、このリバースケールの方を使うっていう流れになっています。

[0:37:26 - 0:37:58]
二つ目の問題として、AlignmentTaxっていうものがありましてこれ何かっていうと、人間の意図通りにこのモデルを学習しようとすると繁華制度が劣化してしまうと事前知識の忘却みたいなことが起きるこれを税金Alignmentぜっていうふうに呼んでるんですが、こういった問題が起こりますこういった問題を解決するためにReplayっていう所があって、もうこれは単純な手法で事前学習データの一部を用いて、それの誘導最大化みたいなことをします。

[0:37:59 - 0:38:34]
つまり普通のPretrainのデータを使って、Pretrain目的関数を、先ほどのRRHFのf-divergenceに追加してるしてあげるような流れになります。要するに、こういったことになっていて、全額主人のデータDPretrainを使って、繁華性の維持するんですけれども、やってることはPretrainのデータからXサンプルして、そのXについて、この言語モデル猫の方策は今言語モデルなので言語モデルの確率の尤度最大化を行っていく流れになります。

[0:38:35 - 0:38:58]
この頑張ってるのが、そのReplayはどんだけ考慮するかっていうところで、大きいと博士の維持できるんですけど、今週はあまり考慮してしまうしなくなっちゃうのでよろしくないっていうところですね小さいと報酬よりジュースんですが、何か制度が劣化しやすくなるのでこのパラメータハイパーポールみたいなんですが、こういったものを追加することで、全額主人の顧客を防ぐっていう流れになっています。

[0:39:00 - 0:39:22]
はい。PPO-ptxっていうものが、この先ほどのKLペナルティとReplayの二つをクリアしたものがこのPPO-ptxってなっていて、InstructGPTで使われてる目的達成になります。やってることは先ほどの二つのものを追加しただけの単純な式になっています。

[0:39:24 - 0:40:08]
この目的関数で学習すると、元々のSFTと比べてもさらにPPOのモデルと比べても性能改善が見られたっていう結果が出ています。米StepGPTの最終的な評価なんですけれども、GPT-3と比較してより正しい指示に従って、Hallucinationだったりも抑えられている結果になっていて、さらによくあのGPTとかだと日本語で返し、日本語で売ってるんで英語で返してしまうみたいな問題あったりするんですが、そういった問題もこれを使うことによって抑えられてるっていうそういった結果になっていますつまり英語で打つと英語で返してくれるみたい。

[0:40:08 - 0:40:28]
そういったことですね。公開データセットでの評価っていうところで、Alignmentそもそも何が言いたかったかっていうと、真実性だったり、あとはHarmlessness村一成っていうのを言語モデル実装したいっていうところになるのでそういうところの紹介あります。

[0:40:29 - 0:40:49]
これは何か悪意のある程度セットってか海のあるデータだけはかないかっていうところで、これは小さい方がいいんですけど、MSXGPTが一番使えばいいTruthfulQAは真実性ですね、より正しいこと言ってるかっていう判断ですが、これはSFTと比べてもだいぶ良くなってるって形になります。

[0:40:49 - 0:41:15]
実際て性的な評価ってのも明らかに良くなっているっていうところです。ただ、PPO-ptx使えば、十分に学習できるかってところで実はそんなことなくて、RLとかやってる人はわかるかと思うんですが、基本的にある程度学習が不安定で、さらにこのハイパーパラメータが非硫黄に多いんですね。

[0:41:15 - 0:41:28]
PPOの中でも結構多くて、しかも何か細かい実装のテクニックが必要だったりして、すごい調整しないとうまく性能が出ないっていうのがあります。なので基本的にこのRっていうのはみんなやりたくないっていうところです。

[0:41:30 - 0:41:58]
そこら辺を解決するためにこのPPO-maxっていうものがあって、いろんなこの既存の強化学習の教育としては昔からあるので、それの学習安定化のためのいろんなテクニックがあるんですね。なんか報酬Clippingしたりとか、ペナルティ他のペナルティ追加してみたりみたいなそういったものがあるんですが、こういったものをいろいろ駆使して何とかちょっと学習できるようにしたっていうのがこのPPO-maxっていうものになります。

[0:42:00 - 0:42:23]
詳細は割愛するんですがこんぐらい、あの工夫しないとなかなかうまくいかないんだよってところを知ってほしいっていうところです。あれこれはPPO-maxの結果ですね、一応このPPO-maxを使うことによって、長期的にも安定した結果を実現してSFTモデルよりも、精度が良くなったよってのが右の図になってます。

[0:42:24 - 0:43:31]
はい。そ、そんな感じで都丸チーフってのは学習されていきます。でも一応これ、全体像、復習として一応出してるんですが今話しながらですね強化学習して報酬モデル学習して教育するっていう流れになっています。どういった学習データ使うかっていうところについてちょっと話していきたいと思うんですがFeedbackは先ほどの説明だと、二つのペアがあって、どっちがいいのかっていうのをアノテーションしていくっていうふうに説明したんですが、Feedbackタイプはその他にもいろいろあって例えば絶対値の数値すから値で与えてあげたり、例えば何か10段階評価で5とか、そういうことですねとかあとはRankingってのはペアを評価さんと一緒なんですが、自然言語っていうのはあるプロに対してどこが悪かったのかっていうのを自然言語の形式でFeedback与えるみたいなそういったいろんなFeedbackKCが一応考えられてはいます。

[0:43:35 - 0:44:06]
ただ学習は難しくなるんでどうやって学習するのかっていうところが一応問題になったわけです。よいしょあと学習意欲、使用されるデータセットとしては、HH-RLHF言われるもの、このH1っていうのは、先ほども説明したんですが振ると、テープRaceですね、Helpfulの二つ合わせてHH-RLHFだったりあとはHPっていうスタンフォードNPがリリースしている。

[0:44:07 - 0:44:41]
物があったり、あとはFeedbackとか、ありますHGRチェフでは、こういうのプロンプトに対して、シューズってこれが出力ですね出力とリジェクトサンプル悪い出力の二つが与えられてるって形になります。その他の出せたとしても、いろいろあってhealthcareとか輸入RGFとかUltraFeedbackみたいな詳細はちょっと説明しないんですけどそういったセットもいろいろ提案されています。

[0:44:45 - 0:45:07]
データの集め方ですがこういったデータセットもどうやって集めればいいのかっていうところなんですが、元々のインストラクションソフトGPTの論文だといろいろ工夫されていて、そもそもかったら意味ないので、Labelerの選択っていうところからどうすればいいのかってところが論文に書かれています。

[0:45:09 - 0:45:34]
まず少数のデータにラベル付けを行いますこのWづけこの人本当に間違いが絶対ないような、限られた人で、ラベル付けを行いますそのラベル付けを行った結果をもとに、他の人をスクリーニングして、そのベルトの位置度合いが高いLabelerってのが信頼できるLabelerだとして、スクリーニングを行います。

[0:45:36 - 0:45:57]
そういったLabelerに対しLabelerを選択して、その人に対してクラウドソーシングか何かでどっちのPROのかっていうのを選択してもらってデータを集めるってのがわかります。あとはLabelerの特性が偏らないように、統計データっていうのはアンケートを用いて収集してまで契約Biasが載ったような状態ならないようにするっていうところの工夫も行われてます。

[0:45:59 - 0:46:21]
Labelerのマニュアルみたいなのを作って、WebGUIでこういった形でポチやっていくみたいなそういった集め方にInstructGPTではなっています。RRHFを実装するためのライバルとして、いろいろあって、TRLだったり日RXRLforMLとかReplayチャットみたいなチャット形式のモデル学習できるようなモデルもあったりします。

[0:46:22 - 0:46:43]
それぞれのモデルで何が違うのかっていうと、PROとPPOだけ実装してるんですが、TRXだとKTOも別のメソッドを使えたり、あとはこのRLforMLだと、他の様々な強化学習を使ってたりあと報酬関数だったりいろんなトリックが使えるってところでライブラリの差別化がされています。

[0:46:45 - 0:47:11]
RRHFの評価なんですけど一番発信とRealignmentの基準であるHonestHelpfulnessとHarmlessnessで三つの基準が一般的に使われてるものでHonestSだとTruthfulQAAtariはるばるHelpfulnessだといちいちRHFとか、HarmlessHarmlessnessだとクラスプレスとかあとジェンダーとか、そういったものが使われてます。

[0:47:13 - 0:47:35]
評価に関するデータセットの一応説明なんですがloss9Aは真実を評価するようなベンチマークになっていていろんなカテゴリー38のカテゴリーに分け、または810名の質問と回答を用意してあるって形でこの評価ってのはどうするかっていうと、もうφに記されたBPSDを使って評価を自動化するような、そういった流れになっています。

[0:47:35 - 0:47:53]
はるばるっていうのは、イエスノーであったりあと単語の出力なってるココアの自動評価とかは、GPT-3とか別のモデルを使う必要はないんですが、完全解凍の一律を見て、どれだけHallucinationを着てないのかっていうのを評価するようなものにしています。

[0:47:54 - 0:48:38]
はい。1R1フェア前半でも何度も説明したので、ちょっと飛ばしますが皮膚RaceとHarmlessに関する一つも評価にも使ったりします。あとはクラウスパリスというAgentですね。lossPressは肌の色とか政治とか宗教とか年齢とか、そういった様々なBiasに関する評価でベストになっていて、例えばこの右の図がCrows-Parisす左ですね、例がクロスプラスなんですが、前半の方で説明したようにフリー感だったり、割と白人だったりあとMLメールに関するデータとか、あとオールドヤングっていう時に関するBiasとかが含まれてますね。

[0:48:38 - 0:49:13]
Atariの人だっていうのは人だBias技ナースっていうときに、それがハードルが必須なのか、税なのかっていう、そこのBiasだけのっていうのかっていう評価セットになっております。あとはFLASKっていうものがあってこれはOpen-set作曲ぞClaudeセットである、ある意味も答えがあるような問題だったんですけどこれは答えがないOpen-setのベンチマークになっていて、ロジカルシンキングだったり、バックなんなりしちゃったり、そういったいろんな観点で合計12個のスキルを評価してるのになっています。

[0:49:14 - 0:49:41]
GPT-4を用いてそれぞれの評価っていうのを5段階評価していて、一応論文では人間ベースの評価とGPT-4ベースの結果っていうのは、同様の傾向を示しているっていうふうに主張されていて、これ左が人間の右がGPT-4なんですが、同じって言えるかってのはちょっと若干微妙なところあるんですが、少なくともあの人所関係はそう考えるっていうふうに元論文の方では示されています。

[0:49:42 - 0:50:03]
あとは前半の方にもちょっと説明したんですけど、うどんと合わさってるゲームのリスク評価のための包括的なデータセットも評価にも使われますし学習にも使えたりします。これもGPT-4によって各カテゴリーに該当するかっていうのを01で判定しているようなそういったものになっています。

[0:50:05 - 0:50:37]
あとは報酬モデル評価ですね。報酬モデルがちゃんと心の報酬生地的できているのかってところを評価してるのがこのRewardベンチになっていて、いろんな観点のデータセットを、が包括されていて、しかもこのリーダーボードも公開されていて今が今どれが一番いいRewardモデルなのかっていうのが出ていますこれ一応昨日の結果なんですけどLLaMA-2り3.1mっていうのが、70ビジョンが一番今評価としては高いものになってます。

[0:50:39 - 0:51:15]
はい次に、1R1Fの発展的内容に行くんですが、これが40分ぐらいなので、一旦この発展的内容をやってから質問か。一旦ちょっとバーっと説明してしまいますね。はい。今では基礎やってきたんですが、いろんな発展手法があって先ほどのRL使いたくないっていう話もそうなんですが、いろいろ苦労してるところを改善するような手法っていうのがいろいろ出てきてます。

[0:51:16 - 0:51:40]
その代表的な指標としてDPOっていうのがあってこれがRLフリーのアルゴリズムになるんですが、これが一番一番デファクトでよく使われてるようなアルゴリズムで玉とかでも、使われてますねミストラルとかでも使われて、他のモデルでもいろいろ他のオープンなモデルでも結構使えたりするので、今回ちょっと重点的に見ていこうかなと思います。

[0:51:40 - 0:52:00]
その後に長谷DPOの派生仕様もたくさん出ているそこの説明と、あとその他のAlignment賞についても説明していきたいと思います。DPOって何かっていうと、報酬モデルを使用せずに直接PreferenceRankingを学習するっていうものです。

[0:52:02 - 0:52:19]
今までのRRHFだと、最初に報酬モデルを学習して、その後に強化学習っていう流れなんですが、その強化学習ってのはできればやりたくないよねっていうのは前回話したんですが、あそこのStepを飛ばしたのが、このDPODirectPreferenceOptimizationっていうものになります。

[0:52:20 - 0:52:40]
Rewardモデルは暗黙的に定義されていて、こういった式で定義されますこれはちょっと後ほどまた詳しく説明します。こういうことをすることによって、実質的に強者学習のみをすることによって、RRHFと同等な効果っていうのがこのDPOになります。

[0:52:40 - 0:53:07]
このDPOの勾配はこういう式で表されるんですが、ちょっとここ、ぱっと見よくわかんないと思うんですが、一応概念だけ説明すると、ここのってのが、ここがワイルズの方が先に来て、そっからは便を引いてるんですが、推定報酬、これ今Rθっての学習してるものなんで、推定報酬が間違えた分だけを見つけをしています。

[0:53:10 - 0:53:34]
より間違えたものに高い重みをつけて間違いないものは小さいをつけてってのがここのその後のってのはワインですね。YBが生成される確率をというの最大化していて、その後はルース、悪い方の回答をより生成されないように、茉夕度最小化みたいなことをしている、そういった購買になってます。

[0:53:34 - 0:53:58]
ちょっと具体的にここを説明していきたいと思います。でしょうか？今言ったDPOとRチーフっていうのは、数式的に10日になってます。それが何でかっていうところを説明していると思います。つまりRRHFのRLをしなくても、何でこの強化学習だけでRチーフと等価の式なのかってところですね。

[0:54:00 - 0:54:20]
RRHFの目的関数っていうのは、こういったふうに表されました。これ先ほど説明したものと全く同じになってます。心の報酬を気にするために、あのブラッドリーモデルを使用して報酬モデルを学習していくっていうのが最初の流れでその後にその報酬を使ってRしていくっていうのが主でした。

[0:54:21 - 0:54:37]
目的はこれになるんですが、これっていうのは、この最適解はクローズドフォームにとっことができて、つまりこれを満たすようなこのπっていう最適なπっていうのが、これを式変形することによって得ることができます。

[0:54:38 - 0:54:58]
その結果ってのがこのような結果になっていて、これが元々のSFTのもとのモデルを、ベータ分のRで、このEXPOのイニシャルをかけた分、ものになっていて、つまりこの元の分布をこのXポテンシャルで分布変えてるみたいなそういったイメージですね。

[0:54:58 - 0:55:25]
なってます。Zっていうのは、これ全体を確立するために、低下するためのこうやってて分配関数って言われたりします。Zってのはこういうふうに表されて、これはちょっと難しいかもしれないんですが、よく見ると簡単で、この上のこのπSFT×EXPO電車のところのYに対して相は取って、取ってる形になります。

[0:55:25 - 0:55:45]
なんでこれをそのままYで全部そう取ってるんです。が、このZってのは直接計算は深野やってます。なんでかっていうと、RXに対して、全ての考えうる具合を計算して足してあげるみたいな、こうやってるんで直接これは負荷計算不可能です。

[0:55:49 - 0:56:11]
先ほどのこのクルーズホームに最適化計算できるって言ったんですが、それはどうやってやってるのかっていうのを一応軽く説明しますね。式変形自体は結構簡単な式変形導出できて、これはもう元論文からすぐ引っ張ってきてるんですが、このπFって書いてるのはφSFTだと思って読み替えてください。

[0:56:11 - 0:56:30]
元々の目的ますこれなんですが、これをそのまま試験してあげます。この下の式では、これ単純にあれですね、あのKLの子を中に入れるだけですね、中に入れてまとめただけです。これをどう変形していくかってことなんですが、最初にこのマイナスデータで終わっております。

[0:56:30 - 0:56:51]
ML全てで割ってあげると、この部分ところとRが-データ分の1πされますねマイナスでは待ってるので最大化問題じゃなくて最初かもしれないます。このその次にこのマイナスべた分の1っていうのを、このログの中に無理いただけます真ん中にいるんで、これはXPolicyがかかるわけですね。

[0:56:53 - 0:57:16]
この中に入ると、これ確実にメールしたいんで、ずっとで終わります。Zで終わった分だけこっち外に出てくるっていう流れです。これをそのまま地形変形してあげると、ここはKLのになるんで、このπとこのφアスタリスクってのはこうですね。

[0:57:17 - 0:57:50]
この分子の前πSFTます。この項のKLと-6Zってのが出てくるわけですけど、この-6ZってのはこのY2φの出力に依存しないので、無視することができて、結果的にこの二つのKL距離の最初かになります。このKLこのKLの最初かって、この二つが一致してるときが一番最適なので、つまりKLが0になるんで、最適方策はこのπアスタリスクが最適方策になります。

[0:57:50 - 0:58:15]
つまり、この部分ですね、の部分が最適な方策なるってことが証明できます。はい。これ見てもらえばわかるんですが、この最適方策のπアスタリスクってのはこういうふうに表せるっていう説明したんですが、これよく見ると、これRの形にも関わることができて、つまりこのRを取り出して書いてみるとこのような形にかけます。

[0:58:16 - 0:58:51]
これは対の関係になっていて、つまり報酬関数がまとまれば、最適な方策πってのが決まって、逆にこのπが決まってあげ決まってしまえば、その裏にある報酬関数っていうのも自動的に求めることができますこれがこのDPOのタイトルになっていて、LanguageModelsCPOAtariRewardモデル、つまりこの言語モデルっていうのは、目的にRewardモデルも含まれてますよっていうのが、一応このタイトルになってます。

[0:58:51 - 0:59:10]
これが結構すごいことです。これ発見したのがそういうことですこれがわかってしまえば、このRっていうの学習をするだけで、方策も一緒に学習できてしまう。それを学習すれば最適なπが決まるんで、つまりRだけ学習すればいいっていう流れになります。

[0:59:12 - 0:59:43]
よいしょええ。ここちょっと変なサイトが混じってます。一応これ復習のスライドなんですけど、報酬モデルの福祉学習どうやったかっていうと、こういったような形で学習してましたこれ復習ですね、前のスライドで、この報酬関数の学習のところに、さっき求めたRっていうのをそのまま代入してあげるような形なります。

[0:59:44 - 1:00:08]
つまり、さっきのこのlossのところ、これですね、これ元の式です。これは一応説明しておくと、このワインルールは便が勝つ確率への誘導を最大化しようとして、ぷらっとテリーモデルか仮定するとこのようなlossが出てくるこのlossのこのRのところに、この最適なRっていうのを大にしてあげます。

[1:00:08 - 1:00:37]
するとこのような試験出てくるんですが、このデータ6Zっていうところは、よいしょデータ6Zっていうところは、これ差分を見てるので、消え消えます。本来ならこのZっていうのは計算できないんですが、この目的完遂いてあげると綺麗にこのベトナムセットっていうのは聞いてくれるので、ピースしなくて良くなるっていうところです。

[1:00:38 - 1:01:23]
そのまま来院してきてこのような式変形ができて、これ何してるかっていうと、報酬関数がこれとみなしてると同じです。だから最初に言って、と思うんですが、Rって述べたログをπSFT分のp θとみなして、報酬モデルを学習してると同じこれが暗黙的に報酬モデルを仮定してるっていう、そういったものになりますこの目的関数に従って学習すると自動的にこの最適なφθが得られるっていうそういった流れになっていて非常にすごい発見みたいという形でnewsの2023のBestPaperにもこの準備を行っています。

[1:01:24 - 1:02:07]
よいしょつまり今まで説明した通り何の近似とか、過程とかを追加せずに、このRチーフとDPOが等価であることが今示されました。つまりこのlossRewardとlossRって二つのStepを使って学習しなきゃしなきゃいけなかったものが、この一つの目的関数を強者学習の形で学習することによって、RLAIFと全く同じことをすることができるっていうのが今示されてなんか非常にこのRLの細かいこの不安定さとかを全く考えずによくなので、非常によく使われててそういった流れになります。

[1:02:08 - 1:02:26]
はいPPOの接種をどういうものがあるかっていうところなんですが、いろいろこのDPOの接種を考えられていて、このIPOIPOっていうものは、DPOをさらに一般化しようPモチベーションで提案されたアルゴリズムになってます。

[1:02:28 - 1:03:04]
これもいきなりこのφっていうΨっていうこの数が出てきて訳わかんないかもしれないですが、このΨっていうので、元々の目的関数は、ほとんど変わらないんですけど、このYルールがYBに勝つ確率っていうのをこのΨで囲ってあげるようなそういった目的関数になってますこのすΨっていうのをこういうふうに置くことでPPOと同じ目的化していますつまりこの手法ってのはDPO包含してるような支援をやってます。

[1:03:05 - 1:03:56]
さらにこのΨを9行動関数にしてあげたものを家にφPreferenceReplayションIPOって言われたりもしてよく発生しようとしてよく使われてるものになっています。あとはKPOっていうものもありまして、これ何かっていうと、今まではXYWXYVワイルズっていう三つのピアノデータが必要だったんですが、これは単一のXYのピアノから学習できる手法で、プロスペクト理論って言われる人間の声モデル方策に学習に組み込んだ小Raceこのプロスペクト理論って何かっていうと、この左側の効用関数になっていて、横軸がどんだけ委託を受けたときに、紅葉が縦軸ですね。

[1:03:56 - 1:04:24]
Novel超えるかっていうもので、つまり5万円を得るときの高揚、喜びですねある意味、喜びっていうのは少ないんだけど、5万円を失ったっていう喜びとか悲しみってのは圧倒的にこのなんですか。高くなっていく大きくなっていくっていうのが、この価値関数という関数をやっていて、だからここがこうなんでしょうね、逆向きの椅子みたいなってる感じですね。

[1:04:25 - 1:05:08]
なんでつまりこの失ったファイルがどんどん失った方が、悲しみが大きくなっていく喜びも大きいような感じなっててこれを組み込んだものがこのKPOになってます。詳しくは解決しないんですがlossはこんな感じになっていて、単一のペアから学習するんであるPreferenceっていうのが三つのペアじゃないんで、どうやってこの報酬の竿っていうのを学習するかっていうとある基準値からの差分を使って、どれだけそのだから失うのと、いるのみたいなええ。

[1:05:08 - 1:05:25]
なんていうんでしょう。説明になると思うんですがどれだけ報酬がこの失ったか出たかっていうところで、このKPOのプロスペクト理論を使って、うまいこと向けBotモデルを学習していくっていう、そういった流れになります。

[1:05:27 - 1:05:59]
DPOの発生資料結構いろいろ出てるんですが結局どれがいいのかっていうところで、一応おさらいすると、マルチFDPOIPOIPOKPOっていうのをLIMAで説明したんですが、それはどういった分類になってるかっていうと、データセットと、あと報酬関数に買って関する過程で一応開けることができて、RRHFからIPOまではPreferenceデータ使ってるんですが、KPOってのは、このアンペアと毎日データを使ってます。

[1:06:00 - 1:06:25]
報酬関数に関する家庭だと、RJSDPPOってのは元々のブラッドリーモデルを使っていて、IPOってのはそういうの一般化でIPOってのはIPOの特殊なケースですね。使っていて、KPOってのが、これはプロスペクト理論を使ったモデルなんですが、このモデルを使ってる、そういった報酬関数以下する過程がこの各章で違うっていうのが担ってます。

[1:06:28 - 1:07:16]
結局どれを使えばいいのかってところなんですが、これDPOIPOKPOCPOってちょっと今説明したいんですがこれはネガティブデータだけを使って学習するみたいな手法になるんですが、これが結果になっていて、SFTをした後にこのDPOとか各種を適用していく手法だと、一番DPOが高いんですが、最初のこのSFTっていう段階を除いてそのまま適用させるってこともできて、そうした場合は、DPOっていうのは当たり前しかもよくなくて、ただこのKPOとかCPOと呼ばれる方法は、SFTなしでもある一定程度の性能を出すことができるので、学習コストが抑えたい場合はこのKTOとかCPOを使うのがいいよ下げっていう形です。

[1:07:16 - 1:07:53]
ただ一番汗の出るのは、SFTをした後に、さらにDPO数ってのが一番精度が出るっていう結果になってます。気になるのは、DPOがSonyDPOの接触の中でも、学習コストをかければ一番良くなりそうだなっていうのはあったんですが、DPOとそもそもQ学習SPPOってのはどちらが失礼だのかっていうところで、一応数式的にはどうかっていうところは先ほど説明したんですが、実際に性能的にどうなのかっていうところだと、実際DPOはPPOより悪くなるっていう結果が出ています。

[1:07:58 - 1:08:23]
この表がその結果になってるんですがベージュを見ればよくて、PPOとDPOだとPPOなんかも軒並みいろんなデザートで精度が高くなっていることがわかります。結構いい勝負はするんですが、なかなかこの最後のひと押しがPPOには勝てないっていうのが報告されてますいろんな論文で報告されてます。

[1:08:24 - 1:08:53]
順番としては、SFTが一番悪くて、その次にDPOしたモデルその次にTDPOって言われるDPOが何回か複数回提供させる手法もあって損傷が本当のDPOが良くなるあとフィルターのDPOもあって、これ何かっていうと、データのフィルタリングしてあげる質の良いデータを使ってDPOしてあげると結構性能が良くなるんですが、PPOにはやっぱり勝てなくて、こういった順番になってます。

[1:08:54 - 1:09:28]
この右の図がそんなような手法になってますが、最初のSFTモデルがで、AtariDPO使って、さらにデータでDPOも返してあげるとまたよくなって、DPOPPOに変えてあげると、またさらに良くなる。PPOの中でももっと強いRewardモデルを使ってあげるとまたさらに良くなるみたいな感じになっていてよくあるのはDPO何回かやって最後にPPOみたいな流れが、精度を出すためにはよくやられてるような手法になっています。

[1:09:29 - 1:10:00]
なんでDPOは学習コストは低いんですけれどもあれに比べて、ただ、そこそこの製造モデルがトップの性能を出すにはPPOを使うっていうのをやらないといけないのが、今の現状の結果になってます。これなんでかっていうと、PPOを使うことで、Rewardモデルを最初に学習するわけなんで、Rewardモデルの外装つまりいろんな入力データをRewardモデル上げる学習データで、与えなかったような分布も一応Rewardで推定できるわけですね。

[1:10:00 - 1:10:19]
なんでそこの外装のデータがうまくある程度うまく検知できていれば、その外挿データのRewardを使ってさらに学習ができるんであれば学習を増やすみたいなことができてそのせいで良くなっているんじゃないかって言われたりしますし個人的な主観としてもそう思ってるっていう形です。

[1:10:19 - 1:11:08]
これ実際本当に何で良くなってるのか数値的に等価なのに、なぜPPOおよびいいのかっていうのはまだわかってない段階です。よいしょあとは先ほどRRHFリバースケール使うって言ったんですが、リバースケールのときの問題点っていうのもありまして、最適報酬が、先ほどDPOの説明でこういうふうに書けるって言ったんですが、これって結構すごいやばいことをしてて、なんでかっていうと表のモデルをこのEXPO電車に変えてるというか、もっとモデルを結構分布を尖らせるような形で分布を変えていて、こうすることによって、多様性が結構損なわれてしまうってのが、いろんな報告でもあります。

[1:11:09 - 1:11:31]
この右の所っていうのはRSFTで、各Z(x)っていうのは、多様性が高ければ高いほど多様性があるっていうふうなメトリクスになってるんですが、これを見ると、明らかにこのRRHFが多様性が減ってもこの最後はちょっと同じぐらいですけど、明らかに減ってるってのが見られます。

[1:11:33 - 1:12:30]
なんで、この左もそうですね。DiversityのやつもBardKLとRewardKPORKLでReward傾斜ですね、リバース系ですね、リバースケールで多様性が下がってしまう現象が見られてます。これを解決するために、リバース系じゃなくてフォワード系ファンドの経路を使ってあげたり、あとはこのXポテンシャルが悪いので、このXプレーンシャルっていうのが、どこから来たかっていうと、KL子のところで、無理矢理ログの中にこの日Rっていうのを入れてあげる言いたげるってことをしたと思うんですが、そのときにXPolicyが出てきてしまって、ここをなくすためにKL-divergenceでなくてFダイバージェンスって言われる国がスペシャルlogじゃないやつですね。

[1:12:30 - 1:13:01]
6じゃない不具合ダイバージェンスを使ってる研究もあって、こういった研究だと、多様性っていうのは確保されるんですがただ、Alignmentの性能は落ちてしまいますこの表でもある通り、リバースKLってのが一番Alignmentの明義は高いんですが、多様性が下がってしまっているっていう、他のφRKLとAlignmentはかなり握手下がってしまうんですが多様さがあるってここがなんかTrade-offになっていて、ここがうまく両立できる手法っていうのはおそらくまだ考えていない段階だと思います。

[1:13:02 - 1:14:15]
何でもいろいろ研究がやられてるっていう形です。あとその他の手法についていろいろDPOの派生手法ありまして、気晴らしに対IterativeなDPOだとSelf-Rewardingって所があったりあとはこのRewardを一番最後に与えるっていうことをやってたと思うんですが、それを選定することじゃなくてトークレベルに与えてあげるような手法とかもありますあとは、これはもうSFTの段階でPreferenceを考慮しながらSFTをしてしまうっていう乙BフレッツこれORPO出力忘れちゃいましたけど打つ日を使って、それを使ってSFTをしてあげるっていうORPOって症があったり、あとはBPOするときにも、ReferenceモデルとBotのモデル二つのモデルを読み込まないといけないんですが、そのリファレンスモデルっていうのをSFTもですね、消してしまおうっていうのがこのPreferenceFreeモデルでSimPLePreferenceOptimizationって言われるんですがこういった手法もありますこれも結構精度が出てて、メモリ効率も良くなるんで、かなり良いショーですね。

[1:14:15 - 1:14:34]
あとはネガティブPreferenceだけを使うPrevalenceOptimizationAtariAtariCPOってましょうあとはLUSHラーニングっていうのがあってこれは普通のピアノ専攻だったら、SIMPOだけを考えたらいいんですけど三つの専攻と考えるとそれが何か3足みたいになってしまう。

[1:14:34 - 1:14:48]
ようなラベルも存在していて、それのナッシュ均衡を取るような形で学習していくっていうのが、このSPPOReplayPPOPreferenceOptimizationだったり、ダイレクトな種Optimizationで呼ばれる手法になります。

[1:14:49 - 1:15:10]
あといろんな立派な手法があって、ちょっと全部説明しきれないんですが大体仕様のところは今、ざっくり示したかなと思いますこの赤が今説明してください。はい。その他のAlignment首相はどういったものがあるかところです。

[1:15:12 - 1:15:28]
最初にRAFTすいませんごめんなさいちょっと一旦休憩を挟みつつ、質疑対応していただくこと可能ですかちょっとわかりました。一旦休憩挟みますか。そうですね5分ぐらい取っていただけると良いかなと思います。わかりました。

[1:15:28 - 1:15:49]
一旦5分休憩をとります。その間ちょっと質問とか行っていきますね。はい、よいしょ。はい一旦5分休憩で20分から始めさせていただきたいと思います。はい。皆さんちょっと各自はい休憩とっていただきつつ、もしはい聞ける方は質疑応答の方を聞いていただければと思います。

[1:15:49 - 1:16:33]
ちょっと高城さん画面切り替えていただくことが可能ですか。はいわかりました。はい。上は見えてますかね。そうそうそうここのFeedbackのところをこれは選択してうん。その状態で大丈夫です。範囲内でごめんなさいFeedbackはごめんなさい選択してくださいとうんわかりました優先度を入れる。

[1:16:36 - 1:16:50]
はい。はい、ありがとうございます。はい。いくつか出てきてるのでここからいくつかはいピックアップして皆さんで共有すると良さそうなやつを回答いただけるといいかと思います。わかりました。これはばっと入って解決した方がいいんですかね。

[1:16:51 - 1:17:19]
そうですね一旦そこを気にせずで大丈夫かなと思います。わかりました。とか、そうですね。そこら辺からやっていけますか。これに無限大の人がFeedbackられ来庁な学習管理をさせます。これは無限大のですが駆使された言語モデルとなることあるんですか。

[1:17:20 - 1:17:50]
これは無限大のZがちょっと何かわかってないんですが、WebのデータだったらWebの全てのデータを使っているとLengthのと、あとはFeedbackも何かあらゆるFeedbackを与えてRKLするっていうのと何が違うのかっていうところっていうふうに認識したんですが一応無限Rセット何かやるんすかそういえばWebの全体の技術を使ってPre-Trainingをしたとします。

[1:17:51 - 1:18:18]
その分布っていうのはあらゆる価値観が入った平均的な多分分布になるはずなんですよね。つまり、悪い時もあるし止まるっていうそういったものでPPOにすると何だろうな平均的な分布になっていて、その分布のどこを押さえてどこを尖らせるかっていうのが、ある意味RRHFの役割になってると思います。

[1:18:18 - 1:18:48]
つまりそのPretrainとモデルは、その暴力的なことも要といえるし、そのBiasがあるようなことも知ってはいるんだけれども、知ってるけどそれを抑えていく、出す。確率を抑えていくって形ですね。いうようなことを言う確率どんどん増やしてあげるように、この確率分布を曲げていく形っていうんですかね、尖らしていく形がRHFの役割になってます。

[1:18:48 - 1:19:25]
ちょっともう書いてあってかわかんないんすかそういった形になってあとは、これとかですかね、弱いやつ監督するということでしょうか？靴の良い絵が強い監督するということでしょうか？解釈としては合ってますね。ウィークてストロングジェネレーションということで、例えばすと7Billionのモデルとcolorとかが70ビデオのモデルを、うまい具合にAlignmentしてあげる。

[1:19:25 - 1:19:53]
例えば、二つの出力をlossがいいのかって、どこがいいのかっていうのは、そのちっちゃいモデルだと判断できないかもしれないすけど何となく、どっちがいいのかっていう日分類の問題に落とし込んであげると、ちっちゃいモデルでもある程度いい悪いっていうのは判断できるじゃないかっていうのが一応仮定になっていて、そういったモデルを使うと、大きなモデルの分布を上手いこと変更してあげる。

[1:19:54 - 1:20:14]
さっき言ったようにまたばらしてあげるみたいなことが可能なんじゃないかっていうところでOpenAIが出るものがこの仮説になってますね。ただこれが絶対できるかっていうところはまだわかってないところで、一応概念として存在しているものの、ここが本当にできるのかみたいなところが疑問を持った人たちもたくさんいるっていう感じです。

[1:20:18 - 1:20:39]
あと1個ぐらい行きましょうか？Post-Trainingでも人間にとってより良い回答が学習として与えると思いますが、それだけではLLMが人間にとって良くない回答をしてしまう理由は何でしょうか？Post-Trainingって言ってるのはSFTという話ですかね。

[1:20:40 - 1:22:03]
一応R時半Post-Trainingに入るんですが、ていうかToxicity一応SFTだけっていうふうに解釈をするんですが、SFPだけだと確率分布を変えるっていうよりは、何て言うんでしょうね。その文章がよくよく生成できるように、その部分だけを尖らしてるようなイメージ、何ていうんですかね、そのデータだけを尖らしてるみたいな形である意味このAlignmentってふわっとしたもの例えば、Biasあること言わないとか、Biasなることでも例えば窃盗のことだけ言わないとかだったらSFTでも十分できると思うんですけど、そのBiasん中でもこの抽象的ないろんなBiasがある中で、全てのBiasをSFTで隠すのは難しいと思うんで、このRFPある意味Preferenceっていう曖昧なものを使って、その各その中からこのBiasっていうのは大体こういうものだよみたいなところを、分布を飛ばせることで、それを言わないように学習していくみたいな、そういったイメージですかね。

[1:22:03 - 1:22:34]
つつあってかわかんないですか。なんでSFTだけだと完全にこのAlignmentするのは難しいってのはそういうことです。はい。ちょっと時間が過ぎてしまったんで、一旦説明に戻りたいと思います。尚、はい。続きしたいと思います。

[1:22:36 - 1:23:02]
その他のAlignment手法っていうところで、DPO以外の他のRLFree以外のAlignmentについても触れていこうかなと思います。はい最初にこのRAFTっていうもので、これ何かっていうと、先ほど言いましたフィルターのDPOにも若干近い近いんですけど、データをフィルタリングすることによるAlignmentっていうものをやってます。

[1:23:03 - 1:23:23]
これは報酬モデル学習した後に、報酬モデルのトップ上位K%っていうのをKLの100%ですね。なんでKが後だったら20%ですねファインチューニングすることによって、Alignmentしていくっていう流れになってます。

[1:23:25 - 1:23:48]
これはPPOを使用することなく、いつも同等の性能になっていて、フィルタリングするだけで、ある一定程度の効果は出るっていう形ですかね。でもPPOとかには完全に勝ててるわけじゃないのでそこが難しいところですがある程度の当てるっていう形です。

[1:23:51 - 1:24:26]
あとはRLCDって言われるAIFeedbackを使った仕組みになるんですがこれは、あるプロンプトに対して、Harmlessとハム振るっていう二つのプロンプトを見込みますつまり、Harmlessに行ってくださいパンフに行ってくださいっていう二つの指示を与えて、実際このHarmlessっていったものには1のラベルを与えてあげて、振るっていうRAFT与えたものに対しては0のLabelerを与えてあげて、これを使ってPreferenceモデル学習してPPOするっていう、そういった流れになります。

[1:24:27 - 1:24:49]
元々のRLAIFだと二つの回答があったときにどっちがいいのかっていうのをAI例えばGPT-4とかによって、Preferenceをラベル付けてあげるみたいなそういった流れなんですが、これはそういったような外部の家を使わずにπRLAIFAIFができるような、そういった手法になってます。

[1:24:50 - 1:25:21]
あとは複数のモデルを使ってランク付けするっていうのもあって、これ何かっていうと、RQueryがあったときに、エキスパートのレスポンス、これは人間だったり、何でもいいんですが、だったりあとはChatGPTのレスポンスだったり、あと他のオープンソースのモデルをレスポンスを作ってあげてそれをなんかスコアをラベリングしてあげますその下スコアとLanguageモデルのスコアってのが一致するように学習していくっていうのがこのRRHFってそういった所になっていきます。

[1:25:22 - 1:25:37]
あとは知恵部品のサイトっていうもので、あとGAEを使ったFeedbackするっていうのでRankingを付けた後に、このRankingなぜこのRankingなったかっていうのを後付けでFeedbackの分を与えてあげます。

[1:25:37 - 1:25:54]
つまり、AはBより悪い、こういう観点で悪いBRewardCはこういう観点で、イコールで一番いいのはDですよみたいなのを文章によってFeedbackを与えてあげるそれをπにしてあげるってのがこのショーになっています。

[1:25:55 - 1:26:22]
あとは人間社会を使命とするスティーブAlignmentってのがあってこれは何かっていうと、LLMのこのAgentっていうのを多数用意してあげて一度同士があるブログであったときに、それに対してどういう回答するのかっていうのを対話のような形でFeedbackを与えてあげるそのFeedback不足されたFeedbackを使って、ファイリングするってのが、このStepAlignmentっていう手法になったりします。

[1:26:22 - 1:26:39]
ちょっと時間ないんで、ずっと説明していくんですが、これAlpacaFarmっていうのもあってこれは評価をどうするのかっていうところで人間の評価とできるだけ同じような評価をするようにメディックスを作りたいので、それをどうやってやるのかっていうのが、このAlpacaFarmって言われる手法です。

[1:26:41 - 1:27:05]
これはやってることとしてはいろんなDPOのモデルだったりオープンソースのモデルを複数使って、どれがどの出力よかったのかっていうのを評価してあげて、評価を付けるっていう方法で、人間の数評価と高い相関で一致していて、Alpacaevalって言われますけど、そういったものが評価として使われたりします。

[1:27:08 - 1:27:26]
こういった発展的に本体別としては、このような形でいろんな軸で分けられて、隙はFeedbackを使う方だったりAIFeedbackを使う方法、マジックだったりあとはRLを使う方かRLフリーの手法なのかDPOとかあるいは使わない所ですね。

[1:27:27 - 1:27:48]
あとはRankingでFeedbackするのかLanguageでFeedbackするのかっていう観点RankingでFeedbackってのがPreferenceを使って魅力するR1Fとかそういった手法になるんですがLanguageベースのFeedbackっていうのは先ほど説明した、手ぶらで目とか、あとはチームヒントサイトとかそういったものになります。

[1:27:50 - 1:28:22]
RLの課題や対策についてなんですが、ちょっと時間がないのでちょっと説明していくんですが、全体像としてはこのような形で、日々のFeedbackにおける課題とRewardモデルにおける課題、あとはPolicyにおける課題っていうの三つが存在していて、全体像としてはこのような形になっていて、ちょっといくつか説明していくと、実スライド呼ばれたやつってところでRHFによって訓練されたモデルってのは、誰の意見に反映、誰の意見を特に反映しているのかってところですね。

[1:28:22 - 1:28:55]
先ほどからRRHFによってもトランプを取らせるっていうのを質問したと思うんですが、このRHFに使うデータってのが結構Biasがのったデータある一部の偏ったLabelerによって、ラベリングされているとBiasが尊重されるような結果になって、例えばこれだと荒地踏まえは低額所得とか低学歴に位置するような意見だったんですがRGFRRHFはその黄色だったり毛質を維持するような意見に変わってしまったっていうそういった結果になっています。

[1:28:56 - 1:29:18]
あとは監督するのが難しいところで、そもそもクラウドワーカーに対して、Labelerラベリングしてくださいっていうふうに説明すると思うんですけど、クラウドワーカーがそのLを使ってこずるをして、なんていうんでしょう、鍋をつけてしまうそうする方がもらえる報酬よりもAPIの方が安くなっちゃうんで。

[1:29:18 - 1:29:32]
そうすることに合理性があるんで、そういったことが起こってましたよってのがこの日になっていて、大体クラウドワーカーの33パーから46%がLMを使ったと推定された。ような結果になっていたっていうところが報告されています。

[1:29:32 - 1:29:47]
あとはクオリティですね。このLIMAっていうのは評価Alignment仮説ってのがありまして、それ何かっていうとモデル知識と能力ってのはほとんどこのPretrain段階に学習されてるっていう過程があります。

[1:29:47 - 1:30:20]
なんで、少数の例でも、高い品質のデータを使ってあげることで、より良いAlignmentができるっていうのが一応ここで示されてることになってあとはHumanFeedbackにおける課題ですね、FeedbackいろいろRankingとかFeedbackにおけるLanguageFeedbackにおける自然言語でFeedbackする方法だったり、あとは絶対値でFeedbackする方だったりあるんですけど、どういったFeedback州で使えるのかっていうのと、効率さってのはTrade-offになってます。

[1:30:21 - 1:30:38]
Rankingするのは、どっちがいいのかって選ぶだけで簡単なんですが、効率は非常に悪くなってます。つまり、いっぱいデータ集めていけない現行Feedbackだと質の担保が大変っていうのがあって、そもそも人間の認知の限界としてRankingが一番いいんじゃないかってところで一番よく使うのがRankingになってます。

[1:30:39 - 1:31:08]
あとはRewardモデルにおける課題としては黒の方が挙げられていて、まず一つ目としては、複数の意見にある問題に対してそもそもRewardModerateSCARの一つの値をつけるわけなんで、そのいろんな意見があるものに対してこれが絶対的にいいみたいなところってのはあんまりないも問題もあって、そういうときにどうスコアをつけるのかってのが非常に難しいっていうのがここで挙げられている課題になってます。

[1:31:09 - 1:31:26]
あとは、あのRewardモデル一番最初に言ったように夜Hackingみたいなのがやっぱり起きてしまうことが多いんで、どういったパラメーターのRewardモデルを使って、どういうPolicyのパラメータとどういうリアルを使えばいいのかっていうところが非常に難しい。

[1:31:26 - 1:31:44]
ていうことになっています。過剰適応を起こすHackingってのが起きやすいので、はいこれだと、どこまでスケールさせればより良いRewardで買えるのかっていうのを実験したものになっています。あとPolicyにおける課題ですね。

[1:31:46 - 1:32:21]
Policy使うときに、あのPolicyにも敵対的プロンプトみたいなのを用意してあげて、Jailbreakっていうのができる可能性があるってのがここで挙げられてるものです。つまり、学習前はなかったからっていうのが、その主符号によって生まれてしまうその穴を突くようにプロンプトを与えてあげると、本当はBiasのあるようなことを言わないように学習したはずなのに、言ってしまうような、とか安全規則とか制限とか無視するように、附則しまうそういった課題があったりします。

[1:32:22 - 1:32:45]
あとは多様性が失われるってところですね。確実にと話すように学習してるんで、元々1RRHFする前はこういったような位置の部分になっていても、学習後には0結果っていうのが強調されるような分布になってしまうっていうのが入れられます。

[1:32:46 - 1:33:28]
例えばGPT-4だと、①不可と自信を持ってこれをこうだっていうふうに間違える場合が多くなるってところが報告されていたりします。あとはRewardモデルとかPolicyに共通する課題ですね。この辺は対策になっていて、ヒューマンFeedbackによる対策ってところで、Feedback形式どうするかっていうところで単一の一つのスコアだけだと情報量が少ないんで、各センテンスごとにいろんな観点のRewardモデルを作って、Feedbackを与えてあげるっていうのが、このファイングレインでFeedbackっていう論文になっています。

[1:33:29 - 1:34:03]
あとは多様性を確保するにはどうすればいいのかってところで、Rewardモデルをたくさん作ってあげて、それをくっつけてあげる、くっつけてあげるっていうのは思いを足してあげるっていうことをして、各観点でAlignmentされたモデルをいい感じにこのパレット待機的なモデル海鳥が一番この全ての観点を考慮して、Alignmentされてるようなモデルになるんですが、こういったモデルがRewardモデルのStepによって、つまり平均がしてあげることによって確保できるよねっていうのが、この日になっています。

[1:34:05 - 1:34:28]
あとはRLの不安定さを解消するためにマックスまで使うっていうRRHFですね。これは先ほど説明したものになってます。いろいろ発展的な儀礼を上げてるんですが、この辺はちょっと個人的な主観なんで事実じゃないんで、なるほどねぐらいの感じで聞いてもらえばいいんですが、そもそもなんでRLで性能があるのかっていうところですね。

[1:34:29 - 1:35:25]
そもそもイベントをやりたくてRしてるってのが前提なんですが、何か性能上がってるわけじゃないそうってところですね。だからそれは今までずっと言ってますが人分布データ分布っていうのをそういうように出力に変化させてるだけで何か新しい知識とかを得ているわけではなくて、元のモデルのこの分布っていうのをいい感じに請求してあげるっていうのをやってるっていうところですRL本当に必要なのかっていうところなんですが基本的に一番最初に説明しましたがDPOでほとんど名前こと足りることがおそらく多くてただ本当にトップの性能を使うトップの性能のモデルを作るには、PPOとかを用いないと今現状ではできないってところでおそらく元々の目的関数自体は、DPOも歩いても全く同じなので、データも同じだった場合は、同じ結果でないと本当はおかしい。

[1:35:26 - 1:35:44]
OKです。なんでRじゃなくても、本当はAlignmentできるはずなんですが、今なぜかPPOの方が優位になってるっていうところで、おそらくここら辺は研究が進んできて、RLが必要なく、PPOと同等、もしくはそれを超すようなモデルってのが出てくるっていうふうに個人的には思っています。

[1:35:45 - 1:36:03]
あとはSFTとRFですね。さっきの説明でも、質問でもあったと思うんですが、SFTは人間からのLanguageFeedbackって解釈することもできて、そうなったらSFTだけで十分じゃないかっていうふうな疑問が湧くとあります先ほど質問でもちょっと関連するかもしれないですね。

[1:36:04 - 1:36:21]
一応結論として僕の中の結論としてはある程度まではSFTで十分だと思うんですが、どうしても残りの1%を請求するには必ず必要になると思っていてつまりモデル出力制御には紐Feedbackは今後も絶対必要になると思ってます。

[1:36:21 - 1:36:42]
あと人間Feedbackの限界としてもLanguageSlackってのはかなり難しいんで、つまり分散が結構大きくなってしまう。人によってだいぶ差がわかってしまうんですがRankingってのは大体こっちとこっちどっちがいいって言われれば、大体こっちがいいっていうふうな結果って、分散が少なく得られるんで、そっちの方が最適なんじゃないかっていうところですね。

[1:36:42 - 1:37:26]
あと①太RLAIFっていう未熟どっちがいいのかってところで、これも最近もいろいろ研究されてますが、人間が介在しないようなFeedbackだと、Feedback元のモデルの性能を超えることは多分ないんじゃないかなと思っていて、最近だと合成データとかを使って、もっとモデルをどんどん上げていくみたいなのもあるんですが、アレフの文脈に関しては、もっとモデルの性能情報的に増えてないんで、こういうことないんじゃないかなって個人的には思ってますただ、あの人間のFeedbackの性能を家で引き上げて、より何でしょうFeedbackのラベルの質を上げていくっていう手法でのNFっていうのはついていくんじゃないかなというふうに思います。

[1:37:26 - 1:38:01]
プラス、あとはあの外物ですね例えば数学とか、リングの問題とかだと、明確な答えが計算できるので自動でそういったあらゆる外部ツールを用いてFeedbackを与えることによって勝手に行動なんでしょう、自己進化じゃないですけど、自分自身でどんどんどんどん性能が上がってくみたいなことはあり得るんじゃないかなと思いますあとは何かWebデータに自動的にアクセスしてそこのデータをもとに学習していくみたいなそういった景色だとせ自己進化を送るんじゃないかなと思ってます。

[1:38:01 - 1:38:25]
こういったのはRDCFリーフの炭谷fromコンピュータのFeedbackっていうふうな名前も付けられていて、つまりいろんなソフトウェア上の何でしょう、ソフトウェア世界のあらゆる情報を外部のデータをアクセスできればいいかもしれないですけど、現実世界、そういった情報を使ってFeedbackをしていて自己進化していくみたいなそういった概念もあったりします。

[1:38:26 - 1:38:46]
はい。ちょっと若干長くなってしまったんですが、一応まとめとして、マルエフってのは、Alignmentを適用する手法の一つで、マイニングのフィールド使うものでしたよとAlignmentの基準としては、一時HelpfulとHonestとHarmlessっていう三つのものが代表的な基準としてありますよというところです。

[1:38:47 - 1:39:07]
あとChatGPTの詳細とInstructGPTは、このSFTとRewardモデル学習とRLの三つのステップで書きされていて、RRHFには様々な問題が今あるんですがRL必要としないDPOみたいな学習法も次々と提案されてますよってのが全体としてのまとめになります。

[1:39:07 - 1:39:29]
はい。ちょっと長くなってしまって、しかも若干過ぎてしまったんですが以上になります。ここからちょっと編集に入っていきたいんですが、休憩したらいいですかね。そうですねちょっと時間ないので、はいちょっと皆さん演習環境準備がてらに細部5分ぐらい休憩いただければと思います。

[1:39:29 - 1:39:44]
あと高橋さんありがとうございました。あとあれですよねコミュニティのイベントでまた10月14日14日発表いただく来週月曜か発表いただくのかなと思ってるんですけどそこで何か今日の補足的な部話とかってあるある感じなんですかね。

[1:39:45 - 1:40:08]
そうですね。もしあの、求められバスっていうのと、あとRHF関係の労務について発表しようと思うんでそういったうまく質問があれば、この講義の質問とかもしてもらって大丈夫ですね。わかりました。はいということなので皆さんぜひご参加いただければと思いますちょっとリンクに参加リンクは、あの、チャットの方に参加リンクを貼っておきますね。

[1:40:10 - 1:40:32]
はい。今、負けましたので、ぜひはいこちらご参加いただければと思いますございます。はい、ありがとうございましたお疲れ様ですお疲れ様です編集42分ぐらいですねちょっと短いんですが、2分間だけ休憩をさししていただいて、その間に演習環境準備とかしていきます。

[1:40:32 - 1:41:08]
はい、お願いします。そっか演習も高城さんがいるんでしたっけそうです。ごめんなさいちょっと勘違いしました。はいちょっと休憩はい、皆さん休憩取っていただければと思います。自分の方はちょっと演習用意してるんでちなみにあれですね17時半前後ぐらいにダウンロードした方、ご自身の環境に移した方、ちょっとそれ以降にちょっとまた更新しているので、お手数なんですがあの際いたダウンロードいただければと思います。

[1:41:08 - 1:41:24]
17市何分ぐらいで45分ぐらいでしたっけ、それ以降になります。ベンチマークですね。ですね。それ以降にダウンロードされた方は問題ないです。はい。てことで、お手数ですがよろしくお願いします。ありがとうございます。

[1:41:24 - 1:41:48]
データとしては、inputデータ.Jさんと、あのレクチャーセブンのエクササイズの二つのデータを今回用いる予定でノートブック開けていただくと、このような画面が出ると思うんですが、ここのアップロードで、inputデータ.ジェイソンっていうのをあらかじめ今度行ってもらえると助かります。

[1:41:52 - 1:42:13]
ごめんなさい時間的にちょっと惜しいそうですよね。想定の4時間10分ぐらい押してると思うんでそうですね。一応短い加湿もできる。うん。できるので、はい多分arXivも残すので、もし問題なければちょっと延長する前提で話していただければと思います。

[1:42:13 - 1:42:30]
わかりました。はいはい受講者の方々へのご連絡としてちょっともし次の予定がある方すいません、毎度で恐縮なんですが一旦本日は抜けていただいてまたarXivでキャッチアップいただければいただけるとありがたいなというふうに思っておりますのでよろしくお願いいたします。

[1:42:33 - 1:43:14]
ありがとうございます。ちょっと押し気味で申し訳ありません。ではでは、時間ですかね。はい。よろしくお願いします。はいこれからも引き続き演習の方やっていきたいと思います。ちょっと皆さんちょっとお疲れで申し訳ないんですけれども、今回の演習では今まで説明してきたRRHFのStep1からStepⅢの中で、最初のファインチューニングっていうのは今回省いていて、このStep2の報酬モデル学習とStepⅢのPPOで強化学習を行うっていうところの実装を行っていきたいと思います。

[1:43:16 - 1:43:51]
ライブラリとしては、①から実装するっていうのもいいんですが、PPOの実装って1からやるとかなり大変というか本質は違うところでかなり行動が増えてしまうんで、がこのTRXっていうライブラリを使います。このprxトラブルではPPOとこのエルキュールっていう二つのアルゴリズムが使えて、今回この後者の方は無視していただいて構わないんですが、ImitationRankingとかで使われるような思考になっています。

[1:43:51 - 1:44:36]
今回はIPOの方を使うので特に気にしなくても大丈夫です。最初にPRLXとライブラリっていうのをインストールしていきます。はいこれはもう待つだけなので次の説明していくんですが、今回はさっきインプットデータとシチュエーションっていうのを読み込んでもらったんですがまだ読み込んでない方の喜んでいただいて大丈夫なんですが、このデータっていうのはどういったデータになってるかっていうと、プロンプトとアンサー1アンサーにっていう二つのデータがあって、Answer1がデータですね。

[1:44:36 - 1:45:07]
ワイン札ってかわいいです悪いデータ法になっています。このデータっていうのを使って報酬モデルを最初に学習していくっていう流れになります。実際にここはサンプルを表示してるようなんですが、プロンプトに対して朝晩と暗殺って二つがあるようなデータになって既存のデータセットを使用する場合は、ここですね。

[1:45:09 - 1:45:37]
今回はサプライズのセットようやくタスクについてのセットを使っています。これでもインプットそうですね。今回は、ちょっとインプットデータが使えない、使わない設定になってましたね。ちょっと勘違いしてました。勘違いしたんですが、多分使わない設定になってるんでいや違うか。

[1:45:37 - 1:46:22]
最初の報酬モデルはIMPALAデータを使ってその後のRLでは、翻訳タスクの使ってますね。はい。実装の中になっていくんですが、最初に報酬モデルを定義して報酬モデルを学習するっていう流れを実装しています。このGPTRewardモデルっていうのがGPTをベースとして報酬モデルをラッピングする形で計算するものになっていて、関数としてユニットとフォワードって二つが実装されてるんですが何をやってるかっていうといろいろやっていてごちゃごちゃしてて、わかんないかもしれないですが本質的にはこのRewardlossっていうのを計算してます。

[1:46:22 - 1:46:47]
つまり、中寸っていう選ばれた方のデータのRewardとリジェクトの方のRewardを使って、その差のマッチングモードを取ってるって形ですね。今回の講義でもいろいろ数式説明したと思うんですが、ここでこの損失関数を用いて、Rewardモデルを学習しているとそういった流れになります。

[1:46:47 - 1:47:08]
ちょっとこの詳しい流れはpaddingが存在するかどうかだったり細かい種類なってるんで後からじっくり見ていただきたいなというふうに思いますとりあえずこれで報酬モデルっていうのを定義しています。これ一応上から実行すれば動くやってるので、皆さんそうしていただけたらというふうに思います。

[1:47:09 - 1:47:28]
次にデータセットを定義している段階なんですが、今回POSのデータセットつまり、Xというプロンプトを出すときにワインとワイルズっていう二つが出てくるようなセットになってるんで、この会津セットを使って学習をしていきます。

[1:47:28 - 1:47:58]
今度セットの定義なんですが、それぞれの今回はchooseリジェクトって二つのデータですね、データがあったときにそれを特例として挙げる。IDに変えてあげるってところがここでやってるところです。それに対して、インプットとシューズのインプットとチーズのアテンションリジェクトのインプットリストのアテンションを返すようにデータセットをこれ作っています。

[1:47:58 - 1:48:34]
コンセプトのインテック戦略すると、各それぞれの中でリステッドのIDとテンションます勝手に入るっていうそういったデータセットの構造になっています。この実行してもらって、はいここはあのDセットの間瀬正解率を算出する関数と、あとはこのバッチ化の処理のところのデータになっていて、中ずっとリジェクトのデータを結合しているようなそういったデータになっています。

[1:48:37 - 1:49:04]
ここでモデルセットと報酬モデルとさ確認をすることができて、フジッコ作ります。この制度を実行してもらうと、chooseプロジェクトとかどういうデータがあるのかっていうのが、これ見れます。これでマーティセットのデータセットを用意することができているところになりますね。

[1:49:09 - 1:49:32]
中でアテンションのこのマスクってのはどういうふうに定義されてるかっていうと、地図のこのインデックスはこんな形で表されていて、この一番最後の50256っていうのがpadding表していて、データ自体はもうここまでここまでが文章で、とか全部paddingになってるって形です。

[1:49:33 - 1:49:51]
そのpaddingの部分と、0テンションゼロにしていってそういったマスクやってます。リジェクトっての方も一緒ですね。はい。このようなデータを作るためにこの上でいろいろごちゃごちゃやってたっていう形です。

[1:49:55 - 1:50:35]
でしょうか？あとはバッチ化する処理ですね。ちょっと若干入れてますが、もしちょはい。これでネクタイバッチかされたセットインプットとアテンションマスクと、あとはこのラベルですね。ゼロのエトワール悪い方のすデータなのか、いい方でいたらかっていう01のラベルが作られたデータがここで出力することが確認できていると思います。

[1:50:35 - 1:50:50]
あとはこのモデルRewardモデルっていうのをこれ定義してあげて、実際に出力としてどうなってるのかっていうのを見ることができます。これ元々のGPT-2のPretrainのモデルを使って学習をしています。

[1:50:52 - 1:51:20]
ここはモデルのダウンロード段階です。このGPTRewardモデルが何してるかっていうと、魚はちょっと戻るんですが、ここはGPTRモデルの提言ところなんですが、本当のGPT-2のモデルをロードして、ここで最後の日に出力だけまりnewsを押すcolorのAtariに変更するようにBiasをこの変更してます。

[1:51:20 - 1:51:51]
これをVヘッドっていうすることによって、一番最後の出力だけをスクラッチにして、Reward出力するようなモデルに変えてます。あとはものすごくさせるっていうワードが追加されてるだけです。このモデルを使うことで、出力とlossが入るなそういったものになってます。

[1:51:54 - 1:52:11]
ここで見てもらえばわかりますが、データを入力するとlossとchooseの方のスコアとリジェクトの方のスコアが、こういった形で出る形になってます。これあのバッチ化して、今回の位置に三、四5個を出力しているっていうそういったものになっています。

[1:52:15 - 1:52:39]
実際にこのRewardモデルっていうのを学習していくんですが、ひょここでとりあえずセットとバリエーションとセットっていうのを定義して、と、Rewardモデルの最初の70パーの層を凍結して、最後の30パーだけを学習するようにしてます。

[1:52:42 - 1:53:03]
僕はあの学習効率化のためですね。実際にこのトレーニング面と、っていうのを使って、プラスFormatのトレーナーってのがあるんですけど、それを使うことによってコンフィグを設定するだけで簡単にこのRewardモデルっていうのを学習することができます。

[1:53:04 - 1:53:30]
今これ実行してもらうと実際にこれ50Epochの学習が始まると思います。ここでRewardモデルを学習している、そういったものになります。実際にここ秋らしいが出てくると思うんですが、Stepが増えることに悔しいが高くなっていくのがわかると思います。

[1:53:30 - 1:53:56]
これちょっとTforなので、若干学習が遅いんですが、あのもしこのProとか課金されてる方は、これ尺とか押してもらうと、まっすぐ学習が進むかなというふうに思います。ちょっとこれ、今日は時間がないので最後まで待ってる暇がないかもしれないんですが、一旦この秋で仕上がっていくところを確認していただけたらなというふうに思います。

[1:54:05 - 1:54:30]
こんな感じっすね若干曲がってないですが、lossを下げ、lossも上がってますね。もうちょっと多分増やしてみるのと、あとデータセットとかが今少ないというセットなので、あまりうまくいってないかもしれないんですがここ量を増やして学習していくと、Rewardモデルもどんどん性のいいものが学習されていくっていう流れです。

[1:54:30 - 1:55:08]
流れです。今回はあの、ちょっとDPOの関係で、あの流れだけを体感していく行動の流れだけを体感していただいて、そもそもRL何やってるのかっていうのをざっくり理解してもらうのが目的に一応なっています。ちょっとこれ時間かかるので委託を飛ばすんですが、この上の段階で報酬モデルっていうのが実際に学習できたらそれをフリーズさせて、TRXっていうライブラリで実際にPPOを行うところが下の行動になっています。

[1:55:09 - 1:56:10]
ここの行動で、先ほどこの学習したRewardモデルっていうのを読み込んでる何かになっています。実行していくか。これも必要な数を定義していて、大体GPTの関数なんで、詳細は中身っていう部分あと中見てもらえばいいと思うんですがこのスコアだとサンプルをRewardモデルに入れてあげて、それTheRewardモデルの結果のスコアっていうのを根拠として出すような関数になっていてなプロンプトですあと、これ今回ようやくタスクでやってるので、TDRっていう文言を追加して、独立してデータ設定をしてあげるっていうものをやってます。

[1:56:10 - 1:56:47]
あとはRewardFunctionはサンプルを入力したときに、Rewardの前でしまして、成果したスコアってのが取得するように必要な関数になっています。よいしょその後にいろいろはいぱらがあるわけですが、いろんなハイパワーを設定して報酬モデルも学習するっていう流れになってます。

[1:56:50 - 1:57:09]
報酬もじゃないっすねこれ方策のモデルですね。方策のモデルを学習する流れになっています。OpenAIのサプライズDeNARっていう予約タスクのデータセットを使って、今回は報酬モデルじゃなくて方策もですね、方策モデルを学習するっていう流れになっています。

[1:57:14 - 1:57:57]
まだ終わらないっすね。ここがこれ見てもらえばわかる通り、PPOのコンフィグをこのたくさんあって、あとはスケジュールのコンパクト化をしていざこの日もありますし、お祭りなのは、元々の今週もあるんですがPPOだと結構ここら辺のコンフィグ1個多分初見だと全くわかんないかもしれないんですが、このクリップする練習とかだったりそういったような行為がたくさんあるのでもうこのコンフィグをどれが最適なハイパーなのかって探索するのが、非常にこのRだとRLだと、クロスってところがこの行動見てもわかるかなというふうに思います。

[1:57:59 - 1:58:53]
実際にここコンフィグを設定してあげて、TRXのTRAINっていう感想を読んであげると、実際に報酬モデルを使って方策の学習っていうのをPPOで行うってのができます。ちょっと時間もないので、この学習した後の成績結果だけを確認してみるんですが、元々のノートブックは学習後のチェックした結果のしてあるので、このような形で1dBで結果を見るような形にしていて実際にここにもプロンプトとアウトプットの結果、あとRewardのスコアが見えるような形でなっているんですが、これ全部上から実行していくとこれが出力される形になってます。

[1:58:54 - 1:59:27]
実際にこの日上のプロンプト最後にこのDeNARをつけて、要約してあげるっていうものをやってるんですが、一番報酬が低いのが、何か高いのか高いのがこれになってますね。こういうこれはメールですかね。メールの文に対して、これがようやく結果になっていてもこれが本当に若干正しいのかっていうのは、まだ疑問かもしれないんですが、これ出力に対して高い報酬が付き合ってるってのが、実際見てわかると思います。

[1:59:27 - 1:59:52]
今回はあの学習リソースの関係もあって、Epoch数が少なかったりlossが少ないので500円しか使ってないので、あんまりちゃんと学習はできないんですがこういったイメージとしては、この書くことに対してアウトプットができて、それに対するRewardを、高い音がいるように学習していくっていうそういった流れになっています。

[1:59:56 - 2:00:26]
はい。元の方も元の方はやっとRewardモデルの学習が終わりそうな段階ですね。Rewardモデル学習があるとあとは、この辺はすぐ多分実行できて、最後のRLのところまでスムーズに実行されていくと思います。

[2:00:31 - 2:01:03]
はい。一応どういうふうな結果だったのかと、あとはDBの結果とかも見ていただいて、やっぱりRL難しいなっていうところを体感していただけたらなというふうに思います。はい。若干あの行動の解説は、流しで細かいところは外出できなかったんですが、一旦9時になったので、ここで退出される方は、一旦講義を終了したいと思います。

[2:01:08 - 2:01:26]
はい。ということで演習は一旦このような形です。ありがとうございました。はいありがとうございます。これ延長しなくて大丈夫です。一旦これでちょっと、はいとどうします。一旦ここの最終的な結果出るところまで一旦やってみますね。

[2:01:26 - 2:02:03]
うんはいわかりましたはい、お付き合いできる方はそこまでいきたいというふうに思います。ちょっと質問を答えましょうか？では、薬処はいっぱいありますね。こっちの実行がちょっと終わらないんで若干引き続き残っていただいてる方向けに、質問の回答をしていきたいなというふうに思います。

[2:02:05 - 2:02:40]
はい。これですかね。なぜPPOを用いて、ランク付けした文書の良し悪しから次の登録を予測する確率分布を変更することができるんですか。これはいい質問ですね。なんでかっていうと、言語モデルにおける強化学習っていうのは何してるかっていうと、あるプロンプトが得られたときに、次のトークンを出力するってのが一つのActionに当たるわけですね。

[2:02:41 - 2:03:10]
つまりトークンを複数取得していくのは、Actionを何回も探索していく。流れになっていて、一番この文章を生成しきった段階で、Actionの決議まで積み重なってると思うんですけど、それがある意味文章を採用していてその文書に対して、あの出力だったのか悪い出力出力だったのかっていう01がRewardとして与えるわけです。

[2:03:10 - 2:03:50]
なので、最終的な出力としては、Rewardモデルは使うと、その文章の絶対的なスコアっていうのがRewardモデル変えられるので、それを用いて各ActionActionっていうのはトークンですね。次のトークン不足する、こういう方向にトークンの系列を生成していけば、最終的に高いスコアを得るってのがPPOの強化学習によって学習できるっていう、そういったわけですそれが間接的にさえ元のモデルの分布っていうのをとがらせるような役割をしている。

[2:03:51 - 2:04:17]
先ほどの最適方策のXポテンシャルのところの子からわかるように、元の分布をエクスプレーンシャルの方で強化してるようなそういった学習になっている。わけです。はい。ええ。あとは、これとかですかね。これさっき答えましたね。

[2:04:21 - 2:05:18]
結構解決も多いです。これはわかりましたRLのプロセスで各モデルの回答の振る舞いに差が出ると考えているでしょうか？学者の価値観やIMPALAとかHarmlessというかそうですね。これは正しいくて基本的にAlignmentの基準はさっき示した通り三つの位置でクリアしてるってのは前提の上でそれぞれモデルクローズがジムニーがどういったデータを用いて主しているか、もしくはDPOしてるかってのは公開されてないんでそのどういったLabeler、もしくはそうですねどういったLabelerによって作られたデータかによってどういう価値観を表現されてるのかっていうのはそれぞれ別ですね。

[2:05:18 - 2:05:33]
ただ一般的にこれはないといけない価値っていうのは、どのモデルでも多分きちっとされていると思うんで、そこの細かい違いってのはそのデータによって変わってくる、もしくはLabelerって変わってくるってそういった名が形だと思います。

[2:05:35 - 2:05:56]
はい。これあれですね。1DのPPO入れてないとエラーが出るんで、多分この1DのAPI機、皆さん持ってる方はあのご入力画面が出てくると思うんで、それを入力していただけると学習が進むかなというふうに思います。

[2:05:56 - 2:07:14]
もしくは、ここですね。ドラッカーイコールのなんていうのを設定してあげると、一応、1Dのアカウントがなくても学習できるようにはしていて、よいしょこれちょっと重すぎて、金リセットされてしまいましたランタイムが何でここで皆さんの方ではもう実行できたかもしれないんですが、またランタイムがちょっとやっぱPPOの実行って重くて、あのメモリが落ちてしまうとかあるので今ちょっとRankingしちゃったんですが、一応皆さんの方では一応、上から順次実行していくか実行していくだけだと学習できるようなってるんで結果として見えたのかなというふうに思うんですが、自分の方ではまた1からRewardまでの学習から直さなきゃいけなくなっちゃったんで、ちょっと今回の時間内に間に合うかわかんないんですがあれですよねなんか10分以上超えるとちょっとまずい感じですよね、おそらく。

[2:07:16 - 2:07:44]
すいません。今んところ特に10分以上とか指定はしてなくてですね。はい本当ですか。はい可能な限り質問答えてっていう感じで大丈夫ですかねそれではい、そんな感じでも大丈夫そうであれば、はい、お願いします皆さんあの適宜用がある方抜けていただいて構わないんですが、自分の方は一旦この最後の出力が出るまで、あの質問答えていきたいなというふうに思います。

[2:07:44 - 2:08:20]
なんで今質問してくれたら、自分の方でも答えるんで録っていただいて大丈夫です。ビジネス答えますか。コード生成を行った場合にそのコードについて方法として具体的な方法ありますか。ですね。コード生成の評価のバリエーションとして、よく使われてるのは、大きく二つあって、マヒワevalっていうものと、これはOpenAIを公開してますね。

[2:08:21 - 2:08:53]
確かオープンだと思います。MLってものと、あとMBPって言われる行動のジェネレーションタスクがあるんですけど、そのタスクとかだと、テストコードっていうのが用意されていて、つまり、テストケースみたいなのを書くと思うんですけどACERプログラムするときもちょっと押してしまう人も多いかもしれないですけどそのテストコードがあってそのテスト行動の正解に合うようにFeedbackを与えてあげるっていう形です。

[2:08:53 - 2:09:23]
つまり、そのテストケース通ったら、1通らなかったらゼロみたいな形ですね。力を与えてあげて、collegeするみたいなそういったものもありますね。なんで行動の場合は絶対評価があるそもそもそのプローブが実行できたできたのかっていう評価もありますし、実行できた上で、ちゃんとテストケースが通ったのかっていう、二つの軸が評価としてはあるかなというふうに思います。

[2:09:24 - 2:10:36]
はい。あとは、ここら辺を解決したらいいんですかね。あとはAlignment税が発生する原因について教えてくださいってところですね。ちょっとこれ資料を見ながらいいか待ってください。AlignmentTaxっていうのは、評価1個になるんですが、Pretrainのデータから、そのアップデートしてるわけなんで、完全に分布だけを変更してるかって言われると、その元のPretrain殿実を更新してるわけなんで、もちろん何かしらの知識がそこにある可能性っていうのは全然あるわけですね。

[2:10:37 - 2:11:10]
このAlignmentXを得ない場合っていうのは、その事前知識のTRAINの分布っていうのを、ある程度維持するっていうことをしたくて、それをしないと、一見別な方向の分布に偏ってしまう。っていうのが起こりえたり、あとはパラメータプレートによって知識が回せてしまうっていうのがあるんで普通にパラメータ更新してる限りこのAlignmentでっていうのは起きる可能性がある。

[2:11:11 - 2:11:37]
ていうところです。なんでこのReplayって所を使って、Alignment勢をできるだけ少なくするように学習をしているっていう、そういった流れになります。なんでパラメータ会としては、答えとしてはパラメータアプリとしてる限り、少なからずも忘却起こってしまうっていうのが答えになります。

[2:11:43 - 2:12:36]
ここまできてます。はい。あとは、質問ありますかね。ちょっと検索でしょうか？ミドルとかもできますか。以上です。言いますね。はい失礼します。RRHFやDPOはモデルとの総合学習更新することが多いですか。そうですね。

[2:12:36 - 2:13:17]
GIFの場合は報酬モデルと方策モデルの学習がわかれているので、それぞれ違うんですが方策モデルは、報酬モデルは先ほどの行動の例でも、最初の70パーとか80%は固定して学習する。って言った例が多いですね。つまり最終的な出力値color値の図値になるので、最終層付近と最後のヘッドだけを更新するっていう流れで、普通の方策のモデルは全部学習することが多いですねDPOも一緒だと思います。

[2:13:17 - 2:13:36]
ただDPOの場合は、マルチもそうかもしれないですけど、ローラーを使って、学習するってのもあってDPOのラップとか、あのHackingSのモデルとかでも見たことある人いますがいるかもしれないですけど学習効率化のためにDPOローラーを使って学習することも多いですね。

[2:13:37 - 2:13:59]
PROってのは要約しろっていうことですグローブとかでもarXivとかもEADRH5の薬とかあると思うんですけど予約分って言うんです。これかな。結局このRチーフ流れる部分は人間に対する回答文をかけてうまく変換するような出力する海斗の確立できればいいかそうですね。

[2:13:59 - 2:14:29]
大体合ってます。てかほとんどその通りだと思います。RJSDPPOChartsACERRチーフはPPOを使って学習してるんでRジェフとDPOって比較なると思うんですが、DPOのが圧倒的に少ないし、メモリ容量も少なくて済みますとかRHFってあれ使ってるんで、Actor-Criticって所を最初に説明したと思うんですけど、ActorとCriticでもさらにこのモデルを分けないといけない。

[2:14:29 - 2:14:54]
ですねそうするとすごいあのメモリは苦しい、あの学習も重いし、時間もかかるっていうのであんまりっていうか絶対やりたくない。地方ですね。しかも、実際これも時間がかかってるので、あとはSFTR違うしたときにlossのスパイクを発することはありますね。

[2:14:54 - 2:15:11]
Pretrainでもlossのスパイクが発生して、なんていう学習としてやり直したり、あのlossのスパイクを抑えるためにいろんな工夫したりとかすると思うんですけどレジでもしますね。なんで工夫は必要だと思います。

[2:15:12 - 2:15:40]
DPODPOの方が優れているかわかりません各newsがPPOの方が精度が高い理由はゲームとか見せている認識でよろしいですか。そうですね。DPORPPOの方がすごいって理由に関しては先ほども高下の中でも一つ説明したのは、これはちょっと自分の主観も入っちゃうかもしれないんですけどDPOだと強者学習なんで、ストロー教師データ以外の情報使えないんですね。

[2:15:40 - 2:15:59]
それはPPOも一緒じゃないかって思うと思うんですけど、PPOはRL段階において探索っていうものが含まれてるんですね。つまり元の学習って言ったではない入力Xのプロンプトもう探索してそれに対するRewardも出てるんですね。

[2:16:00 - 2:16:33]
なんで、そこのデータの違いっていうか、その道のプロ人に対してRewardモデルは元の学生体内出力に対してReward計算しないといけないんですが、そこのデータがいい感じにそのRewardモデルがシノブ近似していると、その外装のデータに外装の出力をにアクセスできるので、そのデータを使って、方策モデルを学習することで、PPOよりもDPOよりも良くなるんじゃないか。

[2:16:33 - 2:16:59]
ていうふうに自分は考えてますね。そうやって主張してる人もいます。ただ、元々考えてみると、そのデータセットは一緒なので、ぺPPOもDPOも同じ目的関数で数学的に本岡なわけなので、本来的には一瞬ないとおかしいはずなんですね。

[2:17:00 - 2:17:14]
ただそのPPOが良くなるってのは先ほど説明した理由もありますし、他にも多分おそらくいろいろあるかもしれないんですが、確定的にこれだから悪くなるっていう理由は解明されてないっていう認識で合ってると思います。

[2:17:18 - 2:18:29]
はい。もうちょっとできそうですね動き始めたあと一、二個ぐらい外出して、質問に回答して終わろうかなというふうに思います。画像生成モデル言語モデルRRHF使ってるんですがこれは結構いい質問ですね。これは結論としてはイエスで、最近Preferenceを使ったreligionとかもあって、何やってるかっていうと、religionのロジックプロセスで自ら画像生成してるわけですけど、その生成した最後の画像を複数用意してあげて、最初のノイズのランダム性によっていろんな多様性がある画像って生成できるので、それに対して10とかRankingとかをつけてあげて、それを使って、あの言語モデルと同じようにcollegeするってのは実際に研究でもあります言語モデルの場合はActionが次のトークンだったんですけど画像の場合は次のステップの画像、つまりでのミーティングの1Stepですね、ノイズを取り除くて1Stepを一つのActionとしてやってます。

[2:18:30 - 2:18:57]
なんで結論としては使われてて、性能もいくらっていうか、画像だとそのボール傾向と言わないとかあんまないんですが、何かスタイル、あの生成性能を何て言うんでしょうね、よりプロンプトに忠実な画像生成の結果とかそういったものに、そういった面で性能向上してるっていう研究はあったりしますね。

[2:19:00 - 2:19:20]
できたかな。若干もうちょっと多分ここら辺はこの時間ちょっと難しいかなと思うんで、一旦ここまで出力できたら皆さん、いいかなというふうに思います。RAFTとアウトプットと、あとRewardの結果が見れたらいいかなと思います。

[2:19:21 - 2:19:46]
これと一番高いのこれですね。ここんRewardこれですね一番高いなんかあんま良くなってるかわかんないんですがうん結構BPO難しいのであとこれ50Epoch近くしたいんで、こんなもんかなっていうふうに思います。

[2:19:49 - 2:20:27]
はい。っていう感じですかね。ラスト1個だけあの回答して、今回の講義時間終わりたいと思います。よっかなこれ答えますか。例えばドラマなどのSSモデル特定のビジネスやタスクで使う場合、継続事前学習を配置することがありますが、その際にRLのDPOも一緒に実施した方がいいんでしょうか？それともモデルが硬直して、ときにDPOなどが実施されている場合不要だと考えるものですか。

[2:20:27 - 2:20:55]
これは結構、良いといいですね。元モデルが既にDPOされてる場合、RRHFもさらにすればいいのかっていう質問と捉えたんですが、結論としてはやった方がいいと思ってAlignment特にDPOだと多段階の最適化でもできるわけですね。

[2:20:55 - 2:21:52]
さっきの最適方策がXポテンシャルで分布を変化させてる。っていうふうに言ったと思うんですけど、あれって、また例えば何かある観点でAlignmentして学習しましたその次にまた別の観点でAlignmentして学習しました2段階学習してるわけですけど、これってEXPOね者の方がまた掛け算で掛け合わさってくだけなので、結局スペースを掛け算すると中身が私を去っていって、一番元のモデルにその全てのこのだったでしょ、それぞれAlignment全部混ぜこぜにしてあげて、学習するのと同じ子を変えるってのは薄々基準でも示されて、つまり、最初にもう出すとごっちゃにしてPPOしてあげるのと、多段階でそれぞれの観点でそれぞれ別々に学習するのって、数式的には10日になるんですね。

[2:21:52 - 2:22:29]
なんで、既にBPOされたモデルでも、さらに毎日とかDPOしても、全然精度良くなると思います。なんで制度た局面を高めたい場合は、実施した方がいいっていうふうな回答になります。はい。っていう感じで、ちょっとだいぶ長くなりましたが、ここまで実行していただけたら皆さん大丈夫なので、今回の講義はこれで終了させていただきたいと思いますかなりちょっと長い長丁場やってしまって申し訳ないんですが皆さんありがとうございました。

[2:22:30 - 2:22:49]
はい、それでは終了したいと思います。はい高階さんありがとうございましたお疲れ様です。お疲れ様です。はい、ではちょっと最後運営の方からいくつかご連絡させてくださいちょっと時間もしてるので手短にご説明できればと思います。

[2:22:50 - 2:23:19]
まずですねイベントのご案内ですちょっと詳細はあの時間の都合上割愛するんですがコミュニティのもくもく会ですね、同コミュニティのメンバー主導の勉強会っていうものをそれぞれやりますもくもく会はですね各自講座の内容だったりとかをオンラインで繋ぎながら、あの勉強するっていうような、あのフリーの何ていうかね勉強会です。

[2:23:20 - 2:23:52]
こちらのリンクありましたのでぜひこの講座の振り返りだったりとか、次の予習でお使いいただければと思いますもう一つですね、今貼ったのが去年のLLM講座の受講生であって今年人役のプロジェクトで実際LLMを開発した人たちによるLLMの作り方の勉強会っていうところが、あの企画企画してます今週日曜日なのでぜひ皆さん参加いただければと思います既に450名ぐらいの方々事前応募いただいてるというような状況になります。

[2:23:53 - 2:24:23]
はい。というのがイベントのご案内でございました。次貼ってるのが、コミュニティのですね、あのなんだっけコミュニティLLMコミュニティのポータルページっていうのを作りましたリンクのところから入っていただいて、ですね入っていただくと様々な情報をこちら集約しておりますのでぜひご活用いただけるとありがたいですと思っております。

[2:24:23 - 2:24:39]
はい。あと最後ちょっとまた改めて補足でSlack上でもご連絡させていただければと思っているんですけども、チャットボットですねちょっと若干バージョンアップしましたのでちょっとそこだけ最後すみませんクイックにお話させてください。

[2:24:42 - 2:25:29]
このちょっと質問方法についてですね、あれどこだっけ管理職チャットボットでいつも質問いただいているのは基本的には同じやり方なんですが、あれ、かなりちょっとBotで質問するでですねこの質問範囲のところをちょっと若干バージョンアップしました後期範囲内のところを選んでいただいて質問をすることでこれまで講義内でしか集め扱ってなかった専門知識以外のごめんなさい。

[2:25:30 - 2:26:01]
言い直します講義内で講師の人が教えたりとかして、講義で扱った内容に対して質問をいただきたいという他いただく場合はこの講義範囲内ですね質問いただければと思います。講義範囲外で質問いただくと例えば今日の講座とかもまだ情報が入ってない状況なんですが、そういったときとかに通常のちょっとGPTないしはLLMを使った回答っていうのをできるようにしてますので併せてこちらご活用いただければというふうに思います。

[2:26:02 - 2:26:34]
はい。あとですねここでみんな以外でもどちらでもそうなんですけども参考文献というものが出てくると思うんですけども、そちらをですね、より深く知りたいっていう方に関しましては先ほど高城講師の方からありましたが、このarXivinterpreterというものを実装しておりますクリックしていただくと、Slackのページ、チャンネルの方に飛ぶと思いますので、そこで気になる論文のリンクをペタペタ貼っていただくとですね、ちょっとお見せした方がいいです。

[2:26:37 - 2:27:01]
ようやく説明がしをしてくれるっていうような感じですね。例えばここに、このチャンネルなんですけども、いろんなarXivインタプリタところあるので皆さんこちら入っていただいて気になる論文のリンクをペタッて貼っていただくと、このスレッドで要約してくれたものっていうのを見ることができます。

[2:27:01 - 2:27:33]
これは高城さんが作ってくれたあの素晴らしいアプリケーションなのでぜひ皆さんご活用いただければと思いますので、はい。そちらのご案内になりました。はいということですいませんちょっと漢字も弱くなってしまったんですが以上とさせていただければと思いますご参加いただきありがとうございますまた次回、来週の19時ですかねまたご参加いただければと思ってますので、よろしくお願いします通常通り出欠アンケートにご回答いただくっていうのと、宿題も来週の水曜日の17時までですかね。

[2:27:33 - 2:27:40]
にご提出いただくようお願いいたしますでは本日以上とさせていただきますお疲れ様でしたありがとうございます。